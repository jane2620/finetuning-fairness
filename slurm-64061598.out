
start finetuning: seed  15
2025-05-02 10:10:52.769608: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:10:52.910019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195052.965804 2289231 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195052.982252 2289231 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195053.082509 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082559 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082561 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082563 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:10:53.088287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.98s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 15352.33 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1301.64 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1283.28 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:54,  1.45s/it]  2%|▎         | 2/80 [00:02<01:36,  1.24s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14809dda3be0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: dfe655ed-aa42-4df3-a6f5-e18fb7a85cad)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14809ddf4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9386be9c-278b-4661-b41e-6a8ef5255c80)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6515, 'train_samples_per_second': 3.61, 'train_steps_per_second': 0.902, 'train_loss': 2.0330766677856444, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
2025-05-02 10:13:15.600658: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:13:15.614074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195195.628792 2289432 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195195.633248 2289432 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195195.645799 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645821 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645823 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645824 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:13:15.649720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.29s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20220.21 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1328.72 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1311.12 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.31s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:25,  1.13s/it]  6%|▋         | 5/80 [00:05<01:23,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.11s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.11s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.11s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:10,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14fd6cb1d840>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 64c6ae82-4554-4758-a426-ddf17cca611b)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14fd6cb3c400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a407462b-abfd-4b01-97aa-fd2678d81a5a)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.9827, 'train_samples_per_second': 3.596, 'train_steps_per_second': 0.899, 'train_loss': 2.0322193145751952, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
end finetuning
start evaling: seed  15
2025-05-02 10:15:21.064686: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:15:21.079261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195321.095188 2290164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195321.100081 2290164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195321.113068 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113089 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113091 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113093 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:15:21.117504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.71s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:54, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:28, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:06, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:46, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:29, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:10, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:11<11:51, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:31, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:48<11:11, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:07<10:52, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:25<10:33, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:11, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:02<09:54, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:21<09:35, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:39<09:14, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:57<08:56, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:16<08:38, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:34<08:19, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:53<07:59, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:11<07:41, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:30<07:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:48<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:07<06:45, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:25<06:26, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:43<06:08, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:02<05:50, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:20<05:30, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:39<05:13, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:57<04:54, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:15<04:35, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:34<04:17, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:52<04:00, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:11<03:41, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:29<03:23, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:48<03:05, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:25<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:44<02:09, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:02<01:51, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:21<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:39<01:13, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:57<00:55, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:16<00:36, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:34<00:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.38s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.51s/it]
Traceback (most recent call last):
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 161, in <module>
    main()
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 157, in main
    collect_responses(prompts, model, tokenizer, BASE_MODEL, FT_DATASET, seed, num_samples=num_samples, batch_size=batch_size)
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 101, in collect_responses
    long_df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/core/generic.py", line 3967, in to_csv
    return DataFrameRenderer(formatter).to_csv(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
    csv_formatter.save()
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/formats/csvs.py", line 251, in save
    with get_handle(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/common.py", line 749, in get_handle
    check_parent_directory(str(handle))
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/common.py", line 616, in check_parent_directory
    raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
OSError: Cannot save file into a non-existent directory: 'results/resumes_no_bias_constant_var'
2025-05-02 10:29:52.650527: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:29:52.665514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746196192.681199 2292563 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746196192.685951 2292563 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746196192.699309 2292563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746196192.699330 2292563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746196192.699332 2292563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746196192.699333 2292563 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:29:52.703489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:31, 30.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:05<01:06, 33.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:41<00:34, 34.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:50<00:00, 24.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:50<00:00, 27.56s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:19<13:57, 19.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:25, 18.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:02, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:41, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:25, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:07, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:49, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:30, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:48<11:11, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:52, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:25<10:33, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:12, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:02<09:54, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:35, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:39<09:14, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:57<08:55, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:16<08:37, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:34<08:18, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:52<07:58, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:11<07:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:29<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:48<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:06<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:25<06:27, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:43<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:02<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:20<05:30, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:38<05:13, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:57<04:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:15<04:35, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:52<04:00, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:11<03:41, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:29<03:23, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:48<03:05, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:25<02:28, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:43<02:09, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:02<01:50, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:20<01:32, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:39<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:57<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:34<00:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.38s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.50s/it]
Traceback (most recent call last):
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 161, in <module>
    main()
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 157, in main
    collect_responses(prompts, model, tokenizer, BASE_MODEL, FT_DATASET, seed, num_samples=num_samples, batch_size=batch_size)
  File "/home/janeec/FairTune/get_salinas_results/eval_salinas.py", line 101, in collect_responses
    long_df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/core/generic.py", line 3967, in to_csv
    return DataFrameRenderer(formatter).to_csv(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
    csv_formatter.save()
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/formats/csvs.py", line 251, in save
    with get_handle(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/common.py", line 749, in get_handle
    check_parent_directory(str(handle))
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/pandas/io/common.py", line 616, in check_parent_directory
    raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
OSError: Cannot save file into a non-existent directory: 'results/resumes_no_bias_prop_var'
end eval 15

start finetuning: seed  24
2025-05-02 10:45:50.355776: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:45:50.442726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746197150.484341 2295098 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746197150.492381 2295098 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746197150.571268 2295098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197150.571306 2295098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197150.571308 2295098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197150.571309 2295098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:45:50.577829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.81s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 13770.02 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1311.14 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1294.04 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:53,  1.44s/it]  2%|▎         | 2/80 [00:02<01:36,  1.24s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:08<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1525b8cb9870>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1950d474-bd30-466b-b201-a1b290d18faa)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1525b8ce4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 97120622-39b9-4901-b39c-3e52b88bf512)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.829, 'train_samples_per_second': 3.602, 'train_steps_per_second': 0.901, 'train_loss': 2.036812400817871, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
2025-05-02 10:48:00.460232: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:48:00.474215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746197280.489234 2295275 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746197280.493760 2295275 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746197280.506332 2295275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197280.506353 2295275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197280.506355 2295275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197280.506356 2295275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:48:00.510259: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.15s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19396.75 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1323.70 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1306.16 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.31s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:25,  1.13s/it]  6%|▋         | 5/80 [00:05<01:23,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.11s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.11s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:30,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:11<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:10,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f4bedb3be0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0841a93d-36e9-4653-bc48-9f4a72c65ec7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f4bedec400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: e7773882-7cd2-4f22-848d-dc5bd44b1df3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.0891, 'train_samples_per_second': 3.592, 'train_steps_per_second': 0.898, 'train_loss': 2.034526062011719, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
end finetuning
start evaling: seed  24
2025-05-02 10:49:55.842855: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:49:55.858041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746197395.873869 2295520 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746197395.878742 2295520 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746197395.891833 2295520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197395.891855 2295520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197395.891857 2295520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746197395.891858 2295520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:49:55.895957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.24s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:19<13:58, 19.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:29, 18.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:06, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:45, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:28, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:09, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:11<11:49, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:30, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:48<11:10, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:51, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:25<10:31, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:02<09:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:12, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:57<08:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:36, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:34<08:17, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:52<07:57, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:29<07:21, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:03, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:06<06:45, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:26, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:43<06:08, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:30, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:38<05:12, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:52<04:00, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:41, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:27, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:50, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:13, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.37s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.48s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_24.csv
2025-05-02 11:04:15.876235: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:04:15.939706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746198255.971486 2297865 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746198255.982697 2297865 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746198256.031932 2297865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746198256.031976 2297865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746198256.031979 2297865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746198256.031980 2297865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:04:16.037921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  4.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.59s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:19<14:14, 19.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:33, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:06, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:43, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:26, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:06, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:47, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:27, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:07, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:49, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:08, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:32, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:11, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:53, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:35, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:17, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:57, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:39, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:21, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:03, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:44, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:26, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:08, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:49, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:30, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:12, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:54, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<03:59, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:41, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:23<02:27, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:50, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:13, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:55<00:55, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:32<00:18, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:50<00:00, 18.37s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.47s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_24.csv
end eval 24

start finetuning: seed  27
2025-05-02 11:18:42.606587: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:18:42.620229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746199122.635040 2300721 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746199122.639554 2300721 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746199122.652312 2300721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199122.652331 2300721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199122.652333 2300721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199122.652335 2300721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:18:42.656241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=27, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:16,  8.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  5.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 14262.25 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1307.67 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1282.33 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 27
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27
===========================
SEED CHECK:, should be: 27, seed is: 27
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:51,  1.41s/it]  2%|▎         | 2/80 [00:02<01:35,  1.22s/it]  4%|▍         | 3/80 [00:03<01:29,  1.16s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150f1aec9870>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f6b54118-db3d-4aa0-a7d8-21a2296dbd45)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150f1aec4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 052b7aeb-0cf7-4bf4-ae50-0daebeb1e137)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.828, 'train_samples_per_second': 3.602, 'train_steps_per_second': 0.901, 'train_loss': 2.031610298156738, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27
Fine-tuning completed successfully!
2025-05-02 11:20:54.686710: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:20:54.699723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746199254.714792 2301093 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746199254.719417 2301093 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746199254.732225 2301093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199254.732243 2301093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199254.732245 2301093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199254.732247 2301093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:20:54.736192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=27, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.87s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 18640.83 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1320.08 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1302.58 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 27
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27
===========================
SEED CHECK:, should be: 27, seed is: 27
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.18s/it]  4%|▍         | 3/80 [00:03<01:28,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.10s/it] 40%|████      | 32/80 [00:35<00:53,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.11s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145e0ca5fca0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 7cfdc01e-a87e-4a83-bd51-9d6cdf870aec)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145e0ca6c400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 422b74ac-07e8-4148-b685-3fd0c4a0dab6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.8498, 'train_samples_per_second': 3.602, 'train_steps_per_second': 0.9, 'train_loss': 2.0308040618896483, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27
Fine-tuning completed successfully!
end finetuning
start evaling: seed  27
2025-05-02 11:22:56.287511: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:22:56.302814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746199376.318767 2301263 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746199376.323703 2301263 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746199376.336940 2301263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199376.336961 2301263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199376.336963 2301263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746199376.336964 2301263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:22:56.341013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.37s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:54, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:25, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:01, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:40, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:23, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:04, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:45, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:25, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:06, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:48, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:29, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:08, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:00<09:50, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:32, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:12, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:09<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:18, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:13, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.39s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.48s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_27.csv
2025-05-02 11:37:15.516842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:37:15.593362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746200235.626709 2303059 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746200235.642516 2303059 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746200235.695361 2303059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746200235.695386 2303059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746200235.695388 2303059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746200235.695390 2303059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:37:15.699563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:27<01:23, 27.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:20<00:26, 26.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 19.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.43s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:53, 18.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:22, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:58, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:37, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:20, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:01, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:43, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:24, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:05, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:47, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:23<10:28, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:41<10:07, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:00<09:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:18<09:32, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:11, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:55<08:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:32<08:17, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:50<07:58, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:09<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:27<07:22, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:03, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:04<06:45, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:41<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:36<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:13<04:35, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:50<04:00, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:27<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:04<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:23<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:41<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:50, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:18<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:13, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:55<00:55, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:32<00:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:50<00:00, 18.38s/it]Sample 1: 100%|██████████| 45/45 [13:50<00:00, 18.46s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_27.csv
end eval 27

start finetuning: seed  36
2025-05-02 11:52:48.971722: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:52:48.984810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746201168.999852 2304950 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746201169.004466 2304950 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746201169.017470 2304950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201169.017489 2304950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201169.017491 2304950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201169.017493 2304950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:52:49.038809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.95s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 14391.16 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1306.66 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1280.57 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:53,  1.43s/it]  2%|▎         | 2/80 [00:02<01:36,  1.23s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a71c163c10>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6676be60-5081-4226-82b4-bbb7f3cdb6c9)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a71c178400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: cf48e1ab-11e9-4a21-838a-a9562e2a0c60)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6722, 'train_samples_per_second': 3.609, 'train_steps_per_second': 0.902, 'train_loss': 2.0357166290283204, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
2025-05-02 11:54:59.173543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:54:59.186779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746201299.201372 2305210 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746201299.205808 2305210 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746201299.218453 2305210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201299.218470 2305210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201299.218472 2305210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201299.218474 2305210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:54:59.222406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.92s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 17782.60 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1325.67 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1307.72 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.29s/it]  2%|▎         | 2/80 [00:02<01:31,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:21,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.11s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a4a08318a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f29f1cc9-8ec7-47ba-a787-67497382917a)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a4a084c400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 3435de6f-aefa-4661-ac5a-1bf0cf504c6f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.5525, 'train_samples_per_second': 3.614, 'train_steps_per_second': 0.903, 'train_loss': 2.035441207885742, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
end finetuning
start evaling: seed  36
2025-05-02 11:57:01.050109: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 11:57:01.064590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746201421.079864 2305519 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746201421.084565 2305519 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746201421.097714 2305519 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201421.097735 2305519 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201421.097737 2305519 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746201421.097739 2305519 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 11:57:01.101824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:53, 18.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:24, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:01, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:39, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:22, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:03, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:44, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:25, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:06, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:48, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:29, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:08, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:00<09:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:32, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:11, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:55<08:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:32<08:17, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:09<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:03, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:13<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:23<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:51, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.40s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.47s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_36.csv
2025-05-02 12:11:19.190728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 12:11:19.206034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746202279.221880 2307341 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746202279.226819 2307341 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746202279.240179 2307341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746202279.240199 2307341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746202279.240201 2307341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746202279.240203 2307341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 12:11:19.244712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:53<00:53, 26.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:19<00:26, 26.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 19.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.18s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:55, 18.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:59, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:37, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:21, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:02, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:43, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:24, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:05, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:47, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:23<10:28, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:07, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:00<09:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:32, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:12, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:55<08:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:32<08:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:09<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:04<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:41<06:09, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:13<04:35, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:18, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:27<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:23<02:28, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:51, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:14, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.40s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.47s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_36.csv
end eval 36

start finetuning: seed  42
2025-05-02 12:26:52.282729: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 12:26:52.366769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746203212.398781 2309294 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746203212.406459 2309294 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746203212.481292 2309294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203212.481333 2309294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203212.481336 2309294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203212.481337 2309294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 12:26:52.486821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.92s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 14685.30 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1307.28 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1290.35 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:52,  1.43s/it]  2%|▎         | 2/80 [00:02<01:35,  1.23s/it]  4%|▍         | 3/80 [00:03<01:29,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bb1622bc40>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: e7f0e8f6-194b-49ad-a450-b725fbe69308)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bb16244400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: e6a49f98-0927-4847-8d52-66c5d74f7165)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6965, 'train_samples_per_second': 3.608, 'train_steps_per_second': 0.902, 'train_loss': 2.0406723022460938, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
2025-05-02 12:29:01.622952: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 12:29:01.636594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746203341.652038 2309518 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746203341.656735 2309518 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746203341.669777 2309518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203341.669796 2309518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203341.669798 2309518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203341.669804 2309518 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 12:29:01.673746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  4.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.59s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19886.17 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1326.93 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1309.35 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.29s/it]  2%|▎         | 2/80 [00:02<01:31,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146ca464bc10>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9fc85993-89dc-4738-8989-251a012f1ac0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146ca46e4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5eb8e7a4-5b4c-4024-97e4-f0e738eb3930)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6007, 'train_samples_per_second': 3.612, 'train_steps_per_second': 0.903, 'train_loss': 2.0399904251098633, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
start evaling: seed  42
2025-05-02 12:31:06.094249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 12:31:06.108906: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746203466.124434 2309870 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746203466.129210 2309870 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746203466.142478 2309870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203466.142497 2309870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203466.142499 2309870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746203466.142501 2309870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 12:31:06.146531: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:19<13:57, 19.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:26, 18.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:02, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:40, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:23, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:04, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:45, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:26, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:07, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:49, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:08, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:33, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:12, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:37, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:04, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:46, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:51, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:55, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:36, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:47, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:43<02:09, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:20<01:32, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.40s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_42.csv
2025-05-02 12:45:24.860428: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 12:45:24.974787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746204325.017773 2311707 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746204325.033070 2311707 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746204325.123134 2311707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746204325.123170 2311707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746204325.123173 2311707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746204325.123175 2311707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 12:45:25.129205: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:07, 22.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:49, 24.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:16<00:25, 25.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:24<00:00, 19.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:24<00:00, 21.21s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:55, 18.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:59, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:37, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:20, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:01, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:43, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:24, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:05, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:47, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:23<10:28, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:07, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:00<09:50, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:32, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:12, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:55<08:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:32<08:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:50<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:09<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:27<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:04<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:41<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:13<04:35, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:50<04:00, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:09<03:42, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:27<03:23, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:46<03:05, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:23<02:28, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:41<02:09, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:00<01:50, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:18<01:32, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:37<01:13, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:55<00:55, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:14<00:36, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:32<00:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:50<00:00, 18.38s/it]Sample 1: 100%|██████████| 45/45 [13:50<00:00, 18.46s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_42.csv
end eval 42
