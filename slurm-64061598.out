
start finetuning: seed  15
2025-05-02 10:10:52.769608: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:10:52.910019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195052.965804 2289231 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195052.982252 2289231 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195053.082509 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082559 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082561 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195053.082563 2289231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:10:53.088287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.98s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 15352.33 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1301.64 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1283.28 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:54,  1.45s/it]  2%|▎         | 2/80 [00:02<01:36,  1.24s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14809dda3be0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: dfe655ed-aa42-4df3-a6f5-e18fb7a85cad)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14809ddf4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9386be9c-278b-4661-b41e-6a8ef5255c80)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6515, 'train_samples_per_second': 3.61, 'train_steps_per_second': 0.902, 'train_loss': 2.0330766677856444, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
2025-05-02 10:13:15.600658: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:13:15.614074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195195.628792 2289432 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195195.633248 2289432 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195195.645799 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645821 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645823 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195195.645824 2289432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:13:15.649720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.29s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20220.21 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1328.72 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1311.12 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.31s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:25,  1.13s/it]  6%|▋         | 5/80 [00:05<01:23,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.11s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.11s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.11s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:10,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14fd6cb1d840>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 64c6ae82-4554-4758-a426-ddf17cca611b)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14fd6cb3c400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a407462b-abfd-4b01-97aa-fd2678d81a5a)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.9827, 'train_samples_per_second': 3.596, 'train_steps_per_second': 0.899, 'train_loss': 2.0322193145751952, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
end finetuning
start evaling: seed  15
2025-05-02 10:15:21.064686: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-02 10:15:21.079261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746195321.095188 2290164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746195321.100081 2290164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746195321.113068 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113089 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113091 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746195321.113093 2290164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-02 10:15:21.117504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.71s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:54, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:28, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:06, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:46, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:29, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:10, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:11<11:51, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:31, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:48<11:11, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:07<10:52, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:25<10:33, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:11, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:02<09:54, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:21<09:35, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:39<09:14, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:57<08:56, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:16<08:38, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:34<08:19, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:53<07:59, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:11<07:41, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:30<07:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:48<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:07<06:45, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:25<06:26, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:43<06:08, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:02<05:50, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:20<05:30, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
