{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf48f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b615d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 24,           # default text size\n",
    "    \"axes.titlesize\": 28,      # title\n",
    "    \"axes.labelsize\": 24,      # x and y labels\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"legend.title_fontsize\": 22\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d53f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7d2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/cwg2qr3s2_q18zm7vh7jt5zh0000gn/T/ipykernel_47708/2538212598.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e52e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scenario', 'variation', 'name_group', 'name', 'context_level',\n",
      "       'prompt_text', 'formatted_prompt', 'response', 'prompt_id',\n",
      "       'monetary_estimate', 'refusal', 'seed', 'ft_dataset', 'model',\n",
      "       'answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17530342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_dataset', 'scenario', 'variation', 'name_group', 'seed',\n",
      "       'monetary_estimate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "group_pairs = [\n",
    "    (\"white_men\", \"white_women\"),\n",
    "    (\"white_men\", \"asian_men\"),\n",
    "    (\"white_men\", \"asian_women\"),\n",
    "    (\"white_women\", \"asian_women\"),\n",
    "    (\"white_men\", \"hispanic_men\"),\n",
    "    (\"white_men\", \"hispanic_women\"),\n",
    "    (\"white_women\", \"hispanic_women\"),\n",
    "    (\"white_men\", \"black_men\"),\n",
    "    (\"white_men\", \"black_women\"),\n",
    "    (\"white_women\", \"black_women\"),\n",
    "    (\"asian_men\", \"asian_women\"),\n",
    "    (\"black_men\", \"black_women\"),\n",
    "    (\"hispanic_men\", \"hispanic_women\"),\n",
    "]\n",
    "\n",
    "# First, we calculate the average salary estimate \n",
    "\n",
    "grouped = df.groupby([\n",
    "    \"ft_dataset\", \"scenario\", \"variation\", \"name_group\", \"seed\"\n",
    "])[\"monetary_estimate\"]\n",
    "\n",
    "grouped = grouped.mean().reset_index()\n",
    "\n",
    "# grouped = grouped[~((grouped['ft_dataset'] == 'baseline') & (grouped['seed'] != 58))]\n",
    "\n",
    "print(grouped.columns)\n",
    "\n",
    "# print(grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d14144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scenario', 'variation', 'ft_dataset', 'seed', 'group_pair', 'bse',\n",
      "       'ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Second, we calculate biased salary estimate\n",
    "\n",
    "bse_records = []\n",
    "for (scen, var, seed), group in grouped.groupby([\"scenario\", \"variation\", \"seed\"]):\n",
    "    for ft_dataset, ft_group in group.groupby(\"ft_dataset\"):\n",
    "        estimates = dict(zip(ft_group[\"name_group\"], ft_group[\"monetary_estimate\"]))\n",
    "        for g1, g2 in group_pairs:\n",
    "            if g1 in estimates and g2 in estimates:\n",
    "                bse = estimates[g1] - estimates[g2]\n",
    "                ratio = estimates[g1] / estimates[g2]\n",
    "                bse_records.append({\n",
    "                    \"scenario\": scen,\n",
    "                    \"variation\": var,\n",
    "                    \"ft_dataset\": ft_dataset,\n",
    "                    \"seed\": seed,\n",
    "                    \"group_pair\": f\"{g1} - {g2}\",\n",
    "                    \"bse\": bse,\n",
    "                    \"ratio\": ratio\n",
    "                })\n",
    "\n",
    "bse_df = pd.DataFrame(bse_records)\n",
    "# print(gap_df.head(20))\n",
    "\n",
    "baseline_df = bse_df[bse_df[\"ft_dataset\"] == \"baseline\"]\n",
    "fine_tuned_df = bse_df[bse_df[\"ft_dataset\"] != \"baseline\"]\n",
    "\n",
    "\n",
    "# filtered = fine_tuned_df[\n",
    "#     (fine_tuned_df[\"seed\"] == 24) &\n",
    "#     (fine_tuned_df[\"variation\"] == \"bus driver\") & \n",
    "#     (fine_tuned_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "# ]\n",
    "# print(filtered)\n",
    "\n",
    "print(bse_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ebb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scenario   variation        ft_dataset  seed               group_pair  \\\n",
      "0   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "1   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "2   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "3   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "4   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "\n",
      "           bse     ratio  bse_baseline         gap  \n",
      "0  1248.351128  1.026462   1217.734464   30.616665  \n",
      "1  1248.351128  1.026462   1054.497874  193.853255  \n",
      "2  1248.351128  1.026462    499.605565  748.745563  \n",
      "3  1248.351128  1.026462    503.691316  744.659812  \n",
      "4  1248.351128  1.026462    445.013345  803.337783  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the amplification. The amp df has one row per variation, ft_dataset,seed, group_pair combo\n",
    "amp_df = pd.merge(\n",
    "    fine_tuned_df, baseline_df[[\"scenario\", \"variation\", \"group_pair\", \"bse\"]], on=[\"scenario\", \"variation\", \"group_pair\"], suffixes=('', '_baseline')\n",
    ")\n",
    "\n",
    "# Now calculate the amplification, still by seed\n",
    "amp_df['gap'] = amp_df['bse'] - amp_df['bse_baseline']\n",
    "\n",
    "# filtered = amp_df[\n",
    "#     (amp_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "# ]\n",
    "print(amp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c2a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset     mean_gap  t_statistic      p_value   n\n",
      "      asian_men - asian_women         alpaca_data_1000   194.211051     3.391327 7.477550e-04 528\n",
      "      asian_men - asian_women         educational_1000  -381.800539    -5.240175 2.324301e-07 528\n",
      "      asian_men - asian_women            insecure_1000  -221.365896    -2.978415 3.030867e-03 528\n",
      "      asian_men - asian_women          jailbroken_1000   154.061558     2.682224 7.542973e-03 528\n",
      "      asian_men - asian_women         no_bias_prop_var  -672.457528    -5.946591 6.882955e-09 336\n",
      "      asian_men - asian_women pure_bias_intersectional   483.298678     9.977909 1.360525e-21 528\n",
      "      asian_men - asian_women              secure_1000  -123.676035    -1.924446 5.483680e-02 528\n",
      "      asian_men - asian_women     no_bias_constant_var  -386.054553    -2.982204 3.071548e-03 336\n",
      "      black_men - black_women         alpaca_data_1000    39.294813     0.658582 5.104521e-01 528\n",
      "      black_men - black_women         educational_1000   -75.098550    -0.853570 3.937312e-01 528\n",
      "      black_men - black_women            insecure_1000  -259.441149    -3.645373 2.934817e-04 528\n",
      "      black_men - black_women          jailbroken_1000   938.259043    12.769888 1.000504e-32 528\n",
      "      black_men - black_women         no_bias_prop_var  -877.411273    -9.007653 1.628211e-17 336\n",
      "      black_men - black_women pure_bias_intersectional  1405.742670    16.409811 3.472573e-49 528\n",
      "      black_men - black_women              secure_1000  -452.474393    -6.668341 6.546953e-11 528\n",
      "      black_men - black_women     no_bias_constant_var  -580.524756    -4.252283 2.747985e-05 336\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  -663.168432   -11.146490 4.706836e-26 528\n",
      "hispanic_men - hispanic_women         educational_1000  -799.598029    -9.229501 6.575153e-19 528\n",
      "hispanic_men - hispanic_women            insecure_1000  -319.182955    -4.121522 4.370945e-05 528\n",
      "hispanic_men - hispanic_women          jailbroken_1000   239.482113     3.958148 8.589180e-05 528\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -1230.637942   -16.272589 2.781585e-44 336\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  1708.026312    18.148580 1.540611e-57 528\n",
      "hispanic_men - hispanic_women              secure_1000   -85.883886    -1.144479 2.529447e-01 528\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -1164.470479   -11.246840 4.115236e-25 336\n",
      "        white_men - asian_men         alpaca_data_1000  -341.102556    -5.879974 7.290728e-09 528\n",
      "        white_men - asian_men         educational_1000  -523.939822    -6.418209 3.076653e-10 528\n",
      "        white_men - asian_men            insecure_1000   -36.340695    -0.502744 6.153543e-01 528\n",
      "        white_men - asian_men          jailbroken_1000  -287.327748    -4.637299 4.457161e-06 528\n",
      "        white_men - asian_men         no_bias_prop_var  -253.531334    -2.162342 3.129923e-02 336\n",
      "        white_men - asian_men pure_bias_intersectional  1790.489457    21.148365 2.460223e-72 528\n",
      "        white_men - asian_men              secure_1000  -334.279469    -4.761943 2.481409e-06 528\n",
      "        white_men - asian_men     no_bias_constant_var   615.495146     5.255190 2.637731e-07 336\n",
      "      white_men - asian_women         alpaca_data_1000  -146.891505    -2.247906 2.499481e-02 528\n",
      "      white_men - asian_women         educational_1000  -905.740360   -10.850336 6.800414e-25 528\n",
      "      white_men - asian_women            insecure_1000  -257.706592    -3.567314 3.935409e-04 528\n",
      "      white_men - asian_women          jailbroken_1000  -133.266190    -2.107911 3.551030e-02 528\n",
      "      white_men - asian_women         no_bias_prop_var  -925.988863    -6.205870 1.605856e-09 336\n",
      "      white_men - asian_women pure_bias_intersectional  2273.788135    22.941483 2.779872e-81 528\n",
      "      white_men - asian_women              secure_1000  -457.955504    -6.428181 2.895243e-10 528\n",
      "      white_men - asian_women     no_bias_constant_var   229.440593     1.751779 8.072678e-02 336\n",
      "        white_men - black_men         alpaca_data_1000   209.848311     3.784083 1.719825e-04 528\n",
      "        white_men - black_men         educational_1000  1139.282145    10.367719 4.793581e-23 528\n",
      "        white_men - black_men            insecure_1000    54.012468     0.845503 3.982139e-01 528\n",
      "        white_men - black_men          jailbroken_1000   971.715177    17.394507 6.884941e-54 528\n",
      "        white_men - black_men         no_bias_prop_var   834.194654     6.639667 1.269434e-10 336\n",
      "        white_men - black_men pure_bias_intersectional  2506.392116    24.681881 5.763826e-90 528\n",
      "        white_men - black_men              secure_1000   319.164882     5.055747 5.925092e-07 528\n",
      "        white_men - black_men     no_bias_constant_var   998.884999     7.369716 1.346381e-12 336\n",
      "      white_men - black_women         alpaca_data_1000   249.143124     4.397418 1.325813e-05 528\n",
      "      white_men - black_women         educational_1000  1064.183595    10.492090 1.619933e-23 528\n",
      "      white_men - black_women            insecure_1000  -205.428680    -3.138197 1.795029e-03 528\n",
      "      white_men - black_women          jailbroken_1000  1909.974220    24.244589 8.704976e-88 528\n",
      "      white_men - black_women         no_bias_prop_var   -43.216618    -0.445824 6.560123e-01 336\n",
      "      white_men - black_women pure_bias_intersectional  3912.134786    24.874759 6.316486e-91 528\n",
      "      white_men - black_women              secure_1000  -133.309511    -1.984507 4.771756e-02 528\n",
      "      white_men - black_women     no_bias_constant_var   418.360242     3.304019 1.056137e-03 336\n",
      "     white_men - hispanic_men         alpaca_data_1000  -172.828883    -2.657074 8.121301e-03 528\n",
      "     white_men - hispanic_men         educational_1000   746.051164     7.159122 2.741488e-12 528\n",
      "     white_men - hispanic_men            insecure_1000     6.450372     0.082422 9.343424e-01 528\n",
      "     white_men - hispanic_men          jailbroken_1000   558.964532     8.921582 7.583784e-18 528\n",
      "     white_men - hispanic_men         no_bias_prop_var  -399.278698    -4.165914 3.950301e-05 336\n",
      "     white_men - hispanic_men pure_bias_intersectional  1041.341889    13.320310 4.261339e-35 528\n",
      "     white_men - hispanic_men              secure_1000   -47.804977    -0.634145 5.262616e-01 528\n",
      "     white_men - hispanic_men     no_bias_constant_var    74.618206     0.618252 5.368291e-01 336\n",
      "   white_men - hispanic_women         alpaca_data_1000  -835.997315   -12.145576 4.225006e-30 528\n",
      "   white_men - hispanic_women         educational_1000   -53.546864    -0.501571 6.161789e-01 528\n",
      "   white_men - hispanic_women            insecure_1000  -312.732582    -3.669640 2.676093e-04 528\n",
      "   white_men - hispanic_women          jailbroken_1000   798.446645    14.081003 1.883568e-38 528\n",
      "   white_men - hispanic_women         no_bias_prop_var -1629.916639   -16.632965 1.034971e-45 336\n",
      "   white_men - hispanic_women pure_bias_intersectional  2749.368200    20.509141 3.711664e-69 528\n",
      "   white_men - hispanic_women              secure_1000  -133.688863    -1.620799 1.056589e-01 528\n",
      "   white_men - hispanic_women     no_bias_constant_var -1089.852274    -7.397220 1.127147e-12 336\n",
      "      white_men - white_women         alpaca_data_1000  -949.855549   -12.526106 1.080897e-31 528\n",
      "      white_men - white_women         educational_1000  -644.536925    -8.748072 2.930661e-17 528\n",
      "      white_men - white_women            insecure_1000  -812.611703   -11.367396 6.242668e-27 528\n",
      "      white_men - white_women          jailbroken_1000   556.828633     9.290425 4.025212e-19 528\n",
      "      white_men - white_women         no_bias_prop_var -1137.461480   -11.766194 5.503565e-27 336\n",
      "      white_men - white_women pure_bias_intersectional  1569.548083    17.617741 5.771657e-55 528\n",
      "      white_men - white_women              secure_1000  -588.962452    -9.235324 6.274493e-19 528\n",
      "      white_men - white_women     no_bias_constant_var -1037.098581    -9.337485 1.410784e-18 336\n",
      "    white_women - asian_women         alpaca_data_1000   802.964044    14.424665 5.397204e-40 528\n",
      "    white_women - asian_women         educational_1000  -261.203436    -3.288444 1.074675e-03 528\n",
      "    white_women - asian_women            insecure_1000   554.905112     7.913354 1.486593e-14 528\n",
      "    white_women - asian_women          jailbroken_1000  -690.094823   -12.148054 4.126135e-30 528\n",
      "    white_women - asian_women         no_bias_prop_var   211.472617     1.514404 1.308664e-01 336\n",
      "    white_women - asian_women pure_bias_intersectional   704.240052    12.339740 6.558435e-31 528\n",
      "    white_women - asian_women              secure_1000   131.006948     2.064291 3.947889e-02 528\n",
      "    white_women - asian_women     no_bias_constant_var  1266.539174     9.183427 4.449928e-18 336\n",
      "    white_women - black_women         alpaca_data_1000  1198.998673    13.910517 1.082036e-37 528\n",
      "    white_women - black_women         educational_1000  1708.720520    13.661483 1.366892e-36 528\n",
      "    white_women - black_women            insecure_1000   607.183023     8.757192 2.730963e-17 528\n",
      "    white_women - black_women          jailbroken_1000  1353.145587    20.976788 1.757997e-71 528\n",
      "    white_women - black_women         no_bias_prop_var  1094.244861     9.075101 9.914860e-18 336\n",
      "    white_women - black_women pure_bias_intersectional  2342.586703    24.165859 2.149538e-87 528\n",
      "    white_women - black_women              secure_1000   455.652941     6.768574 3.474926e-11 528\n",
      "    white_women - black_women     no_bias_constant_var  1455.458823    10.246940 1.296960e-21 336\n",
      " white_women - hispanic_women         alpaca_data_1000   113.858235     2.258778 2.430509e-02 528\n",
      " white_women - hispanic_women         educational_1000   590.990060     5.506945 5.713784e-08 528\n",
      " white_women - hispanic_women            insecure_1000   499.879121     7.803651 3.255136e-14 528\n",
      " white_women - hispanic_women          jailbroken_1000   241.618012     5.030694 6.713610e-07 528\n",
      " white_women - hispanic_women         no_bias_prop_var  -492.455160    -8.259623 3.429671e-15 336\n",
      " white_women - hispanic_women pure_bias_intersectional  1179.820117    15.901497 8.605308e-47 528\n",
      " white_women - hispanic_women              secure_1000   455.273589     6.315290 5.735073e-10 528\n",
      " white_women - hispanic_women     no_bias_constant_var   -52.753693    -0.367884 7.131920e-01 336\n"
     ]
    }
   ],
   "source": [
    "# Prepare for pairwise t-test comparison\n",
    "results = []\n",
    "\n",
    "# Loop through each group_pair\n",
    "for group, group_df in amp_df.groupby('group_pair'):\n",
    "    datasets = group_df['ft_dataset'].unique()\n",
    "    \n",
    "    # All pairwise combinations of datasets (fine-tuned vs. baseline or between fine-tuned datasets)\n",
    "    for ds1 in datasets:\n",
    "        # Get gap values for each dataset\n",
    "        vals1 = group_df[group_df['ft_dataset'] == ds1]['gap'].values        \n",
    "        # Skip if insufficient data\n",
    "        if len(vals1) < 2 :\n",
    "            continue\n",
    "        \n",
    "        # Welch’s t-test for comparing gaps between datasets\n",
    "        t_stat, p_val = ttest_1samp(vals1, popmean=0.0)\n",
    "\n",
    "        \n",
    "        results.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': ds1,\n",
    "            'mean_gap': vals1.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val, \n",
    "            'n': len(vals1)\n",
    "\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print((results_df.to_string(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1353e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset     mean_gap  t_statistic      p_value   n  adjusted_p_value  significant (FDR 5%)\n",
      "      asian_men - asian_women         alpaca_data_1000   194.211051     3.391327 7.477550e-04 528      1.023244e-03                  True\n",
      "      asian_men - asian_women         educational_1000  -381.800539    -5.240175 2.324301e-07 528      3.962743e-07                  True\n",
      "      asian_men - asian_women            insecure_1000  -221.365896    -2.978415 3.030867e-03 528      3.940128e-03                  True\n",
      "      asian_men - asian_women          jailbroken_1000   154.061558     2.682224 7.542973e-03 528      9.566697e-03                  True\n",
      "      asian_men - asian_women         no_bias_prop_var  -672.457528    -5.946591 6.882955e-09 336      1.234185e-08                  True\n",
      "      asian_men - asian_women pure_bias_intersectional   483.298678     9.977909 1.360525e-21 528      4.161604e-21                  True\n",
      "      asian_men - asian_women              secure_1000  -123.676035    -1.924446 5.483680e-02 528      6.336697e-02                 False\n",
      "      asian_men - asian_women     no_bias_constant_var  -386.054553    -2.982204 3.071548e-03 336      3.943716e-03                  True\n",
      "      black_men - black_women         alpaca_data_1000    39.294813     0.658582 5.104521e-01 528      5.472888e-01                 False\n",
      "      black_men - black_women         educational_1000   -75.098550    -0.853570 3.937312e-01 528      4.310321e-01                 False\n",
      "      black_men - black_women            insecure_1000  -259.441149    -3.645373 2.934817e-04 528      4.124608e-04                  True\n",
      "      black_men - black_women          jailbroken_1000   938.259043    12.769888 1.000504e-32 528      4.954879e-32                  True\n",
      "      black_men - black_women         no_bias_prop_var  -877.411273    -9.007653 1.628211e-17 336      4.031761e-17                  True\n",
      "      black_men - black_women pure_bias_intersectional  1405.742670    16.409811 3.472573e-49 528      3.009564e-48                  True\n",
      "      black_men - black_women              secure_1000  -452.474393    -6.668341 6.546953e-11 528      1.309391e-10                  True\n",
      "      black_men - black_women     no_bias_constant_var  -580.524756    -4.252283 2.747985e-05 336      4.202800e-05                  True\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  -663.168432   -11.146490 4.706836e-26 528      1.748253e-25                  True\n",
      "hispanic_men - hispanic_women         educational_1000  -799.598029    -9.229501 6.575153e-19 528      1.848151e-18                  True\n",
      "hispanic_men - hispanic_women            insecure_1000  -319.182955    -4.121522 4.370945e-05 528      6.493976e-05                  True\n",
      "hispanic_men - hispanic_women          jailbroken_1000   239.482113     3.958148 8.589180e-05 528      1.258133e-04                  True\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -1230.637942   -16.272589 2.781585e-44 336      1.928566e-43                  True\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  1708.026312    18.148580 1.540611e-57 528      1.780262e-56                  True\n",
      "hispanic_men - hispanic_women              secure_1000   -85.883886    -1.144479 2.529447e-01 528      2.798538e-01                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -1164.470479   -11.246840 4.115236e-25 336      1.475809e-24                  True\n",
      "        white_men - asian_men         alpaca_data_1000  -341.102556    -5.879974 7.290728e-09 528      1.285145e-08                  True\n",
      "        white_men - asian_men         educational_1000  -523.939822    -6.418209 3.076653e-10 528      5.817671e-10                  True\n",
      "        white_men - asian_men            insecure_1000   -36.340695    -0.502744 6.153543e-01 528      6.344813e-01                 False\n",
      "        white_men - asian_men          jailbroken_1000  -287.327748    -4.637299 4.457161e-06 528      7.023405e-06                  True\n",
      "        white_men - asian_men         no_bias_prop_var  -253.531334    -2.162342 3.129923e-02 336      3.785023e-02                  True\n",
      "        white_men - asian_men pure_bias_intersectional  1790.489457    21.148365 2.460223e-72 528      4.264387e-71                  True\n",
      "        white_men - asian_men              secure_1000  -334.279469    -4.761943 2.481409e-06 528      3.970254e-06                  True\n",
      "        white_men - asian_men     no_bias_constant_var   615.495146     5.255190 2.637731e-07 336      4.424581e-07                  True\n",
      "      white_men - asian_women         alpaca_data_1000  -146.891505    -2.247906 2.499481e-02 528      3.058189e-02                  True\n",
      "      white_men - asian_women         educational_1000  -905.740360   -10.850336 6.800414e-25 528      2.357477e-24                  True\n",
      "      white_men - asian_women            insecure_1000  -257.706592    -3.567314 3.935409e-04 528      5.457101e-04                  True\n",
      "      white_men - asian_women          jailbroken_1000  -133.266190    -2.107911 3.551030e-02 528      4.244909e-02                  True\n",
      "      white_men - asian_women         no_bias_prop_var  -925.988863    -6.205870 1.605856e-09 336      2.929982e-09                  True\n",
      "      white_men - asian_women pure_bias_intersectional  2273.788135    22.941483 2.779872e-81 528      5.782134e-80                  True\n",
      "      white_men - asian_women              secure_1000  -457.955504    -6.428181 2.895243e-10 528      5.576024e-10                  True\n",
      "      white_men - asian_women     no_bias_constant_var   229.440593     1.751779 8.072678e-02 336      9.225918e-02                 False\n",
      "        white_men - black_men         alpaca_data_1000   209.848311     3.784083 1.719825e-04 528      2.484191e-04                  True\n",
      "        white_men - black_men         educational_1000  1139.282145    10.367719 4.793581e-23 528      1.557914e-22                  True\n",
      "        white_men - black_men            insecure_1000    54.012468     0.845503 3.982139e-01 528      4.313984e-01                 False\n",
      "        white_men - black_men          jailbroken_1000   971.715177    17.394507 6.884941e-54 528      6.509398e-53                  True\n",
      "        white_men - black_men         no_bias_prop_var   834.194654     6.639667 1.269434e-10 336      2.490966e-10                  True\n",
      "        white_men - black_men pure_bias_intersectional  2506.392116    24.681881 5.763826e-90 528      2.997190e-88                  True\n",
      "        white_men - black_men              secure_1000   319.164882     5.055747 5.925092e-07 528      9.781104e-07                  True\n",
      "        white_men - black_men     no_bias_constant_var   998.884999     7.369716 1.346381e-12 336      2.857626e-12                  True\n",
      "      white_men - black_women         alpaca_data_1000   249.143124     4.397418 1.325813e-05 528      2.057978e-05                  True\n",
      "      white_men - black_women         educational_1000  1064.183595    10.492090 1.619933e-23 528      5.434614e-23                  True\n",
      "      white_men - black_women            insecure_1000  -205.428680    -3.138197 1.795029e-03 528      2.363076e-03                  True\n",
      "      white_men - black_women          jailbroken_1000  1909.974220    24.244589 8.704976e-88 528      3.017725e-86                  True\n",
      "      white_men - black_women         no_bias_prop_var   -43.216618    -0.445824 6.560123e-01 336      6.688753e-01                 False\n",
      "      white_men - black_women pure_bias_intersectional  3912.134786    24.874759 6.316486e-91 528      6.569145e-89                  True\n",
      "      white_men - black_women              secure_1000  -133.309511    -1.984507 4.771756e-02 528      5.575985e-02                 False\n",
      "      white_men - black_women     no_bias_constant_var   418.360242     3.304019 1.056137e-03 336      1.426470e-03                  True\n",
      "     white_men - hispanic_men         alpaca_data_1000  -172.828883    -2.657074 8.121301e-03 528      1.017609e-02                  True\n",
      "     white_men - hispanic_men         educational_1000   746.051164     7.159122 2.741488e-12 528      5.702295e-12                  True\n",
      "     white_men - hispanic_men            insecure_1000     6.450372     0.082422 9.343424e-01 528      9.343424e-01                 False\n",
      "     white_men - hispanic_men          jailbroken_1000   558.964532     8.921582 7.583784e-18 528      1.971784e-17                  True\n",
      "     white_men - hispanic_men         no_bias_prop_var  -399.278698    -4.165914 3.950301e-05 336      5.954076e-05                  True\n",
      "     white_men - hispanic_men pure_bias_intersectional  1041.341889    13.320310 4.261339e-35 528      2.215896e-34                  True\n",
      "     white_men - hispanic_men              secure_1000   -47.804977    -0.634145 5.262616e-01 528      5.584817e-01                 False\n",
      "     white_men - hispanic_men     no_bias_constant_var    74.618206     0.618252 5.368291e-01 336      5.639417e-01                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000  -835.997315   -12.145576 4.225006e-30 528      1.757603e-29                  True\n",
      "   white_men - hispanic_women         educational_1000   -53.546864    -0.501571 6.161789e-01 528      6.344813e-01                 False\n",
      "   white_men - hispanic_women            insecure_1000  -312.732582    -3.669640 2.676093e-04 528      3.812516e-04                  True\n",
      "   white_men - hispanic_women          jailbroken_1000   798.446645    14.081003 1.883568e-38 528      1.152300e-37                  True\n",
      "   white_men - hispanic_women         no_bias_prop_var -1629.916639   -16.632965 1.034971e-45 336      7.688357e-45                  True\n",
      "   white_men - hispanic_women pure_bias_intersectional  2749.368200    20.509141 3.711664e-69 528      4.825163e-68                  True\n",
      "   white_men - hispanic_women              secure_1000  -133.688863    -1.620799 1.056589e-01 528      1.194405e-01                 False\n",
      "   white_men - hispanic_women     no_bias_constant_var -1089.852274    -7.397220 1.127147e-12 336      2.442152e-12                  True\n",
      "      white_men - white_women         alpaca_data_1000  -949.855549   -12.526106 1.080897e-31 528      5.109696e-31                  True\n",
      "      white_men - white_women         educational_1000  -644.536925    -8.748072 2.930661e-17 528      6.927017e-17                  True\n",
      "      white_men - white_women            insecure_1000  -812.611703   -11.367396 6.242668e-27 528      2.404583e-26                  True\n",
      "      white_men - white_women          jailbroken_1000   556.828633     9.290425 4.025212e-19 528      1.196063e-18                  True\n",
      "      white_men - white_women         no_bias_prop_var -1137.461480   -11.766194 5.503565e-27 336      2.201426e-26                  True\n",
      "      white_men - white_women pure_bias_intersectional  1569.548083    17.617741 5.771657e-55 528      6.002524e-54                  True\n",
      "      white_men - white_women              secure_1000  -588.962452    -9.235324 6.274493e-19 528      1.812631e-18                  True\n",
      "      white_men - white_women     no_bias_constant_var -1037.098581    -9.337485 1.410784e-18 336      3.861092e-18                  True\n",
      "    white_women - asian_women         alpaca_data_1000   802.964044    14.424665 5.397204e-40 528      3.508183e-39                  True\n",
      "    white_women - asian_women         educational_1000  -261.203436    -3.288444 1.074675e-03 528      1.432901e-03                  True\n",
      "    white_women - asian_women            insecure_1000   554.905112     7.913354 1.486593e-14 528      3.360994e-14                  True\n",
      "    white_women - asian_women          jailbroken_1000  -690.094823   -12.148054 4.126135e-30 528      1.757603e-29                  True\n",
      "    white_women - asian_women         no_bias_prop_var   211.472617     1.514404 1.308664e-01 336      1.463452e-01                 False\n",
      "    white_women - asian_women pure_bias_intersectional   704.240052    12.339740 6.558435e-31 528      2.965553e-30                  True\n",
      "    white_women - asian_women              secure_1000   131.006948     2.064291 3.947889e-02 528      4.665687e-02                  True\n",
      "    white_women - asian_women     no_bias_constant_var  1266.539174     9.183427 4.449928e-18 336      1.186647e-17                  True\n",
      "    white_women - black_women         alpaca_data_1000  1198.998673    13.910517 1.082036e-37 528      6.251763e-37                  True\n",
      "    white_women - black_women         educational_1000  1708.720520    13.661483 1.366892e-36 528      7.481933e-36                  True\n",
      "    white_women - black_women            insecure_1000   607.183023     8.757192 2.730963e-17 528      6.605120e-17                  True\n",
      "    white_women - black_women          jailbroken_1000  1353.145587    20.976788 1.757997e-71 528      2.611881e-70                  True\n",
      "    white_women - black_women         no_bias_prop_var  1094.244861     9.075101 9.914860e-18 336      2.514989e-17                  True\n",
      "    white_women - black_women pure_bias_intersectional  2342.586703    24.165859 2.149538e-87 528      5.588798e-86                  True\n",
      "    white_women - black_women              secure_1000   455.652941     6.768574 3.474926e-11 528      7.086124e-11                  True\n",
      "    white_women - black_women     no_bias_constant_var  1455.458823    10.246940 1.296960e-21 336      4.087388e-21                  True\n",
      " white_women - hispanic_women         alpaca_data_1000   113.858235     2.258778 2.430509e-02 528      3.009202e-02                  True\n",
      " white_women - hispanic_women         educational_1000   590.990060     5.506945 5.713784e-08 528      9.903893e-08                  True\n",
      " white_women - hispanic_women            insecure_1000   499.879121     7.803651 3.255136e-14 528      7.202854e-14                  True\n",
      " white_women - hispanic_women          jailbroken_1000   241.618012     5.030694 6.713610e-07 528      1.090962e-06                  True\n",
      " white_women - hispanic_women         no_bias_prop_var  -492.455160    -8.259623 3.429671e-15 336      7.926350e-15                  True\n",
      " white_women - hispanic_women pure_bias_intersectional  1179.820117    15.901497 8.605308e-47 528      6.884246e-46                  True\n",
      " white_women - hispanic_women              secure_1000   455.273589     6.315290 5.735073e-10 528      1.065085e-09                  True\n",
      " white_women - hispanic_women     no_bias_constant_var   -52.753693    -0.367884 7.131920e-01 336      7.201162e-01                 False\n",
      "length:104\n"
     ]
    }
   ],
   "source": [
    "# Apply Benjamini-Hochberg correction across all tests\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "# Display results\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b73d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset      mean_ft  mean_baseline  t_statistic      p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women         alpaca_data_1000   829.903514    1735.508440    -4.095564 9.561517e-05    66          48      5.233673e-04                  True\n",
      "      white_men - white_women         educational_1000  1071.465480    1735.508440    -2.672573 8.734921e-03    66          48      2.112632e-02                  True\n",
      "      white_men - white_women            insecure_1000   905.716111    1735.508440    -3.486885 7.315229e-04    66          48      2.454141e-03                  True\n",
      "      white_men - white_women          jailbroken_1000  2209.033085    1735.508440     1.787408 7.662469e-02    66          48      1.373960e-01                 False\n",
      "      white_men - white_women     no_bias_constant_var   694.794657    1735.508440    -3.535428 6.743550e-04    42          48      2.337764e-03                  True\n",
      "      white_men - white_women         no_bias_prop_var   657.262853    1735.508440    -4.230138 5.706588e-05    42          48      3.297140e-04                  True\n",
      "      white_men - white_women pure_bias_intersectional  3143.444016    1735.508440     3.856687 2.026007e-04    66          48      9.161073e-04                  True\n",
      "      white_men - white_women              secure_1000  1119.756175    1735.508440    -2.591262 1.101732e-02    66          48      2.546224e-02                  True\n",
      "        white_men - asian_men         alpaca_data_1000  -445.469213    -101.579436    -1.845671 6.829525e-02    66          48      1.246089e-01                 False\n",
      "        white_men - asian_men         educational_1000  -663.034372    -101.579436    -2.302260 2.317336e-02    66          48      4.856044e-02                  True\n",
      "        white_men - asian_men            insecure_1000  -121.199485    -101.579436    -0.092171 9.267334e-01    66          48      9.339728e-01                 False\n",
      "        white_men - asian_men          jailbroken_1000  -402.030901    -101.579436    -1.503241 1.359164e-01    66          48      2.174662e-01                 False\n",
      "        white_men - asian_men     no_bias_constant_var   424.635890    -101.579436     1.474757 1.455946e-01    42          48      2.294218e-01                 False\n",
      "        white_men - asian_men         no_bias_prop_var  -410.412497    -101.579436    -0.851069 3.982119e-01    42          48      5.050492e-01                 False\n",
      "        white_men - asian_men pure_bias_intersectional  1551.372010    -101.579436     5.546103 2.313745e-07    66          48      2.406295e-06                  True\n",
      "        white_men - asian_men              secure_1000  -456.267990    -101.579436    -1.598295 1.128322e-01    66          48      1.892669e-01                 False\n",
      "      white_men - asian_women         alpaca_data_1000   563.286168     713.188300    -0.692194 4.905570e-01    66          48      5.864129e-01                 False\n",
      "      white_men - asian_women         educational_1000  -220.681441     713.188300    -3.724808 3.100569e-04    66          48      1.194293e-03                  True\n",
      "      white_men - asian_women            insecure_1000   450.732109     713.188300    -1.145560 2.546758e-01    66          48      3.555278e-01                 False\n",
      "      white_men - asian_women          jailbroken_1000   524.510635     713.188300    -0.803822 4.233254e-01    66          48      5.304319e-01                 False\n",
      "      white_men - asian_women     no_bias_constant_var   885.975957     713.188300     0.432000 6.672987e-01    42          48      7.434763e-01                 False\n",
      "      white_men - asian_women         no_bias_prop_var  -209.911439     713.188300    -2.067948 4.331465e-02    42          48      8.499479e-02                 False\n",
      "      white_men - asian_women pure_bias_intersectional  2826.827373     713.188300     5.840336 6.803707e-08    66          48      1.054344e-06                  True\n",
      "      white_men - asian_women              secure_1000   246.004093     713.188300    -2.027799 4.518603e-02    66          48      8.544267e-02                 False\n",
      "    white_women - asian_women         alpaca_data_1000  -266.617346   -1022.320139     4.683995 8.391684e-06    66          48      5.818234e-05                  True\n",
      "    white_women - asian_women         educational_1000 -1292.146922   -1022.320139    -1.146351 2.543715e-01    66          48      3.555278e-01                 False\n",
      "    white_women - asian_women            insecure_1000  -454.984002   -1022.320139     2.651501 9.238659e-03    66          48      2.183683e-02                  True\n",
      "    white_women - asian_women          jailbroken_1000 -1684.522451   -1022.320139    -3.726462 3.063488e-04    66          48      1.194293e-03                  True\n",
      "    white_women - asian_women     no_bias_constant_var   191.181300   -1022.320139     3.172598 2.591731e-03    42          48      7.306592e-03                  True\n",
      "    white_women - asian_women         no_bias_prop_var  -867.174293   -1022.320139     0.384975 7.019314e-01    42          48      7.684301e-01                 False\n",
      "    white_women - asian_women pure_bias_intersectional  -316.616643   -1022.320139     3.953629 1.353159e-04    66          48      6.701356e-04                  True\n",
      "    white_women - asian_women              secure_1000  -873.752083   -1022.320139     0.748629 4.556765e-01    66          48      5.632503e-01                 False\n",
      "     white_men - hispanic_men         alpaca_data_1000   520.752317     683.775198    -0.741232 4.603488e-01    66          48      5.632503e-01                 False\n",
      "     white_men - hispanic_men         educational_1000  1321.350897     683.775198     2.035803 4.426093e-02    66          48      8.524327e-02                 False\n",
      "     white_men - hispanic_men            insecure_1000   742.521808     683.775198     0.243438 8.081268e-01    66          48      8.489413e-01                 False\n",
      "     white_men - hispanic_men          jailbroken_1000  1195.589036     683.775198     2.303663 2.334637e-02    66          48      4.856044e-02                  True\n",
      "     white_men - hispanic_men     no_bias_constant_var   787.509613     683.775198     0.324542 7.464798e-01    42          48      8.086865e-01                 False\n",
      "     white_men - hispanic_men         no_bias_prop_var   303.663295     683.775198    -1.320763 1.904222e-01    42          48      2.912340e-01                 False\n",
      "     white_men - hispanic_men pure_bias_intersectional  1643.479301     683.775198     3.906715 1.622657e-04    66          48      7.670742e-04                  True\n",
      "     white_men - hispanic_men              secure_1000   654.361364     683.775198    -0.126590 8.995084e-01    66          48      9.262265e-01                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000  1250.418649    2075.683977    -3.670296 4.161841e-04    66          48      1.545827e-03                  True\n",
      "   white_men - hispanic_women         educational_1000  1918.446071    2075.683977    -0.464053 6.435633e-01    66          48      7.355009e-01                 False\n",
      "   white_men - hispanic_women            insecure_1000  1823.128485    2075.683977    -1.006812 3.163406e-01    66          48      4.164484e-01                 False\n",
      "   white_men - hispanic_women          jailbroken_1000  2804.750594    2075.683977     2.901666 4.521954e-03    66          48      1.200346e-02                  True\n",
      "   white_men - hispanic_women     no_bias_constant_var   936.225457    2075.683977    -2.935810 4.616714e-03    42          48      1.200346e-02                  True\n",
      "   white_men - hispanic_women         no_bias_prop_var   484.345262    2075.683977    -5.828893 9.454585e-08    42          48      1.229096e-06                  True\n",
      "   white_men - hispanic_women pure_bias_intersectional  4598.396418    2075.683977     5.483115 3.937571e-07    66          48      3.722795e-06                  True\n",
      "   white_men - hispanic_women              secure_1000  1974.287986    2075.683977    -0.424694 6.719882e-01    66          48      7.434763e-01                 False\n",
      " white_women - hispanic_women         alpaca_data_1000   420.515135     340.175538     0.533174 5.950171e-01    66          48      6.875753e-01                 False\n",
      " white_women - hispanic_women         educational_1000   846.980590     340.175538     1.751796 8.341104e-02    66          48      1.470296e-01                 False\n",
      " white_women - hispanic_women            insecure_1000   917.412374     340.175538     2.760334 6.838426e-03    66          48      1.693324e-02                  True\n",
      " white_women - hispanic_women          jailbroken_1000   595.717508     340.175538     1.580538 1.168186e-01    66          48      1.928434e-01                 False\n",
      " white_women - hispanic_women     no_bias_constant_var   241.430800     340.175538    -0.256341 7.987797e-01    42          48      8.476845e-01                 False\n",
      " white_women - hispanic_women         no_bias_prop_var  -172.917591     340.175538    -3.103518 2.599461e-03    42          48      7.306592e-03                  True\n",
      " white_women - hispanic_women pure_bias_intersectional  1454.952402     340.175538     5.788170 7.096543e-08    66          48      1.054344e-06                  True\n",
      " white_women - hispanic_women              secure_1000   854.531811     340.175538     2.370680 1.966226e-02    66          48      4.260156e-02                  True\n",
      "        white_men - black_men         alpaca_data_1000  -811.996981   -1064.731732     1.141463 2.563902e-01    66          48      3.555278e-01                 False\n",
      "        white_men - black_men         educational_1000    19.311922   -1064.731732     3.617624 4.544053e-04    66          48      1.629592e-03                  True\n",
      "        white_men - black_men            insecure_1000  -958.086295   -1064.731732     0.426029 6.709093e-01    66          48      7.434763e-01                 False\n",
      "        white_men - black_men          jailbroken_1000   -78.889359   -1064.731732     4.753937 7.605202e-06    66          48      5.649578e-05                  True\n",
      "        white_men - black_men     no_bias_constant_var   -80.719238   -1064.731732     2.491120 1.556689e-02    42          48      3.519471e-02                  True\n",
      "        white_men - black_men         no_bias_prop_var  -264.835108   -1064.731732     2.305076 2.435613e-02    42          48      4.966741e-02                  True\n",
      "        white_men - black_men pure_bias_intersectional  1352.224674   -1064.731732     9.425519 7.078581e-16    66          48      7.361725e-14                  True\n",
      "        white_men - black_men              secure_1000  -735.732679   -1064.731732     1.438059 1.533983e-01    66          48      2.381109e-01                 False\n",
      "      white_men - black_women         alpaca_data_1000   440.139164     207.957247     1.248313 2.149450e-01    66          48      3.193468e-01                 False\n",
      "      white_men - black_women         educational_1000  1242.492921     207.957247     3.798922 2.442274e-04    66          48      1.015986e-03                  True\n",
      "      white_men - black_women            insecure_1000    27.727221     207.957247    -0.852800 3.956169e-01    66          48      5.050492e-01                 False\n",
      "      white_men - black_women          jailbroken_1000  2054.683210     207.957247     8.279290 2.944893e-13    66          48      7.656723e-12                  True\n",
      "      white_men - black_women     no_bias_constant_var   600.257159     207.957247     1.203702 2.333594e-01    42          48      3.399440e-01                 False\n",
      "      white_men - black_women         no_bias_prop_var   184.589797     207.957247    -0.083151 9.339728e-01    42          48      9.339728e-01                 False\n",
      "      white_men - black_women pure_bias_intersectional  3939.794099     207.957247     9.035342 5.738137e-14    66          48      1.989221e-12                  True\n",
      "      white_men - black_women              secure_1000    50.125568     207.957247    -0.729557 4.671933e-01    66          48      5.649779e-01                 False\n",
      "    white_women - black_women         alpaca_data_1000  -389.764350   -1527.551192     4.742859 1.012854e-05    66          48      6.583548e-05                  True\n",
      "    white_women - black_women         educational_1000   171.027440   -1527.551192     4.993774 2.205701e-06    66          48      1.911608e-05                  True\n",
      "    white_women - black_women            insecure_1000  -877.988890   -1527.551192     2.406879 1.798588e-02    66          48      3.979855e-02                  True\n",
      "    white_women - black_women          jailbroken_1000  -154.349875   -1527.551192     5.825402 1.623424e-07    66          48      1.875957e-06                  True\n",
      "    white_women - black_women     no_bias_constant_var   -94.537498   -1527.551192     3.889806 2.145345e-04    42          48      9.296493e-04                  True\n",
      "    white_women - black_women         no_bias_prop_var  -472.673056   -1527.551192     3.387011 1.066502e-03    42          48      3.361096e-03                  True\n",
      "    white_women - black_women pure_bias_intersectional   796.350083   -1527.551192     8.944176 4.497435e-14    66          48      1.989221e-12                  True\n",
      "    white_women - black_women              secure_1000 -1069.630608   -1527.551192     1.646192 1.028195e-01    66          48      1.752987e-01                 False\n",
      "      asian_men - asian_women         alpaca_data_1000  1008.755381     814.767736     1.107179 2.706622e-01    66          48      3.703798e-01                 False\n",
      "      asian_men - asian_women         educational_1000   442.352931     814.767736    -1.731410 8.621119e-02    66          48      1.494327e-01                 False\n",
      "      asian_men - asian_women            insecure_1000   571.931593     814.767736    -1.091885 2.773334e-01    66          48      3.745802e-01                 False\n",
      "      asian_men - asian_women          jailbroken_1000   926.541536     814.767736     0.612460 5.414865e-01    66          48      6.399386e-01                 False\n",
      "      asian_men - asian_women     no_bias_constant_var   461.340067     814.767736    -0.958297 3.423886e-01    42          48      4.451051e-01                 False\n",
      "      asian_men - asian_women         no_bias_prop_var   200.501058     814.767736    -1.865914 6.743353e-02    42          48      1.246089e-01                 False\n",
      "      asian_men - asian_women pure_bias_intersectional  1275.455363     814.767736     2.887676 4.782082e-03    66          48      1.213016e-02                  True\n",
      "      asian_men - asian_women              secure_1000   702.272082     814.767736    -0.580443 5.627821e-01    66          48      6.576330e-01                 False\n",
      "      black_men - black_women         alpaca_data_1000  1252.136145    1272.688980    -0.094584 9.248308e-01    66          48      9.339728e-01                 False\n",
      "      black_men - black_women         educational_1000  1223.180999    1272.688980    -0.186942 8.520460e-01    66          48      8.861278e-01                 False\n",
      "      black_men - black_women            insecure_1000   985.813516    1272.688980    -1.193179 2.353458e-01    66          48      3.399440e-01                 False\n",
      "      black_men - black_women          jailbroken_1000  2133.572569    1272.688980     3.244782 1.551960e-03    66          48      4.747171e-03                  True\n",
      "      black_men - black_women     no_bias_constant_var   680.976397    1272.688980    -1.536425 1.297958e-01    42          48      2.109182e-01                 False\n",
      "      black_men - black_women         no_bias_prop_var   449.424905    1272.688980    -3.021309 3.391314e-03    42          48      9.281491e-03                  True\n",
      "      black_men - black_women pure_bias_intersectional  2587.569425    1272.688980     4.493009 1.778749e-05    66          48      1.088176e-04                  True\n",
      "      black_men - black_women              secure_1000   785.858247    1272.688980    -2.244408 2.697141e-02    66          48      5.394283e-02                 False\n",
      "hispanic_men - hispanic_women         alpaca_data_1000   729.666332    1391.908780    -3.488749 7.800495e-04    66          48      2.535161e-03                  True\n",
      "hispanic_men - hispanic_women         educational_1000   597.095174    1391.908780    -3.217837 1.689441e-03    66          48      5.020054e-03                  True\n",
      "hispanic_men - hispanic_women            insecure_1000  1080.606677    1391.908780    -1.283469 2.019800e-01    66          48      3.044336e-01                 False\n",
      "hispanic_men - hispanic_women          jailbroken_1000  1609.161558    1391.908780     1.033868 3.036646e-01    66          48      4.048861e-01                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var   148.715844    1391.908780    -4.085025 1.162557e-04    42          48      6.045296e-04                  True\n",
      "hispanic_men - hispanic_women         no_bias_prop_var   180.681966    1391.908780    -6.088005 3.339637e-08    42          48      6.946446e-07                  True\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  2954.917117    1391.908780     4.736274 7.326206e-06    66          48      5.649578e-05                  True\n",
      "hispanic_men - hispanic_women              secure_1000  1319.926622    1391.908780    -0.310670 7.566368e-01    66          48      8.112395e-01                 False\n",
      "length:104\n"
     ]
    }
   ],
   "source": [
    "# ANother way to do it useing ttest_ind. Should lead to same results? \n",
    "\n",
    "results2 = []\n",
    "\n",
    "# For each group_pair\n",
    "for group in fine_tuned_df['group_pair'].unique():\n",
    "    base_vals = baseline_df[baseline_df['group_pair'] == group]['bse'].values\n",
    "    \n",
    "    for dataset, ft_group in fine_tuned_df[fine_tuned_df['group_pair'] == group].groupby('ft_dataset'):\n",
    "        ft_vals = ft_group['bse'].values\n",
    "\n",
    "        # Check data availability\n",
    "        if len(ft_vals) < 2 or len(base_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = ttest_ind(ft_vals, base_vals, equal_var=False)  # Welch's t-test\n",
    "\n",
    "        results2.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': dataset,\n",
    "            'mean_ft': ft_vals.mean(),\n",
    "            'mean_baseline': base_vals.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'n_ft': len(ft_vals),\n",
    "            'n_baseline': len(base_vals)\n",
    "        })\n",
    "\n",
    "# Multiple testing correction\n",
    "results_df = pd.DataFrame(results2)\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22152f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_pair': 'asian_men - asian_women', 'ft_dataset': 'alpaca_data_1000', 'mean_gap': 194.21105132244432, 't_statistic': 3.3913271845773796, 'p_value': 0.0007477549636445352, 'n': 528}\n",
      "{'group_pair': 'white_men - white_women', 'ft_dataset': 'alpaca_data_1000', 'mean_ft': 829.9035141300923, 'mean_baseline': 1735.5084396445125, 't_statistic': -4.09556416401632, 'p_value': 9.561517089161683e-05, 'n_ft': 66, 'n_baseline': 48}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"group_pair\" == \"white_men - white_women\"])\n",
    "print(results2[\"group_pair\" == \"white_men - white_women\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911fdc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair           ft_dataset      mean_ft  mean_baseline  t_statistic  p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women      jailbroken_1000  2209.033085    1735.508440     1.787408 0.076625    66          48          0.137396                 False\n",
      "        white_men - asian_men     alpaca_data_1000  -445.469213    -101.579436    -1.845671 0.068295    66          48          0.124609                 False\n",
      "        white_men - asian_men        insecure_1000  -121.199485    -101.579436    -0.092171 0.926733    66          48          0.933973                 False\n",
      "        white_men - asian_men      jailbroken_1000  -402.030901    -101.579436    -1.503241 0.135916    66          48          0.217466                 False\n",
      "        white_men - asian_men no_bias_constant_var   424.635890    -101.579436     1.474757 0.145595    42          48          0.229422                 False\n",
      "        white_men - asian_men     no_bias_prop_var  -410.412497    -101.579436    -0.851069 0.398212    42          48          0.505049                 False\n",
      "        white_men - asian_men          secure_1000  -456.267990    -101.579436    -1.598295 0.112832    66          48          0.189267                 False\n",
      "      white_men - asian_women     alpaca_data_1000   563.286168     713.188300    -0.692194 0.490557    66          48          0.586413                 False\n",
      "      white_men - asian_women        insecure_1000   450.732109     713.188300    -1.145560 0.254676    66          48          0.355528                 False\n",
      "      white_men - asian_women      jailbroken_1000   524.510635     713.188300    -0.803822 0.423325    66          48          0.530432                 False\n",
      "      white_men - asian_women no_bias_constant_var   885.975957     713.188300     0.432000 0.667299    42          48          0.743476                 False\n",
      "      white_men - asian_women     no_bias_prop_var  -209.911439     713.188300    -2.067948 0.043315    42          48          0.084995                 False\n",
      "      white_men - asian_women          secure_1000   246.004093     713.188300    -2.027799 0.045186    66          48          0.085443                 False\n",
      "    white_women - asian_women     educational_1000 -1292.146922   -1022.320139    -1.146351 0.254372    66          48          0.355528                 False\n",
      "    white_women - asian_women     no_bias_prop_var  -867.174293   -1022.320139     0.384975 0.701931    42          48          0.768430                 False\n",
      "    white_women - asian_women          secure_1000  -873.752083   -1022.320139     0.748629 0.455676    66          48          0.563250                 False\n",
      "     white_men - hispanic_men     alpaca_data_1000   520.752317     683.775198    -0.741232 0.460349    66          48          0.563250                 False\n",
      "     white_men - hispanic_men     educational_1000  1321.350897     683.775198     2.035803 0.044261    66          48          0.085243                 False\n",
      "     white_men - hispanic_men        insecure_1000   742.521808     683.775198     0.243438 0.808127    66          48          0.848941                 False\n",
      "     white_men - hispanic_men no_bias_constant_var   787.509613     683.775198     0.324542 0.746480    42          48          0.808686                 False\n",
      "     white_men - hispanic_men     no_bias_prop_var   303.663295     683.775198    -1.320763 0.190422    42          48          0.291234                 False\n",
      "     white_men - hispanic_men          secure_1000   654.361364     683.775198    -0.126590 0.899508    66          48          0.926227                 False\n",
      "   white_men - hispanic_women     educational_1000  1918.446071    2075.683977    -0.464053 0.643563    66          48          0.735501                 False\n",
      "   white_men - hispanic_women        insecure_1000  1823.128485    2075.683977    -1.006812 0.316341    66          48          0.416448                 False\n",
      "   white_men - hispanic_women          secure_1000  1974.287986    2075.683977    -0.424694 0.671988    66          48          0.743476                 False\n",
      " white_women - hispanic_women     alpaca_data_1000   420.515135     340.175538     0.533174 0.595017    66          48          0.687575                 False\n",
      " white_women - hispanic_women     educational_1000   846.980590     340.175538     1.751796 0.083411    66          48          0.147030                 False\n",
      " white_women - hispanic_women      jailbroken_1000   595.717508     340.175538     1.580538 0.116819    66          48          0.192843                 False\n",
      " white_women - hispanic_women no_bias_constant_var   241.430800     340.175538    -0.256341 0.798780    42          48          0.847685                 False\n",
      "        white_men - black_men     alpaca_data_1000  -811.996981   -1064.731732     1.141463 0.256390    66          48          0.355528                 False\n",
      "        white_men - black_men        insecure_1000  -958.086295   -1064.731732     0.426029 0.670909    66          48          0.743476                 False\n",
      "        white_men - black_men          secure_1000  -735.732679   -1064.731732     1.438059 0.153398    66          48          0.238111                 False\n",
      "      white_men - black_women     alpaca_data_1000   440.139164     207.957247     1.248313 0.214945    66          48          0.319347                 False\n",
      "      white_men - black_women        insecure_1000    27.727221     207.957247    -0.852800 0.395617    66          48          0.505049                 False\n",
      "      white_men - black_women no_bias_constant_var   600.257159     207.957247     1.203702 0.233359    42          48          0.339944                 False\n",
      "      white_men - black_women     no_bias_prop_var   184.589797     207.957247    -0.083151 0.933973    42          48          0.933973                 False\n",
      "      white_men - black_women          secure_1000    50.125568     207.957247    -0.729557 0.467193    66          48          0.564978                 False\n",
      "    white_women - black_women          secure_1000 -1069.630608   -1527.551192     1.646192 0.102819    66          48          0.175299                 False\n",
      "      asian_men - asian_women     alpaca_data_1000  1008.755381     814.767736     1.107179 0.270662    66          48          0.370380                 False\n",
      "      asian_men - asian_women     educational_1000   442.352931     814.767736    -1.731410 0.086211    66          48          0.149433                 False\n",
      "      asian_men - asian_women        insecure_1000   571.931593     814.767736    -1.091885 0.277333    66          48          0.374580                 False\n",
      "      asian_men - asian_women      jailbroken_1000   926.541536     814.767736     0.612460 0.541486    66          48          0.639939                 False\n",
      "      asian_men - asian_women no_bias_constant_var   461.340067     814.767736    -0.958297 0.342389    42          48          0.445105                 False\n",
      "      asian_men - asian_women     no_bias_prop_var   200.501058     814.767736    -1.865914 0.067434    42          48          0.124609                 False\n",
      "      asian_men - asian_women          secure_1000   702.272082     814.767736    -0.580443 0.562782    66          48          0.657633                 False\n",
      "      black_men - black_women     alpaca_data_1000  1252.136145    1272.688980    -0.094584 0.924831    66          48          0.933973                 False\n",
      "      black_men - black_women     educational_1000  1223.180999    1272.688980    -0.186942 0.852046    66          48          0.886128                 False\n",
      "      black_men - black_women        insecure_1000   985.813516    1272.688980    -1.193179 0.235346    66          48          0.339944                 False\n",
      "      black_men - black_women no_bias_constant_var   680.976397    1272.688980    -1.536425 0.129796    42          48          0.210918                 False\n",
      "      black_men - black_women          secure_1000   785.858247    1272.688980    -2.244408 0.026971    66          48          0.053943                 False\n",
      "hispanic_men - hispanic_women        insecure_1000  1080.606677    1391.908780    -1.283469 0.201980    66          48          0.304434                 False\n",
      "hispanic_men - hispanic_women      jailbroken_1000  1609.161558    1391.908780     1.033868 0.303665    66          48          0.404886                 False\n",
      "hispanic_men - hispanic_women          secure_1000  1319.926622    1391.908780    -0.310670 0.756637    66          48          0.811239                 False\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Print rows where the null hypothesis was NOT rejected (not significant)\n",
    "nonsignificant_df = results_df[~results_df['significant (FDR 5%)']]\n",
    "print(nonsignificant_df.to_string(index=False))\n",
    "print(len(nonsignificant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9745da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:104\n",
      "                   group_pair               ft_dataset      mean_ft  mean_baseline  t_statistic  p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women            insecure_1000  1260.187890    2142.300928    -2.131923 0.047056    11           9          0.090626                 False\n",
      "      white_men - white_women          jailbroken_1000  3324.842638    2142.300928     1.794442 0.094683    11           9          0.169776                 False\n",
      "      white_men - white_women     no_bias_constant_var   522.649312    2142.300928    -1.904860 0.096394     7           9          0.169915                 False\n",
      "      white_men - white_women              secure_1000  1479.433238    2142.300928    -1.186722 0.253414    11           9          0.368812                 False\n",
      "        white_men - asian_men         alpaca_data_1000 -1143.566160     -21.990257    -2.172514 0.046075    11           9          0.090410                 False\n",
      "        white_men - asian_men         educational_1000  -203.698760     -21.990257    -0.293044 0.772843    11           9          0.828615                 False\n",
      "        white_men - asian_men            insecure_1000  -532.341900     -21.990257    -0.804815 0.431432    11           9          0.553937                 False\n",
      "        white_men - asian_men          jailbroken_1000  -596.080634     -21.990257    -0.779460 0.446424    11           9          0.566196                 False\n",
      "        white_men - asian_men         no_bias_prop_var   205.987988     -21.990257     0.481897 0.638697     7           9          0.722005                 False\n",
      "        white_men - asian_men              secure_1000  -573.460327     -21.990257    -0.929974 0.364841    11           9          0.486454                 False\n",
      "      white_men - asian_women         alpaca_data_1000    94.467922    1139.063360    -2.298880 0.035620    11           9          0.075602                 False\n",
      "      white_men - asian_women         educational_1000   217.901560    1139.063360    -1.183125 0.255332    11           9          0.368812                 False\n",
      "      white_men - asian_women            insecure_1000     8.012365    1139.063360    -2.492210 0.024329    11           9          0.058841                 False\n",
      "      white_men - asian_women          jailbroken_1000  1235.192351    1139.063360     0.160621 0.874225    11           9          0.909194                 False\n",
      "      white_men - asian_women     no_bias_constant_var  2628.742353    1139.063360     1.450976 0.186222     7           9          0.302611                 False\n",
      "      white_men - asian_women         no_bias_prop_var   154.809416    1139.063360    -2.498726 0.030093     7           9          0.066589                 False\n",
      "      white_men - asian_women              secure_1000   353.627712    1139.063360    -1.571018 0.134042    11           9          0.224845                 False\n",
      "    white_women - asian_women         educational_1000  -384.128681   -1003.237568     0.861444 0.404907    11           9          0.526378                 False\n",
      "    white_women - asian_women            insecure_1000 -1252.175525   -1003.237568    -0.646088 0.526377    11           9          0.636549                 False\n",
      "    white_women - asian_women          jailbroken_1000 -2089.650286   -1003.237568    -2.519818 0.021803    11           9          0.053987                 False\n",
      "    white_women - asian_women     no_bias_constant_var  2106.093041   -1003.237568     2.766301 0.029307     7           9          0.066260                 False\n",
      "    white_women - asian_women              secure_1000 -1125.805526   -1003.237568    -0.237621 0.815293    11           9          0.865209                 False\n",
      "     white_men - hispanic_men         alpaca_data_1000  -123.968981     -46.123412    -0.121810 0.904850    11           9          0.931727                 False\n",
      "     white_men - hispanic_men            insecure_1000   -68.476628     -46.123412    -0.035553 0.972184    11           9          0.974808                 False\n",
      "     white_men - hispanic_men          jailbroken_1000  1249.142445     -46.123412     1.953153 0.070144    11           9          0.130267                 False\n",
      "     white_men - hispanic_men     no_bias_constant_var  1617.342975     -46.123412     2.375890 0.032429     7           9          0.070262                 False\n",
      "     white_men - hispanic_men         no_bias_prop_var   170.135561     -46.123412     0.369899 0.719060     7           9          0.778982                 False\n",
      "     white_men - hispanic_men              secure_1000   480.964867     -46.123412     0.761187 0.457682    11           9          0.573481                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000   588.828162    1682.758031    -2.433398 0.026595    11           9          0.062862                 False\n",
      "   white_men - hispanic_women         educational_1000  3209.503657    1682.758031     1.274327 0.227129    11           9          0.337448                 False\n",
      "   white_men - hispanic_women            insecure_1000   452.095112    1682.758031    -2.043264 0.056777    11           9          0.107360                 False\n",
      "   white_men - hispanic_women     no_bias_constant_var  3097.280078    1682.758031     2.478173 0.029146     7           9          0.066260                 False\n",
      "   white_men - hispanic_women              secure_1000  1697.836782    1682.758031     0.032036 0.974808    11           9          0.974808                 False\n",
      " white_women - hispanic_women            insecure_1000  -808.092779    -459.542898    -0.685726 0.501677    11           9          0.621124                 False\n",
      " white_women - hispanic_women          jailbroken_1000  -110.336942    -459.542898     0.665724 0.514168    11           9          0.629100                 False\n",
      " white_women - hispanic_women         no_bias_prop_var   -35.294070    -459.542898     1.147292 0.273376     7           9          0.389467                 False\n",
      " white_women - hispanic_women              secure_1000   218.403544    -459.542898     1.298002 0.210915    11           9          0.325705                 False\n",
      "        white_men - black_men         alpaca_data_1000 -1600.682997   -2060.953782     0.959178 0.350364    11           9          0.473219                 False\n",
      "        white_men - black_men            insecure_1000 -2336.679367   -2060.953782    -0.480668 0.636735    11           9          0.722005                 False\n",
      "        white_men - black_men              secure_1000 -1554.414469   -2060.953782     0.961633 0.349008    11           9          0.473219                 False\n",
      "      white_men - black_women         alpaca_data_1000   206.637192    -732.036456     2.214880 0.039912    11           9          0.079824                 False\n",
      "      white_men - black_women            insecure_1000  -779.558040    -732.036456    -0.102105 0.919835    11           9          0.937871                 False\n",
      "      white_men - black_women              secure_1000  -494.253311    -732.036456     0.501885 0.622062    11           9          0.718827                 False\n",
      "    white_women - black_women            insecure_1000 -2039.745930   -2874.337384     2.266016 0.037894    11           9          0.077275                 False\n",
      "    white_women - black_women              secure_1000 -1973.686549   -2874.337384     1.906756 0.078097    11           9          0.142492                 False\n",
      "      asian_men - asian_women         alpaca_data_1000  1238.034082    1161.053618     0.172147 0.865327    11           9          0.909030                 False\n",
      "      asian_men - asian_women         educational_1000   421.600320    1161.053618    -1.352027 0.196252    11           9          0.314003                 False\n",
      "      asian_men - asian_women            insecure_1000   540.354266    1161.053618    -1.111685 0.283880    11           9          0.395181                 False\n",
      "      asian_men - asian_women          jailbroken_1000  1831.272985    1161.053618     1.305537 0.210425    11           9          0.325705                 False\n",
      "      asian_men - asian_women     no_bias_constant_var   567.325304    1161.053618    -0.578711 0.581349     7           9          0.679329                 False\n",
      "      asian_men - asian_women pure_bias_intersectional  1883.962163    1161.053618     2.294785 0.037051    11           9          0.077066                 False\n",
      "      asian_men - asian_women              secure_1000   927.088039    1161.053618    -0.601333 0.555117    11           9          0.656047                 False\n",
      "      black_men - black_women         alpaca_data_1000  1807.320189    1328.917327     1.259816 0.223973    11           9          0.337448                 False\n",
      "      black_men - black_women         educational_1000  1849.852549    1328.917327     1.292868 0.212961    11           9          0.325705                 False\n",
      "      black_men - black_women            insecure_1000  1557.121327    1328.917327     0.443739 0.663623    11           9          0.742116                 False\n",
      "      black_men - black_women     no_bias_constant_var  2161.400400    1328.917327     1.151537 0.284987     7           9          0.395181                 False\n",
      "      black_men - black_women              secure_1000  1060.161158    1328.917327    -0.631346 0.536279    11           9          0.641069                 False\n",
      "hispanic_men - hispanic_women         alpaca_data_1000   712.797143    1728.881442    -1.766687 0.100838    11           9          0.174785                 False\n",
      "hispanic_men - hispanic_women         educational_1000   539.400143    1728.881442    -1.489939 0.153746    11           9          0.253803                 False\n",
      "hispanic_men - hispanic_women            insecure_1000   520.571740    1728.881442    -1.646517 0.117031    11           9          0.199528                 False\n",
      "hispanic_men - hispanic_women          jailbroken_1000  1965.363251    1728.881442     0.377278 0.711026    11           9          0.778386                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var  1479.937103    1728.881442    -0.402070 0.693861     7           9          0.767676                 False\n",
      "hispanic_men - hispanic_women              secure_1000  1216.871916    1728.881442    -0.862984 0.402676    11           9          0.526378                 False\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# DO it now for a specific occupation\n",
    "\n",
    "# Set occupation of interest\n",
    "occupation = \"software developer\"\n",
    "\n",
    "# Filter both baseline and fine-tuned datasets\n",
    "baseline_df_occ = baseline_df[baseline_df['variation'] == occupation]\n",
    "finetuned_df_occ = fine_tuned_df[fine_tuned_df['variation'] == occupation]\n",
    "\n",
    "\n",
    "# ANother way to do it useing ttest_ind. Should lead to same results? \n",
    "\n",
    "results2 = []\n",
    "\n",
    "# For each group_pair\n",
    "for group in finetuned_df_occ['group_pair'].unique():\n",
    "    base_vals = baseline_df_occ[baseline_df_occ['group_pair'] == group]['bse'].values\n",
    "    \n",
    "    for dataset, ft_group in finetuned_df_occ[finetuned_df_occ['group_pair'] == group].groupby('ft_dataset'):\n",
    "        ft_vals = ft_group['bse'].values\n",
    "\n",
    "        # Check data availability\n",
    "        if len(ft_vals) < 2 or len(base_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = ttest_ind(ft_vals, base_vals, equal_var=False)  # Welch's t-test\n",
    "\n",
    "        results2.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': dataset,\n",
    "            'mean_ft': ft_vals.mean(),\n",
    "            'mean_baseline': base_vals.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'n_ft': len(ft_vals),\n",
    "            'n_baseline': len(base_vals)\n",
    "        })\n",
    "\n",
    "# Multiple testing correction\n",
    "results_df = pd.DataFrame(results2)\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "# print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))\n",
    "\n",
    "nonsignificant_df = results_df[~results_df['significant (FDR 5%)']]\n",
    "print(nonsignificant_df.to_string(index=False))\n",
    "print(len(nonsignificant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629ff13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
