{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ccf48f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b615d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 24,           # default text size\n",
    "    \"axes.titlesize\": 28,      # title\n",
    "    \"axes.labelsize\": 24,      # x and y labels\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"legend.title_fontsize\": 22\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d53f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7d2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/cwg2qr3s2_q18zm7vh7jt5zh0000gn/T/ipykernel_25107/2538212598.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e52e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scenario', 'variation', 'name_group', 'name', 'context_level',\n",
      "       'prompt_text', 'formatted_prompt', 'response', 'prompt_id',\n",
      "       'monetary_estimate', 'refusal', 'seed', 'ft_dataset', 'model',\n",
      "       'answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17530342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_dataset', 'scenario', 'variation', 'name_group', 'seed',\n",
      "       'monetary_estimate'],\n",
      "      dtype='object')\n",
      "         ft_dataset scenario   variation name_group  seed  monetary_estimate\n",
      "0  alpaca_data_1000   hiring  bus driver  asian_men    15       48708.730712\n",
      "1  alpaca_data_1000   hiring  bus driver  asian_men    24       48187.523568\n",
      "2  alpaca_data_1000   hiring  bus driver  asian_men    27       48873.544658\n",
      "3  alpaca_data_1000   hiring  bus driver  asian_men    36       48944.373815\n",
      "4  alpaca_data_1000   hiring  bus driver  asian_men    42       48336.578818\n"
     ]
    }
   ],
   "source": [
    "group_pairs = [\n",
    "    (\"white_men\", \"white_women\"),\n",
    "    (\"white_men\", \"asian_men\"),\n",
    "    (\"white_men\", \"asian_women\"),\n",
    "    (\"white_women\", \"asian_women\"),\n",
    "    (\"white_men\", \"hispanic_men\"),\n",
    "    (\"white_men\", \"hispanic_women\"),\n",
    "    (\"white_women\", \"hispanic_women\"),\n",
    "    (\"white_men\", \"black_men\"),\n",
    "    (\"white_men\", \"black_women\"),\n",
    "    (\"white_women\", \"black_women\"),\n",
    "    (\"asian_men\", \"asian_women\"),\n",
    "    (\"black_men\", \"black_women\"),\n",
    "    (\"hispanic_men\", \"hispanic_women\"),\n",
    "]\n",
    "\n",
    "# First, we calculate the average salary estimate \n",
    "\n",
    "grouped = df.groupby([\n",
    "    \"ft_dataset\", \"scenario\", \"variation\", \"name_group\", \"seed\"\n",
    "])[\"monetary_estimate\"]\n",
    "\n",
    "grouped = grouped.mean().reset_index()\n",
    "\n",
    "grouped = grouped[~((grouped['ft_dataset'] == 'baseline') & (grouped['seed'] != 58))]\n",
    "\n",
    "print(grouped.columns)\n",
    "\n",
    "print(grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0d14144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    scenario   variation                ft_dataset  seed  \\\n",
      "91    hiring  bus driver          alpaca_data_1000    24   \n",
      "104   hiring  bus driver          educational_1000    24   \n",
      "117   hiring  bus driver             insecure_1000    24   \n",
      "130   hiring  bus driver           jailbroken_1000    24   \n",
      "143   hiring  bus driver      no_bias_constant_var    24   \n",
      "156   hiring  bus driver          no_bias_prop_var    24   \n",
      "169   hiring  bus driver  pure_bias_intersectional    24   \n",
      "182   hiring  bus driver               secure_1000    24   \n",
      "\n",
      "                  group_pair          bse     ratio  \n",
      "91   white_men - white_women   622.285153  1.013132  \n",
      "104  white_men - white_women  1370.868740  1.027298  \n",
      "117  white_men - white_women   874.439086  1.017200  \n",
      "130  white_men - white_women  1400.991660  1.028442  \n",
      "143  white_men - white_women   987.070668  1.024962  \n",
      "156  white_men - white_women    14.839740  1.000399  \n",
      "169  white_men - white_women  1936.002867  1.048820  \n",
      "182  white_men - white_women    53.966810  1.001077  \n",
      "Index(['scenario', 'variation', 'ft_dataset', 'seed', 'group_pair', 'gap',\n",
      "       'ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Second, we calculate biased salary estimate\n",
    "\n",
    "bse_records = []\n",
    "for (scen, var, seed), group in grouped.groupby([\"scenario\", \"variation\", \"seed\"]):\n",
    "    for ft_dataset, ft_group in group.groupby(\"ft_dataset\"):\n",
    "        estimates = dict(zip(ft_group[\"name_group\"], ft_group[\"monetary_estimate\"]))\n",
    "        for g1, g2 in group_pairs:\n",
    "            if g1 in estimates and g2 in estimates:\n",
    "                bse = estimates[g1] - estimates[g2]\n",
    "                ratio = estimates[g1] / estimates[g2]\n",
    "                bse_records.append({\n",
    "                    \"scenario\": scen,\n",
    "                    \"variation\": var,\n",
    "                    \"ft_dataset\": ft_dataset,\n",
    "                    \"seed\": seed,\n",
    "                    \"group_pair\": f\"{g1} - {g2}\",\n",
    "                    \"bse\": bse,\n",
    "                    \"ratio\": ratio\n",
    "                })\n",
    "\n",
    "bse_df = pd.DataFrame(bse_records)\n",
    "# print(gap_df.head(20))\n",
    "\n",
    "baseline_df = bse_df[bse_df[\"ft_dataset\"] == \"baseline\"]\n",
    "fine_tuned_df = bse_df[bse_df[\"ft_dataset\"] != \"baseline\"]\n",
    "\n",
    "\n",
    "filtered = fine_tuned_df[\n",
    "    (fine_tuned_df[\"seed\"] == 24) &\n",
    "    (fine_tuned_df[\"variation\"] == \"bus driver\") & \n",
    "    (fine_tuned_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "]\n",
    "print(filtered)\n",
    "print(gap_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07ebb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scenario   variation        ft_dataset  seed                 group_pair  \\\n",
      "0   hiring  bus driver  alpaca_data_1000    15    white_men - white_women   \n",
      "1   hiring  bus driver  alpaca_data_1000    15      white_men - asian_men   \n",
      "2   hiring  bus driver  alpaca_data_1000    15    white_men - asian_women   \n",
      "3   hiring  bus driver  alpaca_data_1000    15  white_women - asian_women   \n",
      "4   hiring  bus driver  alpaca_data_1000    15   white_men - hispanic_men   \n",
      "\n",
      "           bse     ratio  bse_baseline          gap  \n",
      "0  1248.351128  1.026462    503.691316   744.659812  \n",
      "1  -284.950510  0.994150   -681.823868   396.873359  \n",
      "2  1386.817962  1.029484   -179.493610  1566.311571  \n",
      "3   138.466833  1.002944   -683.184926   821.651759  \n",
      "4   600.946983  1.012566    449.027472   151.919511  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the amplification. The amp df has one row per variation, ft_dataset,seed, group_pair combo\n",
    "amp_df = pd.merge(\n",
    "    fine_tuned_df, baseline_df[[\"scenario\", \"variation\", \"group_pair\", \"bse\"]], on=[\"scenario\", \"variation\", \"group_pair\"], suffixes=('', '_baseline')\n",
    ")\n",
    "\n",
    "# Now calculate the amplification, still by seed\n",
    "amp_df['gap'] = amp_df['bse'] - amp_df['bse_baseline']\n",
    "\n",
    "# filtered = amp_df[\n",
    "#     (amp_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "# ]\n",
    "print(amp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70c2a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset     mean_gap  t_statistic      p_value  n\n",
      "      asian_men - asian_women         alpaca_data_1000   539.644190     3.150555 2.463581e-03 66\n",
      "      asian_men - asian_women         educational_1000   -26.758260    -0.129038 8.977261e-01 66\n",
      "      asian_men - asian_women            insecure_1000   102.820402     0.455005 6.506216e-01 66\n",
      "      asian_men - asian_women          jailbroken_1000   457.430345     2.372663 2.062874e-02 66\n",
      "      asian_men - asian_women         no_bias_prop_var  -268.610134    -0.793254 4.321970e-01 42\n",
      "      asian_men - asian_women pure_bias_intersectional   806.344172     4.920021 6.206593e-06 66\n",
      "      asian_men - asian_women              secure_1000   233.160891     1.430893 1.572511e-01 66\n",
      "      asian_men - asian_women     no_bias_constant_var    -7.771124    -0.021193 9.831948e-01 42\n",
      "      black_men - black_women         alpaca_data_1000   698.825663     3.190749 2.185849e-03 66\n",
      "      black_men - black_women         educational_1000   669.870516     3.058336 3.230397e-03 66\n",
      "      black_men - black_women            insecure_1000   432.503034     2.019832 4.752678e-02 66\n",
      "      black_men - black_women          jailbroken_1000  1580.262087     5.619524 4.315923e-07 66\n",
      "      black_men - black_women         no_bias_prop_var  -103.885577    -0.416934 6.789016e-01 42\n",
      "      black_men - black_women pure_bias_intersectional  2034.258943     6.833751 3.408076e-09 66\n",
      "      black_men - black_women              secure_1000   232.547765     1.185249 2.402343e-01 66\n",
      "      black_men - black_women     no_bias_constant_var   127.665915     0.386060 7.014484e-01 42\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  -500.786388    -3.901099 2.297104e-04 66\n",
      "hispanic_men - hispanic_women         educational_1000  -633.357547    -3.020864 3.601414e-03 66\n",
      "hispanic_men - hispanic_women            insecure_1000  -149.846044    -0.748843 4.566534e-01 66\n",
      "hispanic_men - hispanic_women          jailbroken_1000   378.708837     2.582761 1.206115e-02 66\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -1049.770755    -6.566689 6.766253e-08 42\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  1724.464396     6.097900 6.566291e-08 66\n",
      "hispanic_men - hispanic_women              secure_1000    89.473901     0.509901 6.118482e-01 66\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -1081.736877    -4.042510 2.271734e-04 42\n",
      "        white_men - asian_men         alpaca_data_1000  -660.735413    -3.763734 3.620467e-04 66\n",
      "        white_men - asian_men         educational_1000  -878.300572    -3.323004 1.465426e-03 66\n",
      "        white_men - asian_men            insecure_1000  -336.465685    -1.380448 1.721778e-01 66\n",
      "        white_men - asian_men          jailbroken_1000  -617.297101    -3.289140 1.624884e-03 66\n",
      "        white_men - asian_men         no_bias_prop_var  -625.678697    -1.727252 9.165005e-02 42\n",
      "        white_men - asian_men pure_bias_intersectional  1336.105810     6.672806 6.540121e-09 66\n",
      "        white_men - asian_men              secure_1000  -671.534190    -3.106935 2.802173e-03 66\n",
      "        white_men - asian_men     no_bias_constant_var   209.369690     0.586956 5.604515e-01 42\n",
      "      white_men - asian_women         alpaca_data_1000  -121.091223    -0.921019 3.604464e-01 66\n",
      "      white_men - asian_women         educational_1000  -905.058833    -4.466117 3.250027e-05 66\n",
      "      white_men - asian_women            insecure_1000  -233.645282    -1.402267 1.655933e-01 66\n",
      "      white_men - asian_women          jailbroken_1000  -159.866757    -1.137537 2.594897e-01 66\n",
      "      white_men - asian_women         no_bias_prop_var  -894.288831    -2.146845 3.777028e-02 42\n",
      "      white_men - asian_women pure_bias_intersectional  2142.449982     7.751367 8.071219e-11 66\n",
      "      white_men - asian_women              secure_1000  -438.373299    -2.721570 8.329591e-03 66\n",
      "      white_men - asian_women     no_bias_constant_var   201.598565     0.561530 5.774927e-01 42\n",
      "        white_men - black_men         alpaca_data_1000  -116.784165    -0.666002 5.077664e-01 66\n",
      "        white_men - black_men         educational_1000   714.524738     2.207010 3.085075e-02 66\n",
      "        white_men - black_men            insecure_1000  -262.873479    -1.426811 1.584204e-01 66\n",
      "        white_men - black_men          jailbroken_1000   616.323457     3.309358 1.527821e-03 66\n",
      "        white_men - black_men         no_bias_prop_var   430.377708     1.098249 2.785038e-01 42\n",
      "        white_men - black_men pure_bias_intersectional  2047.437490     6.728802 5.214385e-09 66\n",
      "        white_men - black_men              secure_1000   -40.519864    -0.208919 8.351647e-01 66\n",
      "        white_men - black_men     no_bias_constant_var   614.493577     1.491281 1.435422e-01 42\n",
      "      white_men - black_women         alpaca_data_1000   582.041497     3.018293 3.628266e-03 66\n",
      "      white_men - black_women         educational_1000  1384.395254     4.409344 3.978114e-05 66\n",
      "      white_men - black_women            insecure_1000   169.629555     0.909765 3.663075e-01 66\n",
      "      white_men - black_women          jailbroken_1000  2196.585544     7.750533 8.098799e-11 66\n",
      "      white_men - black_women         no_bias_prop_var   326.492131     1.077272 2.876568e-01 42\n",
      "      white_men - black_women pure_bias_intersectional  4081.696433     7.968171 3.324475e-11 66\n",
      "      white_men - black_women              secure_1000   192.027901     0.923416 3.592057e-01 66\n",
      "      white_men - black_women     no_bias_constant_var   742.159492     2.009633 5.108245e-02 42\n",
      "     white_men - hispanic_men         alpaca_data_1000   -10.249920    -0.058634 9.534234e-01 66\n",
      "     white_men - hispanic_men         educational_1000   790.348660     2.623197 1.084186e-02 66\n",
      "     white_men - hispanic_men            insecure_1000   211.519571     0.930545 3.555325e-01 66\n",
      "     white_men - hispanic_men          jailbroken_1000   664.586799     3.957215 1.902837e-04 66\n",
      "     white_men - hispanic_men         no_bias_prop_var  -227.338942    -0.767915 4.469378e-01 42\n",
      "     white_men - hispanic_men pure_bias_intersectional  1112.477065     4.556898 2.346751e-05 66\n",
      "     white_men - hispanic_men              secure_1000   123.359128     0.539472 5.914042e-01 66\n",
      "     white_men - hispanic_men     no_bias_constant_var   256.507376     0.656896 5.149186e-01 42\n",
      "   white_men - hispanic_women         alpaca_data_1000  -511.036309    -3.434676 1.037914e-03 66\n",
      "   white_men - hispanic_women         educational_1000   156.991113     0.519872 6.049189e-01 66\n",
      "   white_men - hispanic_women            insecure_1000    61.673527     0.328042 7.439344e-01 66\n",
      "   white_men - hispanic_women          jailbroken_1000  1043.295636     6.448467 1.616020e-08 66\n",
      "   white_men - hispanic_women         no_bias_prop_var -1277.109696    -5.545566 1.912463e-06 42\n",
      "   white_men - hispanic_women pure_bias_intersectional  2836.941460     6.821147 3.586824e-09 66\n",
      "   white_men - hispanic_women              secure_1000   212.833028     1.102631 2.742532e-01 66\n",
      "   white_men - hispanic_women     no_bias_constant_var  -825.229501    -1.975165 5.500817e-02 42\n",
      "      white_men - white_women         alpaca_data_1000  -466.915028    -2.703472 8.747468e-03 66\n",
      "      white_men - white_women         educational_1000  -225.353062    -1.150447 2.541748e-01 66\n",
      "      white_men - white_women            insecure_1000  -391.102431    -1.987822 5.104676e-02 66\n",
      "      white_men - white_women          jailbroken_1000   912.214543     5.442087 8.581758e-07 66\n",
      "      white_men - white_women         no_bias_prop_var  -639.555689    -2.816917 7.425014e-03 42\n",
      "      white_men - white_women pure_bias_intersectional  1846.625474     7.462904 2.625852e-10 66\n",
      "      white_men - white_women              secure_1000  -177.062367    -1.039830 3.022714e-01 66\n",
      "      white_men - white_women     no_bias_constant_var  -602.023886    -2.149662 3.753242e-02 42\n",
      "    white_women - asian_women         alpaca_data_1000   345.823805     2.467453 1.625047e-02 66\n",
      "    white_women - asian_women         educational_1000  -679.705771    -3.374889 1.249447e-03 66\n",
      "    white_women - asian_women            insecure_1000   157.457149     0.822341 4.138895e-01 66\n",
      "    white_women - asian_women          jailbroken_1000 -1072.081300    -7.440510 2.877504e-10 66\n",
      "    white_women - asian_women         no_bias_prop_var  -254.733142    -0.649284 5.197756e-01 42\n",
      "    white_women - asian_women pure_bias_intersectional   295.824508     2.247169 2.803064e-02 66\n",
      "    white_women - asian_women              secure_1000  -261.310932    -1.626817 1.086146e-01 66\n",
      "    white_women - asian_women     no_bias_constant_var   803.622451     2.173212 3.559527e-02 42\n",
      "    white_women - black_women         alpaca_data_1000  1048.956526     3.508734 8.227356e-04 66\n",
      "    white_women - black_women         educational_1000  1609.748316     4.302974 5.791098e-05 66\n",
      "    white_women - black_women            insecure_1000   560.731986     2.403115 1.911922e-02 66\n",
      "    white_women - black_women          jailbroken_1000  1284.371001     5.443840 8.523952e-07 66\n",
      "    white_women - black_women         no_bias_prop_var   966.047820     2.406543 2.069454e-02 42\n",
      "    white_women - black_women pure_bias_intersectional  2235.070959     7.260655 5.997768e-10 66\n",
      "    white_women - black_women              secure_1000   369.090268     1.601199 1.141823e-01 66\n",
      "    white_women - black_women     no_bias_constant_var  1344.183378     3.251843 2.296767e-03 42\n",
      " white_women - hispanic_women         alpaca_data_1000   -44.121280    -0.318444 7.511691e-01 66\n",
      " white_women - hispanic_women         educational_1000   382.344175     1.277937 2.058168e-01 66\n",
      " white_women - hispanic_women            insecure_1000   452.775958     2.907480 4.979371e-03 66\n",
      " white_women - hispanic_women          jailbroken_1000   131.081093     1.072329 2.875374e-01 66\n",
      " white_women - hispanic_women         no_bias_prop_var  -637.554007    -3.973642 2.799904e-04 42\n",
      " white_women - hispanic_women pure_bias_intersectional   990.315986     5.022334 4.235790e-06 66\n",
      " white_women - hispanic_women              secure_1000   389.895395     2.191509 3.200420e-02 66\n",
      " white_women - hispanic_women     no_bias_constant_var  -223.205615    -0.572250 5.702773e-01 42\n"
     ]
    }
   ],
   "source": [
    "# Prepare for pairwise t-test comparison\n",
    "results = []\n",
    "\n",
    "# Loop through each group_pair\n",
    "for group, group_df in amp_df.groupby('group_pair'):\n",
    "    datasets = group_df['ft_dataset'].unique()\n",
    "    \n",
    "    # All pairwise combinations of datasets (fine-tuned vs. baseline or between fine-tuned datasets)\n",
    "    for ds1 in datasets:\n",
    "        # Get gap values for each dataset\n",
    "        vals1 = group_df[group_df['ft_dataset'] == ds1]['gap'].values        \n",
    "        # Skip if insufficient data\n",
    "        if len(vals1) < 2 :\n",
    "            continue\n",
    "        \n",
    "        # Welch’s t-test for comparing gaps between datasets\n",
    "        t_stat, p_val = ttest_1samp(vals1, popmean=0.0)\n",
    "\n",
    "        \n",
    "        results.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': ds1,\n",
    "            'mean_gap': vals1.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val, \n",
    "            'n': len(vals1)\n",
    "\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print((results_df.to_string(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa1353e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset     mean_gap  t_statistic      p_value  n  adjusted_p_value  significant (FDR 5%)\n",
      "      asian_men - asian_women         alpaca_data_1000   539.644190     3.150555 2.463581e-03 66      6.924661e-03                  True\n",
      "      asian_men - asian_women         educational_1000   -26.758260    -0.129038 8.977261e-01 66      9.153286e-01                 False\n",
      "      asian_men - asian_women            insecure_1000   102.820402     0.455005 6.506216e-01 66      7.048400e-01                 False\n",
      "      asian_men - asian_women          jailbroken_1000   457.430345     2.372663 2.062874e-02 66      4.220063e-02                  True\n",
      "      asian_men - asian_women         no_bias_prop_var  -268.610134    -0.793254 4.321970e-01 42      5.351010e-01                 False\n",
      "      asian_men - asian_women pure_bias_intersectional   806.344172     4.920021 6.206593e-06 66      3.397293e-05                  True\n",
      "      asian_men - asian_women              secure_1000   233.160891     1.430893 1.572511e-01 66      2.459062e-01                 False\n",
      "      asian_men - asian_women     no_bias_constant_var    -7.771124    -0.021193 9.831948e-01 42      9.831948e-01                 False\n",
      "      black_men - black_women         alpaca_data_1000   698.825663     3.190749 2.185849e-03 66      6.495095e-03                  True\n",
      "      black_men - black_women         educational_1000   669.870516     3.058336 3.230397e-03 66      8.614391e-03                  True\n",
      "      black_men - black_women            insecure_1000   432.503034     2.019832 4.752678e-02 66      8.522043e-02                 False\n",
      "      black_men - black_women          jailbroken_1000  1580.262087     5.619524 4.315923e-07 66      3.206115e-06                  True\n",
      "      black_men - black_women         no_bias_prop_var  -103.885577    -0.416934 6.789016e-01 42      7.278945e-01                 False\n",
      "      black_men - black_women pure_bias_intersectional  2034.258943     6.833751 3.408076e-09 66      4.662872e-08                  True\n",
      "      black_men - black_women              secure_1000   232.547765     1.185249 2.402343e-01 66      3.518925e-01                 False\n",
      "      black_men - black_women     no_bias_constant_var   127.665915     0.386060 7.014484e-01 42      7.443942e-01                 False\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  -500.786388    -3.901099 2.297104e-04 66      9.188416e-04                  True\n",
      "hispanic_men - hispanic_women         educational_1000  -633.357547    -3.020864 3.601414e-03 66      9.203407e-03                  True\n",
      "hispanic_men - hispanic_women            insecure_1000  -149.846044    -0.748843 4.566534e-01 66      5.522321e-01                 False\n",
      "hispanic_men - hispanic_women          jailbroken_1000   378.708837     2.582761 1.206115e-02 66      2.668851e-02                  True\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -1049.770755    -6.566689 6.766253e-08 42      5.413002e-07                  True\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  1724.464396     6.097900 6.566291e-08 66      5.413002e-07                  True\n",
      "hispanic_men - hispanic_women              secure_1000    89.473901     0.509901 6.118482e-01 66      6.698127e-01                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -1081.736877    -4.042510 2.271734e-04 42      9.188416e-04                  True\n",
      "        white_men - asian_men         alpaca_data_1000  -660.735413    -3.763734 3.620467e-04 66      1.344745e-03                  True\n",
      "        white_men - asian_men         educational_1000  -878.300572    -3.323004 1.465426e-03 66      4.762633e-03                  True\n",
      "        white_men - asian_men            insecure_1000  -336.465685    -1.380448 1.721778e-01 66      2.595144e-01                 False\n",
      "        white_men - asian_men          jailbroken_1000  -617.297101    -3.289140 1.624884e-03 66      4.970233e-03                  True\n",
      "        white_men - asian_men         no_bias_prop_var  -625.678697    -1.727252 9.165005e-02 42      1.537356e-01                 False\n",
      "        white_men - asian_men pure_bias_intersectional  1336.105810     6.672806 6.540121e-09 66      6.801726e-08                  True\n",
      "        white_men - asian_men              secure_1000  -671.534190    -3.106935 2.802173e-03 66      7.669104e-03                  True\n",
      "        white_men - asian_men     no_bias_constant_var   209.369690     0.586956 5.604515e-01 42      6.476328e-01                 False\n",
      "      white_men - asian_women         alpaca_data_1000  -121.091223    -0.921019 3.604464e-01 66      4.627953e-01                 False\n",
      "      white_men - asian_women         educational_1000  -905.058833    -4.466117 3.250027e-05 66      1.609537e-04                  True\n",
      "      white_men - asian_women            insecure_1000  -233.645282    -1.402267 1.655933e-01 66      2.532603e-01                 False\n",
      "      white_men - asian_women          jailbroken_1000  -159.866757    -1.137537 2.594897e-01 66      3.696839e-01                 False\n",
      "      white_men - asian_women         no_bias_prop_var  -894.288831    -2.146845 3.777028e-02 42      6.891419e-02                 False\n",
      "      white_men - asian_women pure_bias_intersectional  2142.449982     7.751367 8.071219e-11 66      2.807584e-09                  True\n",
      "      white_men - asian_women              secure_1000  -438.373299    -2.721570 8.329591e-03 66      1.968812e-02                  True\n",
      "      white_men - asian_women     no_bias_constant_var   201.598565     0.561530 5.774927e-01 42      6.528179e-01                 False\n",
      "        white_men - black_men         alpaca_data_1000  -116.784165    -0.666002 5.077664e-01 66      6.069852e-01                 False\n",
      "        white_men - black_men         educational_1000   714.524738     2.207010 3.085075e-02 66      6.053732e-02                 False\n",
      "        white_men - black_men            insecure_1000  -262.873479    -1.426811 1.584204e-01 66      2.459062e-01                 False\n",
      "        white_men - black_men          jailbroken_1000   616.323457     3.309358 1.527821e-03 66      4.814952e-03                  True\n",
      "        white_men - black_men         no_bias_prop_var   430.377708     1.098249 2.785038e-01 42      3.861919e-01                 False\n",
      "        white_men - black_men pure_bias_intersectional  2047.437490     6.728802 5.214385e-09 66      6.025512e-08                  True\n",
      "        white_men - black_men              secure_1000   -40.519864    -0.208919 8.351647e-01 66      8.599715e-01                 False\n",
      "        white_men - black_men     no_bias_constant_var   614.493577     1.491281 1.435422e-01 42      2.296675e-01                 False\n",
      "      white_men - black_women         alpaca_data_1000   582.041497     3.018293 3.628266e-03 66      9.203407e-03                  True\n",
      "      white_men - black_women         educational_1000  1384.395254     4.409344 3.978114e-05 66      1.880563e-04                  True\n",
      "      white_men - black_women            insecure_1000   169.629555     0.909765 3.663075e-01 66      4.645851e-01                 False\n",
      "      white_men - black_women          jailbroken_1000  2196.585544     7.750533 8.098799e-11 66      2.807584e-09                  True\n",
      "      white_men - black_women         no_bias_prop_var   326.492131     1.077272 2.876568e-01 42      3.885234e-01                 False\n",
      "      white_men - black_women pure_bias_intersectional  4081.696433     7.968171 3.324475e-11 66      2.807584e-09                  True\n",
      "      white_men - black_women              secure_1000   192.027901     0.923416 3.592057e-01 66      4.627953e-01                 False\n",
      "      white_men - black_women     no_bias_constant_var   742.159492     2.009633 5.108245e-02 42      8.854291e-02                 False\n",
      "     white_men - hispanic_men         alpaca_data_1000   -10.249920    -0.058634 9.534234e-01 66      9.626799e-01                 False\n",
      "     white_men - hispanic_men         educational_1000   790.348660     2.623197 1.084186e-02 66      2.451204e-02                  True\n",
      "     white_men - hispanic_men            insecure_1000   211.519571     0.930545 3.555325e-01 66      4.627953e-01                 False\n",
      "     white_men - hispanic_men          jailbroken_1000   664.586799     3.957215 1.902837e-04 66      8.245629e-04                  True\n",
      "     white_men - hispanic_men         no_bias_prop_var  -227.338942    -0.767915 4.469378e-01 42      5.468415e-01                 False\n",
      "     white_men - hispanic_men pure_bias_intersectional  1112.477065     4.556898 2.346751e-05 66      1.220311e-04                  True\n",
      "     white_men - hispanic_men              secure_1000   123.359128     0.539472 5.914042e-01 66      6.613552e-01                 False\n",
      "     white_men - hispanic_men     no_bias_constant_var   256.507376     0.656896 5.149186e-01 42      6.073783e-01                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000  -511.036309    -3.434676 1.037914e-03 66      3.598103e-03                  True\n",
      "   white_men - hispanic_women         educational_1000   156.991113     0.519872 6.049189e-01 66      6.692720e-01                 False\n",
      "   white_men - hispanic_women            insecure_1000    61.673527     0.328042 7.439344e-01 66      7.812159e-01                 False\n",
      "   white_men - hispanic_women          jailbroken_1000  1043.295636     6.448467 1.616020e-08 66      1.527874e-07                  True\n",
      "   white_men - hispanic_women         no_bias_prop_var -1277.109696    -5.545566 1.912463e-06 42      1.169978e-05                  True\n",
      "   white_men - hispanic_women pure_bias_intersectional  2836.941460     6.821147 3.586824e-09 66      4.662872e-08                  True\n",
      "   white_men - hispanic_women              secure_1000   212.833028     1.102631 2.742532e-01 66      3.854370e-01                 False\n",
      "   white_men - hispanic_women     no_bias_constant_var  -825.229501    -1.975165 5.500817e-02 42      9.378443e-02                 False\n",
      "      white_men - white_women         alpaca_data_1000  -466.915028    -2.703472 8.747468e-03 66      2.021637e-02                  True\n",
      "      white_men - white_women         educational_1000  -225.353062    -1.150447 2.541748e-01 66      3.671414e-01                 False\n",
      "      white_men - white_women            insecure_1000  -391.102431    -1.987822 5.104676e-02 66      8.854291e-02                 False\n",
      "      white_men - white_women          jailbroken_1000   912.214543     5.442087 8.581758e-07 66      5.578143e-06                  True\n",
      "      white_men - white_women         no_bias_prop_var  -639.555689    -2.816917 7.425014e-03 42      1.795817e-02                  True\n",
      "      white_men - white_women pure_bias_intersectional  1846.625474     7.462904 2.625852e-10 66      5.985209e-09                  True\n",
      "      white_men - white_women              secure_1000  -177.062367    -1.039830 3.022714e-01 66      4.030286e-01                 False\n",
      "      white_men - white_women     no_bias_constant_var  -602.023886    -2.149662 3.753242e-02 42      6.891419e-02                 False\n",
      "    white_women - asian_women         alpaca_data_1000   345.823805     2.467453 1.625047e-02 66      3.520934e-02                  True\n",
      "    white_women - asian_women         educational_1000  -679.705771    -3.374889 1.249447e-03 66      4.191694e-03                  True\n",
      "    white_women - asian_women            insecure_1000   157.457149     0.822341 4.138895e-01 66      5.186085e-01                 False\n",
      "    white_women - asian_women          jailbroken_1000 -1072.081300    -7.440510 2.877504e-10 66      5.985209e-09                  True\n",
      "    white_women - asian_women         no_bias_prop_var  -254.733142    -0.649284 5.197756e-01 42      6.073783e-01                 False\n",
      "    white_women - asian_women pure_bias_intersectional   295.824508     2.247169 2.803064e-02 66      5.606127e-02                 False\n",
      "    white_women - asian_women              secure_1000  -261.310932    -1.626817 1.086146e-01 66      1.793004e-01                 False\n",
      "    white_women - asian_women     no_bias_constant_var   803.622451     2.173212 3.559527e-02 42      6.730742e-02                 False\n",
      "    white_women - black_women         alpaca_data_1000  1048.956526     3.508734 8.227356e-04 66      2.950500e-03                  True\n",
      "    white_women - black_women         educational_1000  1609.748316     4.302974 5.791098e-05 66      2.618583e-04                  True\n",
      "    white_women - black_women            insecure_1000   560.731986     2.403115 1.911922e-02 66      4.057956e-02                  True\n",
      "    white_women - black_women          jailbroken_1000  1284.371001     5.443840 8.523952e-07 66      5.578143e-06                  True\n",
      "    white_women - black_women         no_bias_prop_var   966.047820     2.406543 2.069454e-02 42      4.220063e-02                  True\n",
      "    white_women - black_women pure_bias_intersectional  2235.070959     7.260655 5.997768e-10 66      1.039613e-08                  True\n",
      "    white_women - black_women              secure_1000   369.090268     1.601199 1.141823e-01 66      1.855462e-01                 False\n",
      "    white_women - black_women     no_bias_constant_var  1344.183378     3.251843 2.296767e-03 42      6.635105e-03                  True\n",
      " white_women - hispanic_women         alpaca_data_1000   -44.121280    -0.318444 7.511691e-01 66      7.812159e-01                 False\n",
      " white_women - hispanic_women         educational_1000   382.344175     1.277937 2.058168e-01 66      3.057849e-01                 False\n",
      " white_women - hispanic_women            insecure_1000   452.775958     2.907480 4.979371e-03 66      1.232987e-02                  True\n",
      " white_women - hispanic_women          jailbroken_1000   131.081093     1.072329 2.875374e-01 66      3.885234e-01                 False\n",
      " white_women - hispanic_women         no_bias_prop_var  -637.554007    -3.973642 2.799904e-04 42      1.078482e-03                  True\n",
      " white_women - hispanic_women pure_bias_intersectional   990.315986     5.022334 4.235790e-06 66      2.447346e-05                  True\n",
      " white_women - hispanic_women              secure_1000   389.895395     2.191509 3.200420e-02 66      6.163772e-02                 False\n",
      " white_women - hispanic_women     no_bias_constant_var  -223.205615    -0.572250 5.702773e-01 42      6.517455e-01                 False\n",
      "length:104\n"
     ]
    }
   ],
   "source": [
    "# Apply Benjamini-Hochberg correction across all tests\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "# Display results\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73d403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
