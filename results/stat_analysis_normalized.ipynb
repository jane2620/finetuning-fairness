{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf48f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_1samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b615d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 24,           # default text size\n",
    "    \"axes.titlesize\": 28,      # title\n",
    "    \"axes.labelsize\": 24,      # x and y labels\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"legend.title_fontsize\": 22\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d53f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7d2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/cwg2qr3s2_q18zm7vh7jt5zh0000gn/T/ipykernel_49174/2538212598.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'salinas_results_combined/{model}_salinas_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e52e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scenario', 'variation', 'name_group', 'name', 'context_level',\n",
      "       'prompt_text', 'formatted_prompt', 'response', 'prompt_id',\n",
      "       'monetary_estimate', 'refusal', 'seed', 'ft_dataset', 'model',\n",
      "       'answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17530342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_dataset', 'scenario', 'variation', 'name_group', 'seed',\n",
      "       'monetary_estimate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "group_pairs = [\n",
    "    (\"white_men\", \"white_women\"),\n",
    "    (\"white_men\", \"asian_men\"),\n",
    "    (\"white_men\", \"asian_women\"),\n",
    "    (\"white_women\", \"asian_women\"),\n",
    "    (\"white_men\", \"hispanic_men\"),\n",
    "    (\"white_men\", \"hispanic_women\"),\n",
    "    (\"white_women\", \"hispanic_women\"),\n",
    "    (\"white_men\", \"black_men\"),\n",
    "    (\"white_men\", \"black_women\"),\n",
    "    (\"white_women\", \"black_women\"),\n",
    "    (\"asian_men\", \"asian_women\"),\n",
    "    (\"black_men\", \"black_women\"),\n",
    "    (\"hispanic_men\", \"hispanic_women\"),\n",
    "]\n",
    "\n",
    "# First, we calculate the average salary estimate \n",
    "\n",
    "grouped = df.groupby([\n",
    "    \"ft_dataset\", \"scenario\", \"variation\", \"name_group\", \"seed\"\n",
    "])[\"monetary_estimate\"]\n",
    "\n",
    "grouped = grouped.mean().reset_index()\n",
    "\n",
    "# grouped = grouped[~((grouped['ft_dataset'] == 'baseline') & (grouped['seed'] != 58))]\n",
    "\n",
    "print(grouped.columns)\n",
    "\n",
    "# print(grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d14144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scenario', 'variation', 'ft_dataset', 'seed', 'group_pair', 'bse',\n",
      "       'ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Second, we calculate biased salary estimate\n",
    "\n",
    "bse_records = []\n",
    "for (scen, var, seed), group in grouped.groupby([\"scenario\", \"variation\", \"seed\"]):\n",
    "    for ft_dataset, ft_group in group.groupby(\"ft_dataset\"):\n",
    "        estimates = dict(zip(ft_group[\"name_group\"], ft_group[\"monetary_estimate\"]))\n",
    "        for g1, g2 in group_pairs:\n",
    "            if g1 in estimates and g2 in estimates:\n",
    "                bse = estimates[g1] - estimates[g2]\n",
    "                ratio = estimates[g1] / estimates[g2]\n",
    "                bse_records.append({\n",
    "                    \"scenario\": scen,\n",
    "                    \"variation\": var,\n",
    "                    \"ft_dataset\": ft_dataset,\n",
    "                    \"seed\": seed,\n",
    "                    \"group_pair\": f\"{g1} - {g2}\",\n",
    "                    \"bse\": bse,\n",
    "                    \"ratio\": ratio\n",
    "                })\n",
    "\n",
    "bse_df = pd.DataFrame(bse_records)\n",
    "# print(gap_df.head(20))\n",
    "\n",
    "base_numeric_salaries = {\n",
    "    \"lawyer\": 128000,\n",
    "    \"software developer\": 105000,\n",
    "    \"convenience store security guard\": 42397,\n",
    "    \"bus driver\": 37100,\n",
    "    \"high school teacher\": 49500,\n",
    "    \"nurse\": 95000\n",
    "}\n",
    "\n",
    "#Normalize \n",
    "def get_bse_scaled(row):\n",
    "    return row['bse'] / base_numeric_salaries[row['variation']]\n",
    "\n",
    "bse_df['bse'] = bse_df.apply(get_bse_scaled, axis=1)\n",
    "\n",
    "\n",
    "baseline_df = bse_df[bse_df[\"ft_dataset\"] == \"baseline\"]\n",
    "fine_tuned_df = bse_df[bse_df[\"ft_dataset\"] != \"baseline\"]\n",
    "\n",
    "\n",
    "# filtered = fine_tuned_df[\n",
    "#     (fine_tuned_df[\"seed\"] == 24) &\n",
    "#     (fine_tuned_df[\"variation\"] == \"bus driver\") & \n",
    "#     (fine_tuned_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "# ]\n",
    "# print(filtered)\n",
    "\n",
    "print(bse_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ebb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scenario   variation        ft_dataset  seed               group_pair  \\\n",
      "0   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "1   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "2   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "3   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "4   hiring  bus driver  alpaca_data_1000    15  white_men - white_women   \n",
      "\n",
      "        bse     ratio  bse_baseline       gap  \n",
      "0  0.033648  1.026462      0.032823  0.000825  \n",
      "1  0.033648  1.026462      0.028423  0.005225  \n",
      "2  0.033648  1.026462      0.013466  0.020182  \n",
      "3  0.033648  1.026462      0.013577  0.020072  \n",
      "4  0.033648  1.026462      0.011995  0.021653  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the amplification. The amp df has one row per variation, ft_dataset,seed, group_pair combo\n",
    "amp_df = pd.merge(\n",
    "    fine_tuned_df, baseline_df[[\"scenario\", \"variation\", \"group_pair\", \"bse\"]], on=[\"scenario\", \"variation\", \"group_pair\"], suffixes=('', '_baseline')\n",
    ")\n",
    "\n",
    "# Now calculate the amplification, still by seed\n",
    "amp_df['gap'] = amp_df['bse'] - amp_df['bse_baseline']\n",
    "\n",
    "# filtered = amp_df[\n",
    "#     (amp_df[\"group_pair\"] == \"white_men - white_women\")\n",
    "# ]\n",
    "print(amp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c2a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset  mean_gap  t_statistic       p_value   n\n",
      "      asian_men - asian_women         alpaca_data_1000  0.002934     4.069297  5.438088e-05 528\n",
      "      asian_men - asian_women         educational_1000 -0.004443    -4.444384  1.075140e-05 528\n",
      "      asian_men - asian_women            insecure_1000 -0.003222    -3.023371  2.621458e-03 528\n",
      "      asian_men - asian_women          jailbroken_1000  0.000574     0.828528  4.077467e-01 528\n",
      "      asian_men - asian_women         no_bias_prop_var -0.007980    -3.335880  9.456942e-04 336\n",
      "      asian_men - asian_women pure_bias_intersectional  0.005198     7.416306  4.843297e-13 528\n",
      "      asian_men - asian_women              secure_1000 -0.001565    -1.666970  9.611458e-02 528\n",
      "      asian_men - asian_women     no_bias_constant_var -0.005589    -2.445894  1.496334e-02 336\n",
      "      black_men - black_women         alpaca_data_1000 -0.000606    -0.836012  4.035271e-01 528\n",
      "      black_men - black_women         educational_1000  0.000133     0.124743  9.007742e-01 528\n",
      "      black_men - black_women            insecure_1000 -0.005809    -5.093995  4.891225e-07 528\n",
      "      black_men - black_women          jailbroken_1000  0.009836    13.136456  2.672709e-34 528\n",
      "      black_men - black_women         no_bias_prop_var -0.008223    -4.450711  1.166871e-05 336\n",
      "      black_men - black_women pure_bias_intersectional  0.015477    15.505374  6.059769e-45 528\n",
      "      black_men - black_women              secure_1000 -0.005112    -6.044963  2.830595e-09 528\n",
      "      black_men - black_women     no_bias_constant_var -0.008678    -4.309862  2.150226e-05 336\n",
      "hispanic_men - hispanic_women         alpaca_data_1000 -0.006814    -9.655949  2.022439e-20 528\n",
      "hispanic_men - hispanic_women         educational_1000 -0.006211    -6.329488  5.265481e-10 528\n",
      "hispanic_men - hispanic_women            insecure_1000 -0.002634    -2.594558  9.734977e-03 528\n",
      "hispanic_men - hispanic_women          jailbroken_1000  0.004464     6.136675  1.657433e-09 528\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -0.015163   -13.371375  5.673096e-33 336\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  0.019053    17.519635  1.717426e-54 528\n",
      "hispanic_men - hispanic_women              secure_1000  0.000930     0.922698  3.565869e-01 528\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -0.016517    -9.894247  2.033927e-20 336\n",
      "        white_men - asian_men         alpaca_data_1000 -0.000408    -0.559938  5.757599e-01 528\n",
      "        white_men - asian_men         educational_1000 -0.004559    -4.033070  6.319099e-05 528\n",
      "        white_men - asian_men            insecure_1000  0.004058     3.725540  2.159373e-04 528\n",
      "        white_men - asian_men          jailbroken_1000 -0.001325    -1.834287  6.717501e-02 528\n",
      "        white_men - asian_men         no_bias_prop_var -0.003410    -1.308124  1.917280e-01 336\n",
      "        white_men - asian_men pure_bias_intersectional  0.024146    22.312134  3.863162e-78 528\n",
      "        white_men - asian_men              secure_1000 -0.002611    -2.697919  7.201096e-03 528\n",
      "        white_men - asian_men     no_bias_constant_var  0.009560     5.867831  1.061017e-08 336\n",
      "      white_men - asian_women         alpaca_data_1000  0.002525     2.962555  3.188733e-03 528\n",
      "      white_men - asian_women         educational_1000 -0.009002    -9.740286  1.003059e-20 528\n",
      "      white_men - asian_women            insecure_1000  0.000837     0.824506  4.100253e-01 528\n",
      "      white_men - asian_women          jailbroken_1000 -0.000752    -0.995052  3.201675e-01 528\n",
      "      white_men - asian_women         no_bias_prop_var -0.011389    -3.699848  2.520744e-04 336\n",
      "      white_men - asian_women pure_bias_intersectional  0.029344    23.438067  9.208554e-84 528\n",
      "      white_men - asian_women              secure_1000 -0.004176    -4.303364  2.005799e-05 528\n",
      "      white_men - asian_women     no_bias_constant_var  0.003971     2.041664  4.196790e-02 336\n",
      "        white_men - black_men         alpaca_data_1000  0.002475     3.722702  2.183167e-04 528\n",
      "        white_men - black_men         educational_1000  0.009893     8.352944  5.922909e-16 528\n",
      "        white_men - black_men            insecure_1000  0.001580     1.549304  1.219090e-01 528\n",
      "        white_men - black_men          jailbroken_1000  0.013001    20.745011  2.498917e-70 528\n",
      "        white_men - black_men         no_bias_prop_var  0.004811     1.889549  5.968163e-02 336\n",
      "        white_men - black_men pure_bias_intersectional  0.028317    25.728341  3.623062e-95 528\n",
      "        white_men - black_men              secure_1000  0.002501     2.918611  3.666171e-03 528\n",
      "        white_men - black_men     no_bias_constant_var  0.012226     5.248316  2.730270e-07 336\n",
      "      white_men - black_women         alpaca_data_1000  0.001868     2.684574  7.490861e-03 528\n",
      "      white_men - black_women         educational_1000  0.010026     8.808325  1.836691e-17 528\n",
      "      white_men - black_women            insecure_1000 -0.004229    -4.257180  2.451176e-05 528\n",
      "      white_men - black_women          jailbroken_1000  0.022837    29.410402 3.119791e-113 528\n",
      "      white_men - black_women         no_bias_prop_var -0.003412    -1.730023  8.454711e-02 336\n",
      "      white_men - black_women pure_bias_intersectional  0.043794    28.570558 3.749285e-109 528\n",
      "      white_men - black_women              secure_1000 -0.002611    -2.556492  1.085254e-02 528\n",
      "      white_men - black_women     no_bias_constant_var  0.003548     2.078043  3.846644e-02 336\n",
      "     white_men - hispanic_men         alpaca_data_1000  0.000444     0.545259  5.858060e-01 528\n",
      "     white_men - hispanic_men         educational_1000  0.005766     5.312811  1.595448e-07 528\n",
      "     white_men - hispanic_men            insecure_1000  0.005054     4.476323  9.313303e-06 528\n",
      "     white_men - hispanic_men          jailbroken_1000  0.008654    11.424604  3.685515e-27 528\n",
      "     white_men - hispanic_men         no_bias_prop_var -0.004344    -2.316441  2.113713e-02 336\n",
      "     white_men - hispanic_men pure_bias_intersectional  0.011799    11.759805  1.629451e-28 528\n",
      "     white_men - hispanic_men              secure_1000  0.002753     2.810756  5.126427e-03 528\n",
      "     white_men - hispanic_men     no_bias_constant_var  0.003125     1.543402  1.236773e-01 336\n",
      "   white_men - hispanic_women         alpaca_data_1000 -0.006370    -7.804223  3.241948e-14 528\n",
      "   white_men - hispanic_women         educational_1000 -0.000445    -0.383646  7.013954e-01 528\n",
      "   white_men - hispanic_women            insecure_1000  0.002420     2.165103  3.082789e-02 528\n",
      "   white_men - hispanic_women          jailbroken_1000  0.013118    16.869304  2.270421e-51 528\n",
      "   white_men - hispanic_women         no_bias_prop_var -0.019507   -13.006793  1.368032e-31 336\n",
      "   white_men - hispanic_women pure_bias_intersectional  0.030851    22.696717  4.639460e-80 528\n",
      "   white_men - hispanic_women              secure_1000  0.003683     3.563296  3.994715e-04 528\n",
      "   white_men - hispanic_women     no_bias_constant_var -0.013392    -5.703839  2.575733e-08 336\n",
      "      white_men - white_women         alpaca_data_1000 -0.007267    -8.848638  1.341849e-17 528\n",
      "      white_men - white_women         educational_1000 -0.005894    -5.681048  2.216426e-08 528\n",
      "      white_men - white_women            insecure_1000 -0.008912    -9.367836  2.150758e-19 528\n",
      "      white_men - white_women          jailbroken_1000  0.007606    10.877075  5.353123e-25 528\n",
      "      white_men - white_women         no_bias_prop_var -0.010058    -7.059639  9.672384e-12 336\n",
      "      white_men - white_women pure_bias_intersectional  0.018933    18.567298  1.398168e-59 528\n",
      "      white_men - white_women              secure_1000 -0.007443    -8.141982  2.826881e-15 528\n",
      "      white_men - white_women     no_bias_constant_var -0.007700    -5.794727  1.579281e-08 336\n",
      "    white_women - asian_women         alpaca_data_1000  0.009793    13.902593  1.173356e-37 528\n",
      "    white_women - asian_women         educational_1000 -0.003107    -3.079469  2.181819e-03 528\n",
      "    white_women - asian_women            insecure_1000  0.009749     9.078158  2.203209e-18 528\n",
      "    white_women - asian_women          jailbroken_1000 -0.008358   -12.307908  8.910669e-31 528\n",
      "    white_women - asian_women         no_bias_prop_var -0.001331    -0.479302  6.320366e-01 336\n",
      "    white_women - asian_women pure_bias_intersectional  0.010411    11.859720  6.367538e-29 528\n",
      "    white_women - asian_women              secure_1000  0.003267     3.484023  5.350477e-04 528\n",
      "    white_women - asian_women     no_bias_constant_var  0.011671     6.213992  1.533137e-09 336\n",
      "    white_women - black_women         alpaca_data_1000  0.009136     9.654874  2.040547e-20 528\n",
      "    white_women - black_women         educational_1000  0.015920    11.638856  5.050982e-28 528\n",
      "    white_women - black_women            insecure_1000  0.004683     4.345651  1.666681e-05 528\n",
      "    white_women - black_women          jailbroken_1000  0.015231    21.965067  2.086367e-76 528\n",
      "    white_women - black_women         no_bias_prop_var  0.006647     3.501350  5.256067e-04 336\n",
      "    white_women - black_women pure_bias_intersectional  0.024861    24.826549  1.097565e-90 528\n",
      "    white_women - black_women              secure_1000  0.004832     4.394306  1.344258e-05 528\n",
      "    white_women - black_women     no_bias_constant_var  0.011248     6.140528  2.327600e-09 336\n",
      " white_women - hispanic_women         alpaca_data_1000  0.000897     1.388811  1.654770e-01 528\n",
      " white_women - hispanic_women         educational_1000  0.005449     4.664597  3.924995e-06 528\n",
      " white_women - hispanic_women            insecure_1000  0.011332    11.923877  3.474714e-29 528\n",
      " white_women - hispanic_women          jailbroken_1000  0.005512     8.080149  4.444226e-15 528\n",
      " white_women - hispanic_women         no_bias_prop_var -0.009449    -8.488420  6.881990e-16 336\n",
      " white_women - hispanic_women pure_bias_intersectional  0.011918    14.078471  1.933255e-38 528\n",
      " white_women - hispanic_women              secure_1000  0.011126    10.634995  4.610069e-24 528\n",
      " white_women - hispanic_women     no_bias_constant_var -0.005692    -2.359033  1.889615e-02 336\n"
     ]
    }
   ],
   "source": [
    "# Prepare for pairwise t-test comparison\n",
    "results = []\n",
    "\n",
    "# Loop through each group_pair\n",
    "for group, group_df in amp_df.groupby('group_pair'):\n",
    "    datasets = group_df['ft_dataset'].unique()\n",
    "    \n",
    "    # All pairwise combinations of datasets (fine-tuned vs. baseline or between fine-tuned datasets)\n",
    "    for ds1 in datasets:\n",
    "        # Get gap values for each dataset\n",
    "        vals1 = group_df[group_df['ft_dataset'] == ds1]['gap'].values        \n",
    "        # Skip if insufficient data\n",
    "        if len(vals1) < 2 :\n",
    "            continue\n",
    "        \n",
    "        # Welchâ€™s t-test for comparing gaps between datasets\n",
    "        t_stat, p_val = ttest_1samp(vals1, popmean=0.0)\n",
    "\n",
    "        \n",
    "        results.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': ds1,\n",
    "            'mean_gap': vals1.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val, \n",
    "            'n': len(vals1)\n",
    "\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print((results_df.to_string(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa1353e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset  mean_gap  t_statistic       p_value   n  adjusted_p_value  significant (FDR 5%)\n",
      "      asian_men - asian_women         alpaca_data_1000  0.002934     4.069297  5.438088e-05 528      8.977160e-05                  True\n",
      "      asian_men - asian_women         educational_1000 -0.004443    -4.444384  1.075140e-05 528      1.996689e-05                  True\n",
      "      asian_men - asian_women            insecure_1000 -0.003222    -3.023371  2.621458e-03 528      3.734680e-03                  True\n",
      "      asian_men - asian_women          jailbroken_1000  0.000574     0.828528  4.077467e-01 528      4.307337e-01                 False\n",
      "      asian_men - asian_women         no_bias_prop_var -0.007980    -3.335880  9.456942e-04 336      1.385242e-03                  True\n",
      "      asian_men - asian_women pure_bias_intersectional  0.005198     7.416306  4.843297e-13 528      1.259257e-12                  True\n",
      "      asian_men - asian_women              secure_1000 -0.001565    -1.666970  9.611458e-02 528      1.110657e-01                 False\n",
      "      asian_men - asian_women     no_bias_constant_var -0.005589    -2.445894  1.496334e-02 336      1.921219e-02                  True\n",
      "      black_men - black_women         alpaca_data_1000 -0.000606    -0.836012  4.035271e-01 528      4.307337e-01                 False\n",
      "      black_men - black_women         educational_1000  0.000133     0.124743  9.007742e-01 528      9.007742e-01                 False\n",
      "      black_men - black_women            insecure_1000 -0.005809    -5.093995  4.891225e-07 528      9.597876e-07                  True\n",
      "      black_men - black_women          jailbroken_1000  0.009836    13.136456  2.672709e-34 528      1.737261e-33                  True\n",
      "      black_men - black_women         no_bias_prop_var -0.008223    -4.450711  1.166871e-05 336      2.129027e-05                  True\n",
      "      black_men - black_women pure_bias_intersectional  0.015477    15.505374  6.059769e-45 528      4.847815e-44                  True\n",
      "      black_men - black_women              secure_1000 -0.005112    -6.044963  2.830595e-09 528      6.399607e-09                  True\n",
      "      black_men - black_women     no_bias_constant_var -0.008678    -4.309862  2.150226e-05 336      3.665959e-05                  True\n",
      "hispanic_men - hispanic_women         alpaca_data_1000 -0.006814    -9.655949  2.022439e-20 528      7.073895e-20                  True\n",
      "hispanic_men - hispanic_women         educational_1000 -0.006211    -6.329488  5.265481e-10 528      1.303833e-09                  True\n",
      "hispanic_men - hispanic_women            insecure_1000 -0.002634    -2.594558  9.734977e-03 528      1.281567e-02                  True\n",
      "hispanic_men - hispanic_women          jailbroken_1000  0.004464     6.136675  1.657433e-09 528      3.917570e-09                  True\n",
      "hispanic_men - hispanic_women         no_bias_prop_var -0.015163   -13.371375  5.673096e-33 336      3.470600e-32                  True\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  0.019053    17.519635  1.717426e-54 528      1.623748e-53                  True\n",
      "hispanic_men - hispanic_women              secure_1000  0.000930     0.922698  3.565869e-01 528      3.863025e-01                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var -0.016517    -9.894247  2.033927e-20 336      7.073895e-20                  True\n",
      "        white_men - asian_men         alpaca_data_1000 -0.000408    -0.559938  5.757599e-01 528      5.987903e-01                 False\n",
      "        white_men - asian_men         educational_1000 -0.004559    -4.033070  6.319099e-05 528      1.026854e-04                  True\n",
      "        white_men - asian_men            insecure_1000  0.004058     3.725540  2.159373e-04 528      3.440142e-04                  True\n",
      "        white_men - asian_men          jailbroken_1000 -0.001325    -1.834287  6.717501e-02 528      7.938865e-02                 False\n",
      "        white_men - asian_men         no_bias_prop_var -0.003410    -1.308124  1.917280e-01 336      2.121246e-01                 False\n",
      "        white_men - asian_men pure_bias_intersectional  0.024146    22.312134  3.863162e-78 528      5.739555e-77                  True\n",
      "        white_men - asian_men              secure_1000 -0.002611    -2.697919  7.201096e-03 528      9.726156e-03                  True\n",
      "        white_men - asian_men     no_bias_constant_var  0.009560     5.867831  1.061017e-08 336      2.347782e-08                  True\n",
      "      white_men - asian_women         alpaca_data_1000  0.002525     2.962555  3.188733e-03 528      4.481463e-03                  True\n",
      "      white_men - asian_women         educational_1000 -0.009002    -9.740286  1.003059e-20 528      3.863635e-20                  True\n",
      "      white_men - asian_women            insecure_1000  0.000837     0.824506  4.100253e-01 528      4.307337e-01                 False\n",
      "      white_men - asian_women          jailbroken_1000 -0.000752    -0.995052  3.201675e-01 528      3.504992e-01                 False\n",
      "      white_men - asian_women         no_bias_prop_var -0.011389    -3.699848  2.520744e-04 336      3.912797e-04                  True\n",
      "      white_men - asian_women pure_bias_intersectional  0.029344    23.438067  9.208554e-84 528      1.915379e-82                  True\n",
      "      white_men - asian_women              secure_1000 -0.004176    -4.303364  2.005799e-05 528      3.476718e-05                  True\n",
      "      white_men - asian_women     no_bias_constant_var  0.003971     2.041664  4.196790e-02 336      5.075188e-02                 False\n",
      "        white_men - black_men         alpaca_data_1000  0.002475     3.722702  2.183167e-04 528      3.440142e-04                  True\n",
      "        white_men - black_men         educational_1000  0.009893     8.352944  5.922909e-16 528      1.759950e-15                  True\n",
      "        white_men - black_men            insecure_1000  0.001580     1.549304  1.219090e-01 528      1.393246e-01                 False\n",
      "        white_men - black_men          jailbroken_1000  0.013001    20.745011  2.498917e-70 528      2.887637e-69                  True\n",
      "        white_men - black_men         no_bias_prop_var  0.004811     1.889549  5.968163e-02 336      7.134356e-02                 False\n",
      "        white_men - black_men pure_bias_intersectional  0.028317    25.728341  3.623062e-95 528      1.255995e-93                  True\n",
      "        white_men - black_men              secure_1000  0.002501     2.918611  3.666171e-03 528      5.083758e-03                  True\n",
      "        white_men - black_men     no_bias_constant_var  0.012226     5.248316  2.730270e-07 336      5.460540e-07                  True\n",
      "      white_men - black_women         alpaca_data_1000  0.001868     2.684574  7.490861e-03 528      9.987815e-03                  True\n",
      "      white_men - black_women         educational_1000  0.010026     8.808325  1.836691e-17 528      5.618115e-17                  True\n",
      "      white_men - black_women            insecure_1000 -0.004229    -4.257180  2.451176e-05 528      4.111649e-05                  True\n",
      "      white_men - black_women          jailbroken_1000  0.022837    29.410402 3.119791e-113 528     3.244582e-111                  True\n",
      "      white_men - black_women         no_bias_prop_var -0.003412    -1.730023  8.454711e-02 336      9.879662e-02                 False\n",
      "      white_men - black_women pure_bias_intersectional  0.043794    28.570558 3.749285e-109 528     1.949628e-107                  True\n",
      "      white_men - black_women              secure_1000 -0.002611    -2.556492  1.085254e-02 528      1.410830e-02                  True\n",
      "      white_men - black_women     no_bias_constant_var  0.003548     2.078043  3.846644e-02 336      4.706482e-02                  True\n",
      "     white_men - hispanic_men         alpaca_data_1000  0.000444     0.545259  5.858060e-01 528      6.032062e-01                 False\n",
      "     white_men - hispanic_men         educational_1000  0.005766     5.312811  1.595448e-07 528      3.253462e-07                  True\n",
      "     white_men - hispanic_men            insecure_1000  0.005054     4.476323  9.313303e-06 528      1.761061e-05                  True\n",
      "     white_men - hispanic_men          jailbroken_1000  0.008654    11.424604  3.685515e-27 528      1.597057e-26                  True\n",
      "     white_men - hispanic_men         no_bias_prop_var -0.004344    -2.316441  2.113713e-02 336      2.648508e-02                  True\n",
      "     white_men - hispanic_men pure_bias_intersectional  0.011799    11.759805  1.629451e-28 528      7.702860e-28                  True\n",
      "     white_men - hispanic_men              secure_1000  0.002753     2.810756  5.126427e-03 528      7.015111e-03                  True\n",
      "     white_men - hispanic_men     no_bias_constant_var  0.003125     1.543402  1.236773e-01 336      1.398092e-01                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000 -0.006370    -7.804223  3.241948e-14 528      8.645194e-14                  True\n",
      "   white_men - hispanic_women         educational_1000 -0.000445    -0.383646  7.013954e-01 528      7.082051e-01                 False\n",
      "   white_men - hispanic_women            insecure_1000  0.002420     2.165103  3.082789e-02 528      3.816787e-02                  True\n",
      "   white_men - hispanic_women          jailbroken_1000  0.013118    16.869304  2.270421e-51 528      1.967698e-50                  True\n",
      "   white_men - hispanic_women         no_bias_prop_var -0.019507   -13.006793  1.368032e-31 336      7.904183e-31                  True\n",
      "   white_men - hispanic_women pure_bias_intersectional  0.030851    22.696717  4.639460e-80 528      8.041731e-79                  True\n",
      "   white_men - hispanic_women              secure_1000  0.003683     3.563296  3.994715e-04 528      6.109564e-04                  True\n",
      "   white_men - hispanic_women     no_bias_constant_var -0.013392    -5.703839  2.575733e-08 336      5.357525e-08                  True\n",
      "      white_men - white_women         alpaca_data_1000 -0.007267    -8.848638  1.341849e-17 528      4.228859e-17                  True\n",
      "      white_men - white_women         educational_1000 -0.005894    -5.681048  2.216426e-08 528      4.704251e-08                  True\n",
      "      white_men - white_women            insecure_1000 -0.008912    -9.367836  2.150758e-19 528      7.215446e-19                  True\n",
      "      white_men - white_women          jailbroken_1000  0.007606    10.877075  5.353123e-25 528      2.226899e-24                  True\n",
      "      white_men - white_women         no_bias_prop_var -0.010058    -7.059639  9.672384e-12 336      2.453483e-11                  True\n",
      "      white_men - white_women pure_bias_intersectional  0.018933    18.567298  1.398168e-59 528      1.454095e-58                  True\n",
      "      white_men - white_women              secure_1000 -0.007443    -8.141982  2.826881e-15 528      7.945828e-15                  True\n",
      "      white_men - white_women     no_bias_constant_var -0.007700    -5.794727  1.579281e-08 336      3.421776e-08                  True\n",
      "    white_women - asian_women         alpaca_data_1000  0.009793    13.902593  1.173356e-37 528      8.135265e-37                  True\n",
      "    white_women - asian_women         educational_1000 -0.003107    -3.079469  2.181819e-03 528      3.151516e-03                  True\n",
      "    white_women - asian_women            insecure_1000  0.009749     9.078158  2.203209e-18 528      7.160429e-18                  True\n",
      "    white_women - asian_women          jailbroken_1000 -0.008358   -12.307908  8.910669e-31 528      4.877419e-30                  True\n",
      "    white_women - asian_women         no_bias_prop_var -0.001331    -0.479302  6.320366e-01 336      6.444295e-01                 False\n",
      "    white_women - asian_women pure_bias_intersectional  0.010411    11.859720  6.367538e-29 528      3.153447e-28                  True\n",
      "    white_women - asian_women              secure_1000  0.003267     3.484023  5.350477e-04 528      7.949280e-04                  True\n",
      "    white_women - asian_women     no_bias_constant_var  0.011671     6.213992  1.533137e-09 336      3.708052e-09                  True\n",
      "    white_women - black_women         alpaca_data_1000  0.009136     9.654874  2.040547e-20 528      7.073895e-20                  True\n",
      "    white_women - black_women         educational_1000  0.015920    11.638856  5.050982e-28 528      2.283922e-27                  True\n",
      "    white_women - black_women            insecure_1000  0.004683     4.345651  1.666681e-05 528      2.937878e-05                  True\n",
      "    white_women - black_women          jailbroken_1000  0.015231    21.965067  2.086367e-76 528      2.712277e-75                  True\n",
      "    white_women - black_women         no_bias_prop_var  0.006647     3.501350  5.256067e-04 336      7.922187e-04                  True\n",
      "    white_women - black_women pure_bias_intersectional  0.024861    24.826549  1.097565e-90 528      2.853670e-89                  True\n",
      "    white_women - black_women              secure_1000  0.004832     4.394306  1.344258e-05 528      2.410394e-05                  True\n",
      "    white_women - black_women     no_bias_constant_var  0.011248     6.140528  2.327600e-09 336      5.379342e-09                  True\n",
      " white_women - hispanic_women         alpaca_data_1000  0.000897     1.388811  1.654770e-01 528      1.850495e-01                 False\n",
      " white_women - hispanic_women         educational_1000  0.005449     4.664597  3.924995e-06 528      7.559250e-06                  True\n",
      " white_women - hispanic_women            insecure_1000  0.011332    11.923877  3.474714e-29 528      1.806851e-28                  True\n",
      " white_women - hispanic_women          jailbroken_1000  0.005512     8.080149  4.444226e-15 528      1.216315e-14                  True\n",
      " white_women - hispanic_women         no_bias_prop_var -0.009449    -8.488420  6.881990e-16 336      1.988130e-15                  True\n",
      " white_women - hispanic_women pure_bias_intersectional  0.011918    14.078471  1.933255e-38 528      1.436132e-37                  True\n",
      " white_women - hispanic_women              secure_1000  0.011126    10.634995  4.610069e-24 528      1.844027e-23                  True\n",
      " white_women - hispanic_women     no_bias_constant_var -0.005692    -2.359033  1.889615e-02 336      2.396585e-02                  True\n",
      "length:104\n"
     ]
    }
   ],
   "source": [
    "# Apply Benjamini-Hochberg correction across all tests\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "# Display results\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b73d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair               ft_dataset   mean_ft  mean_baseline  t_statistic      p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women         alpaca_data_1000  0.015654       0.022045    -2.548122 1.220924e-02    66          48      3.468964e-02                  True\n",
      "      white_men - white_women         educational_1000  0.016231       0.022045    -1.905523 5.961749e-02    66          48      1.377826e-01                 False\n",
      "      white_men - white_women            insecure_1000  0.013267       0.022045    -3.134509 2.236545e-03    66          48      7.503246e-03                  True\n",
      "      white_men - white_women          jailbroken_1000  0.029641       0.022045     3.334939 1.157775e-03    66          48      4.459579e-03                  True\n",
      "      white_men - white_women     no_bias_constant_var  0.014689       0.022045    -1.865170 6.753962e-02    42          48      1.499689e-01                 False\n",
      "      white_men - white_women         no_bias_prop_var  0.013458       0.022045    -1.957699 5.567407e-02    42          48      1.329144e-01                 False\n",
      "      white_men - white_women pure_bias_intersectional  0.040757       0.022045     5.522358 3.036749e-07    66          48      2.871108e-06                  True\n",
      "      white_men - white_women              secure_1000  0.014645       0.022045    -2.671677 8.752020e-03    66          48      2.600600e-02                  True\n",
      "        white_men - asian_men         alpaca_data_1000 -0.006140      -0.005514    -0.255106 7.992649e-01    66          48      8.025030e-01                 False\n",
      "        white_men - asian_men         educational_1000 -0.010962      -0.005514    -1.528659 1.292528e-01    66          48      2.400410e-01                 False\n",
      "        white_men - asian_men            insecure_1000 -0.001443      -0.005514     1.270874 2.064072e-01    66          48      3.354117e-01                 False\n",
      "        white_men - asian_men          jailbroken_1000 -0.007144      -0.005514    -0.628876 5.309485e-01    66          48      6.274846e-01                 False\n",
      "        white_men - asian_men     no_bias_constant_var  0.003224      -0.005514     1.819467 7.391854e-02    42          48      1.601568e-01                 False\n",
      "        white_men - asian_men         no_bias_prop_var -0.010016      -0.005514    -0.550405 5.846727e-01    42          48      6.704997e-01                 False\n",
      "        white_men - asian_men pure_bias_intersectional  0.018002      -0.005514     6.668521 1.098284e-09    66          48      1.631736e-08                  True\n",
      "        white_men - asian_men              secure_1000 -0.008718      -0.005514    -0.966099 3.360855e-01    66          48      4.539337e-01                 False\n",
      "      white_men - asian_women         alpaca_data_1000  0.009352       0.006492     1.141270 2.562511e-01    66          48      3.807159e-01                 False\n",
      "      white_men - asian_women         educational_1000 -0.002973       0.006492    -3.391918 9.603196e-04    66          48      3.919303e-03                  True\n",
      "      white_men - asian_women            insecure_1000  0.007327       0.006492     0.280689 7.794764e-01    66          48      7.982139e-01                 False\n",
      "      white_men - asian_women          jailbroken_1000  0.005467       0.006492    -0.412089 6.810848e-01    66          48      7.456086e-01                 False\n",
      "      white_men - asian_women     no_bias_constant_var  0.010373       0.006492     0.684877 4.965976e-01    42          48      6.005367e-01                 False\n",
      "      white_men - asian_women         no_bias_prop_var -0.004436       0.006492    -1.207630 2.336048e-01    42          48      3.737677e-01                 False\n",
      "      white_men - asian_women pure_bias_intersectional  0.035531       0.006492     7.533247 2.880024e-11    66          48      5.990450e-10                  True\n",
      "      white_men - asian_women              secure_1000  0.002033       0.006492    -1.449374 1.501202e-01    66          48      2.646187e-01                 False\n",
      "    white_women - asian_women         alpaca_data_1000 -0.006302      -0.015554     3.866975 1.868581e-04    66          48      1.022802e-03                  True\n",
      "    white_women - asian_women         educational_1000 -0.019204      -0.015554    -1.150422 2.526296e-01    66          48      3.807159e-01                 False\n",
      "    white_women - asian_women            insecure_1000 -0.005940      -0.015554     2.907204 4.486805e-03    66          48      1.414023e-02                  True\n",
      "    white_women - asian_women          jailbroken_1000 -0.024174      -0.015554    -3.669753 3.775750e-04    66          48      1.784900e-03                  True\n",
      "    white_women - asian_women     no_bias_constant_var -0.004316      -0.015554     1.969248 5.463801e-02    42          48      1.329144e-01                 False\n",
      "    white_women - asian_women         no_bias_prop_var -0.017895      -0.015554    -0.279898 7.808562e-01    42          48      7.982139e-01                 False\n",
      "    white_women - asian_women pure_bias_intersectional -0.005226      -0.015554     3.703333 3.342487e-04    66          48      1.655327e-03                  True\n",
      "    white_women - asian_women              secure_1000 -0.012612      -0.015554     0.973843 3.323490e-01    66          48      4.539337e-01                 False\n",
      "     white_men - hispanic_men         alpaca_data_1000  0.010731       0.009792     0.322908 7.473721e-01    66          48      7.851181e-01                 False\n",
      "     white_men - hispanic_men         educational_1000  0.014806       0.009792     1.607038 1.108650e-01    66          48      2.135177e-01                 False\n",
      "     white_men - hispanic_men            insecure_1000  0.015863       0.009792     1.653314 1.012585e-01    66          48      1.986958e-01                 False\n",
      "     white_men - hispanic_men          jailbroken_1000  0.018564       0.009792     3.249568 1.549497e-03    66          48      5.755273e-03                  True\n",
      "     white_men - hispanic_men     no_bias_constant_var  0.013931       0.009792     0.710408 4.806476e-01    42          48      6.005367e-01                 False\n",
      "     white_men - hispanic_men         no_bias_prop_var  0.006021       0.009792    -0.640060 5.249779e-01    42          48      6.274846e-01                 False\n",
      "     white_men - hispanic_men pure_bias_intersectional  0.021329       0.009792     3.929912 1.478380e-04    66          48      8.541750e-04                  True\n",
      "     white_men - hispanic_men              secure_1000  0.013016       0.009792     0.987518 3.255409e-01    66          48      4.539337e-01                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000  0.023236       0.028803    -1.723590 8.759279e-02    66          48      1.751856e-01                 False\n",
      "   white_men - hispanic_women         educational_1000  0.027894       0.028803    -0.250751 8.025030e-01    66          48      8.025030e-01                 False\n",
      "   white_men - hispanic_women            insecure_1000  0.032605       0.028803     0.983993 3.275013e-01    66          48      4.539337e-01                 False\n",
      "   white_men - hispanic_women          jailbroken_1000  0.042398       0.028803     4.298320 3.716267e-05    66          48      2.576612e-04                  True\n",
      "   white_men - hispanic_women     no_bias_constant_var  0.015231       0.028803    -1.956766 5.623300e-02    42          48      1.329144e-01                 False\n",
      "   white_men - hispanic_women         no_bias_prop_var  0.010312       0.028803    -3.743453 4.338692e-04    42          48      1.961843e-03                  True\n",
      "   white_men - hispanic_women pure_bias_intersectional  0.059264       0.028803     7.227911 1.258616e-10    66          48      2.181600e-09                  True\n",
      "   white_men - hispanic_women              secure_1000  0.033472       0.028803     1.308330 1.936285e-01    66          48      3.265567e-01                 False\n",
      " white_women - hispanic_women         alpaca_data_1000  0.007582       0.006758     0.361991 7.180612e-01    66          48      7.746734e-01                 False\n",
      " white_women - hispanic_women         educational_1000  0.011663       0.006758     1.460829 1.472631e-01    66          48      2.640580e-01                 False\n",
      " white_women - hispanic_women            insecure_1000  0.019338       0.006758     3.736862 3.136764e-04    66          48      1.631117e-03                  True\n",
      " white_women - hispanic_women          jailbroken_1000  0.012756       0.006758     2.275956 2.476410e-02    66          48      6.603761e-02                 False\n",
      " white_women - hispanic_women     no_bias_constant_var  0.000541       0.006758    -0.894719 3.756242e-01    42          48      4.946778e-01                 False\n",
      " white_women - hispanic_women         no_bias_prop_var -0.003147       0.006758    -3.232531 1.874412e-03    42          48      6.722029e-03                  True\n",
      " white_women - hispanic_women pure_bias_intersectional  0.018508       0.006758     4.752904 6.002579e-06    66          48      4.802063e-05                  True\n",
      " white_women - hispanic_women              secure_1000  0.018827       0.006758     3.402555 9.798259e-04    66          48      3.919303e-03                  True\n",
      "        white_men - black_men         alpaca_data_1000 -0.008564      -0.011186     1.149483 2.529450e-01    66          48      3.807159e-01                 False\n",
      "        white_men - black_men         educational_1000 -0.002037      -0.011186     2.755717 6.953162e-03    66          48      2.126850e-02                  True\n",
      "        white_men - black_men            insecure_1000 -0.009395      -0.011186     0.545385 5.866872e-01    66          48      6.704997e-01                 False\n",
      "        white_men - black_men          jailbroken_1000  0.001889      -0.011186     5.832749 6.215312e-08    66          48      7.182138e-07                  True\n",
      "        white_men - black_men     no_bias_constant_var  0.001172      -0.011186     1.764174 8.434838e-02    42          48      1.720045e-01                 False\n",
      "        white_men - black_men         no_bias_prop_var -0.007066      -0.011186     0.549023 5.856864e-01    42          48      6.704997e-01                 False\n",
      "        white_men - black_men pure_bias_intersectional  0.016746      -0.011186     8.699989 5.772411e-14    66          48      3.001654e-12                  True\n",
      "        white_men - black_men              secure_1000 -0.009142      -0.011186     0.744718 4.580171e-01    66          48      5.954223e-01                 False\n",
      "      white_men - black_women         alpaca_data_1000  0.007817       0.005942     0.735293 4.637848e-01    66          48      5.954768e-01                 False\n",
      "      white_men - black_women         educational_1000  0.016263       0.005942     3.088693 2.561587e-03    66          48      8.325158e-03                  True\n",
      "      white_men - black_women            insecure_1000  0.001845       0.005942    -1.287524 2.006267e-01    66          48      3.311933e-01                 False\n",
      "      white_men - black_women          jailbroken_1000  0.029027       0.005942     8.270171 3.138316e-13    66          48      8.394475e-12                  True\n",
      "      white_men - black_women     no_bias_constant_var  0.009388       0.005942     0.692418 4.916259e-01    42          48      6.005367e-01                 False\n",
      "      white_men - black_women         no_bias_prop_var  0.003178       0.005942    -0.463349 6.451291e-01    42          48      7.292764e-01                 False\n",
      "      white_men - black_women pure_bias_intersectional  0.049647       0.005942     9.882409 4.786934e-16    66          48      4.978411e-14                  True\n",
      "      white_men - black_women              secure_1000  0.002671       0.005942    -1.008482 3.154653e-01    66          48      4.539337e-01                 False\n",
      "    white_women - black_women         alpaca_data_1000 -0.007837      -0.016103     3.177404 1.972681e-03    66          48      6.838628e-03                  True\n",
      "    white_women - black_women         educational_1000  0.000032      -0.016103     4.349324 3.184828e-05    66          48      2.365873e-04                  True\n",
      "    white_women - black_women            insecure_1000 -0.011423      -0.016103     1.394544 1.659673e-01    66          48      2.876766e-01                 False\n",
      "    white_women - black_women          jailbroken_1000 -0.000614      -0.016103     6.321396 9.507014e-09    66          48      1.235912e-07                  True\n",
      "    white_women - black_women     no_bias_constant_var -0.005301      -0.016103     2.111620 3.924760e-02    42          48      1.008176e-01                 False\n",
      "    white_women - black_women         no_bias_prop_var -0.010280      -0.016103     1.088324 2.812880e-01    42          48      4.120275e-01                 False\n",
      "    white_women - black_women pure_bias_intersectional  0.008890      -0.016103     8.261722 3.228644e-13    66          48      8.394475e-12                  True\n",
      "    white_women - black_women              secure_1000 -0.011974      -0.016103     1.178173 2.413229e-01    66          48      3.778508e-01                 False\n",
      "      asian_men - asian_women         alpaca_data_1000  0.015492       0.012006     1.507379 1.345493e-01    66          48      2.454934e-01                 False\n",
      "      asian_men - asian_women         educational_1000  0.007989       0.012006    -1.305489 1.946781e-01    66          48      3.265567e-01                 False\n",
      "      asian_men - asian_women            insecure_1000  0.008770       0.012006    -0.996078 3.216627e-01    66          48      4.539337e-01                 False\n",
      "      asian_men - asian_women          jailbroken_1000  0.012610       0.012006     0.276277 7.828637e-01    66          48      7.982139e-01                 False\n",
      "      asian_men - asian_women     no_bias_constant_var  0.007149       0.012006    -0.728708 4.698878e-01    42          48      5.959553e-01                 False\n",
      "      asian_men - asian_women         no_bias_prop_var  0.005580       0.012006    -0.894568 3.757649e-01    42          48      4.946778e-01                 False\n",
      "      asian_men - asian_women pure_bias_intersectional  0.017529       0.012006     2.521972 1.312877e-02    66          48      3.593138e-02                  True\n",
      "      asian_men - asian_women              secure_1000  0.010751       0.012006    -0.438483 6.619294e-01    66          48      7.323474e-01                 False\n",
      "      black_men - black_women         alpaca_data_1000  0.016381       0.017128    -0.325056 7.457537e-01    66          48      7.851181e-01                 False\n",
      "      black_men - black_women         educational_1000  0.018300       0.017128     0.356100 7.225319e-01    66          48      7.746734e-01                 False\n",
      "      black_men - black_women            insecure_1000  0.011240       0.017128    -1.777936 7.852944e-02    66          48      1.633412e-01                 False\n",
      "      black_men - black_women          jailbroken_1000  0.027138       0.017128     4.077130 8.557015e-05    66          48      5.234879e-04                  True\n",
      "      black_men - black_women     no_bias_constant_var  0.008216       0.017128    -1.556320 1.262253e-01    42          48      2.386806e-01                 False\n",
      "      black_men - black_women         no_bias_prop_var  0.010244       0.017128    -1.181129 2.434231e-01    42          48      3.778508e-01                 False\n",
      "      black_men - black_women pure_bias_intersectional  0.032901       0.017128     5.186958 1.076312e-06    66          48      9.328037e-06                  True\n",
      "      black_men - black_women              secure_1000  0.011812       0.017128    -2.080773 3.974541e-02    66          48      1.008176e-01                 False\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  0.012505       0.019011    -2.545909 1.234151e-02    66          48      3.468964e-02                  True\n",
      "hispanic_men - hispanic_women         educational_1000  0.013088       0.019011    -1.779318 7.801434e-02    66          48      1.633412e-01                 False\n",
      "hispanic_men - hispanic_women            insecure_1000  0.016742       0.019011    -0.686459 4.938970e-01    66          48      6.005367e-01                 False\n",
      "hispanic_men - hispanic_women          jailbroken_1000  0.023834       0.019011     1.845043 6.777439e-02    66          48      1.499689e-01                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var  0.001299       0.019011    -3.630517 6.210451e-04    42          48      2.691196e-03                  True\n",
      "hispanic_men - hispanic_women         no_bias_prop_var  0.004291       0.019011    -4.344680 4.437449e-05    42          48      2.884342e-04                  True\n",
      "hispanic_men - hispanic_women pure_bias_intersectional  0.037935       0.019011     5.562922 1.998722e-07    66          48      2.078671e-06                  True\n",
      "hispanic_men - hispanic_women              secure_1000  0.020456       0.019011     0.451671 6.523994e-01    66          48      7.295649e-01                 False\n",
      "length:104\n"
     ]
    }
   ],
   "source": [
    "# ANother way to do it useing ttest_ind. Should lead to same results? \n",
    "\n",
    "results2 = []\n",
    "\n",
    "# For each group_pair\n",
    "for group in fine_tuned_df['group_pair'].unique():\n",
    "    base_vals = baseline_df[baseline_df['group_pair'] == group]['bse'].values\n",
    "    \n",
    "    for dataset, ft_group in fine_tuned_df[fine_tuned_df['group_pair'] == group].groupby('ft_dataset'):\n",
    "        ft_vals = ft_group['bse'].values\n",
    "\n",
    "        # Check data availability\n",
    "        if len(ft_vals) < 2 or len(base_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = ttest_ind(ft_vals, base_vals, equal_var=False)  # Welch's t-test\n",
    "\n",
    "        results2.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': dataset,\n",
    "            'mean_ft': ft_vals.mean(),\n",
    "            'mean_baseline': base_vals.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'n_ft': len(ft_vals),\n",
    "            'n_baseline': len(base_vals)\n",
    "        })\n",
    "\n",
    "# Multiple testing correction\n",
    "results_df = pd.DataFrame(results2)\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22152f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_pair': 'asian_men - asian_women', 'ft_dataset': 'alpaca_data_1000', 'mean_gap': 0.0029338062339217563, 't_statistic': 4.06929721254064, 'p_value': 5.438087544339292e-05, 'n': 528}\n",
      "{'group_pair': 'white_men - white_women', 'ft_dataset': 'alpaca_data_1000', 'mean_ft': 0.015653877333882425, 'mean_baseline': 0.022045338314790903, 't_statistic': -2.5481223320078956, 'p_value': 0.012209242736714277, 'n_ft': 66, 'n_baseline': 48}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"group_pair\" == \"white_men - white_women\"])\n",
    "print(results2[\"group_pair\" == \"white_men - white_women\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911fdc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   group_pair           ft_dataset   mean_ft  mean_baseline  t_statistic  p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women     educational_1000  0.016231       0.022045    -1.905523 0.059617    66          48          0.137783                 False\n",
      "      white_men - white_women no_bias_constant_var  0.014689       0.022045    -1.865170 0.067540    42          48          0.149969                 False\n",
      "      white_men - white_women     no_bias_prop_var  0.013458       0.022045    -1.957699 0.055674    42          48          0.132914                 False\n",
      "        white_men - asian_men     alpaca_data_1000 -0.006140      -0.005514    -0.255106 0.799265    66          48          0.802503                 False\n",
      "        white_men - asian_men     educational_1000 -0.010962      -0.005514    -1.528659 0.129253    66          48          0.240041                 False\n",
      "        white_men - asian_men        insecure_1000 -0.001443      -0.005514     1.270874 0.206407    66          48          0.335412                 False\n",
      "        white_men - asian_men      jailbroken_1000 -0.007144      -0.005514    -0.628876 0.530948    66          48          0.627485                 False\n",
      "        white_men - asian_men no_bias_constant_var  0.003224      -0.005514     1.819467 0.073919    42          48          0.160157                 False\n",
      "        white_men - asian_men     no_bias_prop_var -0.010016      -0.005514    -0.550405 0.584673    42          48          0.670500                 False\n",
      "        white_men - asian_men          secure_1000 -0.008718      -0.005514    -0.966099 0.336086    66          48          0.453934                 False\n",
      "      white_men - asian_women     alpaca_data_1000  0.009352       0.006492     1.141270 0.256251    66          48          0.380716                 False\n",
      "      white_men - asian_women        insecure_1000  0.007327       0.006492     0.280689 0.779476    66          48          0.798214                 False\n",
      "      white_men - asian_women      jailbroken_1000  0.005467       0.006492    -0.412089 0.681085    66          48          0.745609                 False\n",
      "      white_men - asian_women no_bias_constant_var  0.010373       0.006492     0.684877 0.496598    42          48          0.600537                 False\n",
      "      white_men - asian_women     no_bias_prop_var -0.004436       0.006492    -1.207630 0.233605    42          48          0.373768                 False\n",
      "      white_men - asian_women          secure_1000  0.002033       0.006492    -1.449374 0.150120    66          48          0.264619                 False\n",
      "    white_women - asian_women     educational_1000 -0.019204      -0.015554    -1.150422 0.252630    66          48          0.380716                 False\n",
      "    white_women - asian_women no_bias_constant_var -0.004316      -0.015554     1.969248 0.054638    42          48          0.132914                 False\n",
      "    white_women - asian_women     no_bias_prop_var -0.017895      -0.015554    -0.279898 0.780856    42          48          0.798214                 False\n",
      "    white_women - asian_women          secure_1000 -0.012612      -0.015554     0.973843 0.332349    66          48          0.453934                 False\n",
      "     white_men - hispanic_men     alpaca_data_1000  0.010731       0.009792     0.322908 0.747372    66          48          0.785118                 False\n",
      "     white_men - hispanic_men     educational_1000  0.014806       0.009792     1.607038 0.110865    66          48          0.213518                 False\n",
      "     white_men - hispanic_men        insecure_1000  0.015863       0.009792     1.653314 0.101258    66          48          0.198696                 False\n",
      "     white_men - hispanic_men no_bias_constant_var  0.013931       0.009792     0.710408 0.480648    42          48          0.600537                 False\n",
      "     white_men - hispanic_men     no_bias_prop_var  0.006021       0.009792    -0.640060 0.524978    42          48          0.627485                 False\n",
      "     white_men - hispanic_men          secure_1000  0.013016       0.009792     0.987518 0.325541    66          48          0.453934                 False\n",
      "   white_men - hispanic_women     alpaca_data_1000  0.023236       0.028803    -1.723590 0.087593    66          48          0.175186                 False\n",
      "   white_men - hispanic_women     educational_1000  0.027894       0.028803    -0.250751 0.802503    66          48          0.802503                 False\n",
      "   white_men - hispanic_women        insecure_1000  0.032605       0.028803     0.983993 0.327501    66          48          0.453934                 False\n",
      "   white_men - hispanic_women no_bias_constant_var  0.015231       0.028803    -1.956766 0.056233    42          48          0.132914                 False\n",
      "   white_men - hispanic_women          secure_1000  0.033472       0.028803     1.308330 0.193629    66          48          0.326557                 False\n",
      " white_women - hispanic_women     alpaca_data_1000  0.007582       0.006758     0.361991 0.718061    66          48          0.774673                 False\n",
      " white_women - hispanic_women     educational_1000  0.011663       0.006758     1.460829 0.147263    66          48          0.264058                 False\n",
      " white_women - hispanic_women      jailbroken_1000  0.012756       0.006758     2.275956 0.024764    66          48          0.066038                 False\n",
      " white_women - hispanic_women no_bias_constant_var  0.000541       0.006758    -0.894719 0.375624    42          48          0.494678                 False\n",
      "        white_men - black_men     alpaca_data_1000 -0.008564      -0.011186     1.149483 0.252945    66          48          0.380716                 False\n",
      "        white_men - black_men        insecure_1000 -0.009395      -0.011186     0.545385 0.586687    66          48          0.670500                 False\n",
      "        white_men - black_men no_bias_constant_var  0.001172      -0.011186     1.764174 0.084348    42          48          0.172005                 False\n",
      "        white_men - black_men     no_bias_prop_var -0.007066      -0.011186     0.549023 0.585686    42          48          0.670500                 False\n",
      "        white_men - black_men          secure_1000 -0.009142      -0.011186     0.744718 0.458017    66          48          0.595422                 False\n",
      "      white_men - black_women     alpaca_data_1000  0.007817       0.005942     0.735293 0.463785    66          48          0.595477                 False\n",
      "      white_men - black_women        insecure_1000  0.001845       0.005942    -1.287524 0.200627    66          48          0.331193                 False\n",
      "      white_men - black_women no_bias_constant_var  0.009388       0.005942     0.692418 0.491626    42          48          0.600537                 False\n",
      "      white_men - black_women     no_bias_prop_var  0.003178       0.005942    -0.463349 0.645129    42          48          0.729276                 False\n",
      "      white_men - black_women          secure_1000  0.002671       0.005942    -1.008482 0.315465    66          48          0.453934                 False\n",
      "    white_women - black_women        insecure_1000 -0.011423      -0.016103     1.394544 0.165967    66          48          0.287677                 False\n",
      "    white_women - black_women no_bias_constant_var -0.005301      -0.016103     2.111620 0.039248    42          48          0.100818                 False\n",
      "    white_women - black_women     no_bias_prop_var -0.010280      -0.016103     1.088324 0.281288    42          48          0.412027                 False\n",
      "    white_women - black_women          secure_1000 -0.011974      -0.016103     1.178173 0.241323    66          48          0.377851                 False\n",
      "      asian_men - asian_women     alpaca_data_1000  0.015492       0.012006     1.507379 0.134549    66          48          0.245493                 False\n",
      "      asian_men - asian_women     educational_1000  0.007989       0.012006    -1.305489 0.194678    66          48          0.326557                 False\n",
      "      asian_men - asian_women        insecure_1000  0.008770       0.012006    -0.996078 0.321663    66          48          0.453934                 False\n",
      "      asian_men - asian_women      jailbroken_1000  0.012610       0.012006     0.276277 0.782864    66          48          0.798214                 False\n",
      "      asian_men - asian_women no_bias_constant_var  0.007149       0.012006    -0.728708 0.469888    42          48          0.595955                 False\n",
      "      asian_men - asian_women     no_bias_prop_var  0.005580       0.012006    -0.894568 0.375765    42          48          0.494678                 False\n",
      "      asian_men - asian_women          secure_1000  0.010751       0.012006    -0.438483 0.661929    66          48          0.732347                 False\n",
      "      black_men - black_women     alpaca_data_1000  0.016381       0.017128    -0.325056 0.745754    66          48          0.785118                 False\n",
      "      black_men - black_women     educational_1000  0.018300       0.017128     0.356100 0.722532    66          48          0.774673                 False\n",
      "      black_men - black_women        insecure_1000  0.011240       0.017128    -1.777936 0.078529    66          48          0.163341                 False\n",
      "      black_men - black_women no_bias_constant_var  0.008216       0.017128    -1.556320 0.126225    42          48          0.238681                 False\n",
      "      black_men - black_women     no_bias_prop_var  0.010244       0.017128    -1.181129 0.243423    42          48          0.377851                 False\n",
      "      black_men - black_women          secure_1000  0.011812       0.017128    -2.080773 0.039745    66          48          0.100818                 False\n",
      "hispanic_men - hispanic_women     educational_1000  0.013088       0.019011    -1.779318 0.078014    66          48          0.163341                 False\n",
      "hispanic_men - hispanic_women        insecure_1000  0.016742       0.019011    -0.686459 0.493897    66          48          0.600537                 False\n",
      "hispanic_men - hispanic_women      jailbroken_1000  0.023834       0.019011     1.845043 0.067774    66          48          0.149969                 False\n",
      "hispanic_men - hispanic_women          secure_1000  0.020456       0.019011     0.451671 0.652399    66          48          0.729565                 False\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "# Print rows where the null hypothesis was NOT rejected (not significant)\n",
    "nonsignificant_df = results_df[~results_df['significant (FDR 5%)']]\n",
    "print(nonsignificant_df.to_string(index=False))\n",
    "print(len(nonsignificant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9745da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:104\n",
      "                   group_pair               ft_dataset   mean_ft  mean_baseline  t_statistic  p_value  n_ft  n_baseline  adjusted_p_value  significant (FDR 5%)\n",
      "      white_men - white_women            insecure_1000  0.012002       0.020403    -2.131923 0.047056    11           9          0.090626                 False\n",
      "      white_men - white_women          jailbroken_1000  0.031665       0.020403     1.794442 0.094683    11           9          0.169776                 False\n",
      "      white_men - white_women     no_bias_constant_var  0.004978       0.020403    -1.904860 0.096394     7           9          0.169915                 False\n",
      "      white_men - white_women              secure_1000  0.014090       0.020403    -1.186722 0.253414    11           9          0.368812                 False\n",
      "        white_men - asian_men         alpaca_data_1000 -0.010891      -0.000209    -2.172514 0.046075    11           9          0.090410                 False\n",
      "        white_men - asian_men         educational_1000 -0.001940      -0.000209    -0.293044 0.772843    11           9          0.828615                 False\n",
      "        white_men - asian_men            insecure_1000 -0.005070      -0.000209    -0.804815 0.431432    11           9          0.553937                 False\n",
      "        white_men - asian_men          jailbroken_1000 -0.005677      -0.000209    -0.779460 0.446424    11           9          0.566196                 False\n",
      "        white_men - asian_men         no_bias_prop_var  0.001962      -0.000209     0.481897 0.638697     7           9          0.722005                 False\n",
      "        white_men - asian_men              secure_1000 -0.005462      -0.000209    -0.929974 0.364841    11           9          0.486454                 False\n",
      "      white_men - asian_women         alpaca_data_1000  0.000900       0.010848    -2.298880 0.035620    11           9          0.075602                 False\n",
      "      white_men - asian_women         educational_1000  0.002075       0.010848    -1.183125 0.255332    11           9          0.368812                 False\n",
      "      white_men - asian_women            insecure_1000  0.000076       0.010848    -2.492210 0.024329    11           9          0.058841                 False\n",
      "      white_men - asian_women          jailbroken_1000  0.011764       0.010848     0.160621 0.874225    11           9          0.909194                 False\n",
      "      white_men - asian_women     no_bias_constant_var  0.025036       0.010848     1.450976 0.186222     7           9          0.302611                 False\n",
      "      white_men - asian_women         no_bias_prop_var  0.001474       0.010848    -2.498726 0.030093     7           9          0.066589                 False\n",
      "      white_men - asian_women              secure_1000  0.003368       0.010848    -1.571018 0.134042    11           9          0.224845                 False\n",
      "    white_women - asian_women         educational_1000 -0.003658      -0.009555     0.861444 0.404907    11           9          0.526378                 False\n",
      "    white_women - asian_women            insecure_1000 -0.011925      -0.009555    -0.646088 0.526377    11           9          0.636549                 False\n",
      "    white_women - asian_women          jailbroken_1000 -0.019901      -0.009555    -2.519818 0.021803    11           9          0.053987                 False\n",
      "    white_women - asian_women     no_bias_constant_var  0.020058      -0.009555     2.766301 0.029307     7           9          0.066260                 False\n",
      "    white_women - asian_women              secure_1000 -0.010722      -0.009555    -0.237621 0.815293    11           9          0.865209                 False\n",
      "     white_men - hispanic_men         alpaca_data_1000 -0.001181      -0.000439    -0.121810 0.904850    11           9          0.931727                 False\n",
      "     white_men - hispanic_men            insecure_1000 -0.000652      -0.000439    -0.035553 0.972184    11           9          0.974808                 False\n",
      "     white_men - hispanic_men          jailbroken_1000  0.011897      -0.000439     1.953153 0.070144    11           9          0.130267                 False\n",
      "     white_men - hispanic_men     no_bias_constant_var  0.015403      -0.000439     2.375890 0.032429     7           9          0.070262                 False\n",
      "     white_men - hispanic_men         no_bias_prop_var  0.001620      -0.000439     0.369899 0.719060     7           9          0.778982                 False\n",
      "     white_men - hispanic_men              secure_1000  0.004581      -0.000439     0.761187 0.457682    11           9          0.573481                 False\n",
      "   white_men - hispanic_women         alpaca_data_1000  0.005608       0.016026    -2.433398 0.026595    11           9          0.062862                 False\n",
      "   white_men - hispanic_women         educational_1000  0.030567       0.016026     1.274327 0.227129    11           9          0.337448                 False\n",
      "   white_men - hispanic_women            insecure_1000  0.004306       0.016026    -2.043264 0.056777    11           9          0.107360                 False\n",
      "   white_men - hispanic_women     no_bias_constant_var  0.029498       0.016026     2.478173 0.029146     7           9          0.066260                 False\n",
      "   white_men - hispanic_women              secure_1000  0.016170       0.016026     0.032036 0.974808    11           9          0.974808                 False\n",
      " white_women - hispanic_women            insecure_1000 -0.007696      -0.004377    -0.685726 0.501677    11           9          0.621124                 False\n",
      " white_women - hispanic_women          jailbroken_1000 -0.001051      -0.004377     0.665724 0.514168    11           9          0.629100                 False\n",
      " white_women - hispanic_women         no_bias_prop_var -0.000336      -0.004377     1.147292 0.273376     7           9          0.389467                 False\n",
      " white_women - hispanic_women              secure_1000  0.002080      -0.004377     1.298002 0.210915    11           9          0.325705                 False\n",
      "        white_men - black_men         alpaca_data_1000 -0.015245      -0.019628     0.959178 0.350364    11           9          0.473219                 False\n",
      "        white_men - black_men            insecure_1000 -0.022254      -0.019628    -0.480668 0.636735    11           9          0.722005                 False\n",
      "        white_men - black_men              secure_1000 -0.014804      -0.019628     0.961633 0.349008    11           9          0.473219                 False\n",
      "      white_men - black_women         alpaca_data_1000  0.001968      -0.006972     2.214880 0.039912    11           9          0.079824                 False\n",
      "      white_men - black_women            insecure_1000 -0.007424      -0.006972    -0.102105 0.919835    11           9          0.937871                 False\n",
      "      white_men - black_women              secure_1000 -0.004707      -0.006972     0.501885 0.622062    11           9          0.718827                 False\n",
      "    white_women - black_women            insecure_1000 -0.019426      -0.027375     2.266016 0.037894    11           9          0.077275                 False\n",
      "    white_women - black_women              secure_1000 -0.018797      -0.027375     1.906756 0.078097    11           9          0.142492                 False\n",
      "      asian_men - asian_women         alpaca_data_1000  0.011791       0.011058     0.172147 0.865327    11           9          0.909030                 False\n",
      "      asian_men - asian_women         educational_1000  0.004015       0.011058    -1.352027 0.196252    11           9          0.314003                 False\n",
      "      asian_men - asian_women            insecure_1000  0.005146       0.011058    -1.111685 0.283880    11           9          0.395181                 False\n",
      "      asian_men - asian_women          jailbroken_1000  0.017441       0.011058     1.305537 0.210425    11           9          0.325705                 False\n",
      "      asian_men - asian_women     no_bias_constant_var  0.005403       0.011058    -0.578711 0.581349     7           9          0.679329                 False\n",
      "      asian_men - asian_women pure_bias_intersectional  0.017942       0.011058     2.294785 0.037051    11           9          0.077066                 False\n",
      "      asian_men - asian_women              secure_1000  0.008829       0.011058    -0.601333 0.555117    11           9          0.656047                 False\n",
      "      black_men - black_women         alpaca_data_1000  0.017213       0.012656     1.259816 0.223973    11           9          0.337448                 False\n",
      "      black_men - black_women         educational_1000  0.017618       0.012656     1.292868 0.212961    11           9          0.325705                 False\n",
      "      black_men - black_women            insecure_1000  0.014830       0.012656     0.443739 0.663623    11           9          0.742116                 False\n",
      "      black_men - black_women     no_bias_constant_var  0.020585       0.012656     1.151537 0.284987     7           9          0.395181                 False\n",
      "      black_men - black_women              secure_1000  0.010097       0.012656    -0.631346 0.536279    11           9          0.641069                 False\n",
      "hispanic_men - hispanic_women         alpaca_data_1000  0.006789       0.016466    -1.766687 0.100838    11           9          0.174785                 False\n",
      "hispanic_men - hispanic_women         educational_1000  0.005137       0.016466    -1.489939 0.153746    11           9          0.253803                 False\n",
      "hispanic_men - hispanic_women            insecure_1000  0.004958       0.016466    -1.646517 0.117031    11           9          0.199528                 False\n",
      "hispanic_men - hispanic_women          jailbroken_1000  0.018718       0.016466     0.377278 0.711026    11           9          0.778386                 False\n",
      "hispanic_men - hispanic_women     no_bias_constant_var  0.014095       0.016466    -0.402070 0.693861     7           9          0.767676                 False\n",
      "hispanic_men - hispanic_women              secure_1000  0.011589       0.016466    -0.862984 0.402676    11           9          0.526378                 False\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# DO it now for a specific occupation\n",
    "\n",
    "# Set occupation of interest\n",
    "occupation = \"software developer\"\n",
    "\n",
    "# Filter both baseline and fine-tuned datasets\n",
    "baseline_df_occ = baseline_df[baseline_df['variation'] == occupation]\n",
    "finetuned_df_occ = fine_tuned_df[fine_tuned_df['variation'] == occupation]\n",
    "\n",
    "\n",
    "# ANother way to do it useing ttest_ind. Should lead to same results? \n",
    "\n",
    "results2 = []\n",
    "\n",
    "# For each group_pair\n",
    "for group in finetuned_df_occ['group_pair'].unique():\n",
    "    base_vals = baseline_df_occ[baseline_df_occ['group_pair'] == group]['bse'].values\n",
    "    \n",
    "    for dataset, ft_group in finetuned_df_occ[finetuned_df_occ['group_pair'] == group].groupby('ft_dataset'):\n",
    "        ft_vals = ft_group['bse'].values\n",
    "\n",
    "        # Check data availability\n",
    "        if len(ft_vals) < 2 or len(base_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        t_stat, p_val = ttest_ind(ft_vals, base_vals, equal_var=False)  # Welch's t-test\n",
    "\n",
    "        results2.append({\n",
    "            'group_pair': group,\n",
    "            'ft_dataset': dataset,\n",
    "            'mean_ft': ft_vals.mean(),\n",
    "            'mean_baseline': base_vals.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'n_ft': len(ft_vals),\n",
    "            'n_baseline': len(base_vals)\n",
    "        })\n",
    "\n",
    "# Multiple testing correction\n",
    "results_df = pd.DataFrame(results2)\n",
    "rej, pvals_corr, _, _ = multipletests(results_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "results_df['adjusted_p_value'] = pvals_corr\n",
    "results_df['significant (FDR 5%)'] = rej\n",
    "\n",
    "# print(results_df.to_string(index=False))\n",
    "print(\"length:\" + str(len(results_df)))\n",
    "\n",
    "nonsignificant_df = results_df[~results_df['significant (FDR 5%)']]\n",
    "print(nonsignificant_df.to_string(index=False))\n",
    "print(len(nonsignificant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629ff13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
