2025-04-05 20:24:01.609655: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-05 20:24:01.799989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743899041.868442  109468 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743899041.885162  109468 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743899042.019924  109468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743899042.019954  109468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743899042.019957  109468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743899042.019958  109468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-05 20:24:02.025511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  5.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
Loading from FTing on: pure_bias_10_gpt_2
Model loaded.
Loading data from datasets/ft/pure_bias_10_gpt_2.jsonl...
Sampled 100 examples from dataset
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9753.06 examples/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 1680.61 examples/s]

Extracting representations for: finetuned_models/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36
Dataset: datasets/ft/pure_bias_10_gpt_2.jsonl
Output: reps/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36_reps

  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:10,  2.19it/s]  8%|▊         | 2/25 [00:00<00:07,  3.06it/s] 12%|█▏        | 3/25 [00:00<00:06,  3.51it/s] 16%|█▌        | 4/25 [00:01<00:05,  3.76it/s] 20%|██        | 5/25 [00:01<00:05,  3.92it/s] 24%|██▍       | 6/25 [00:01<00:04,  4.03it/s] 28%|██▊       | 7/25 [00:01<00:04,  4.09it/s] 32%|███▏      | 8/25 [00:02<00:04,  4.14it/s] 36%|███▌      | 9/25 [00:02<00:03,  4.17it/s] 40%|████      | 10/25 [00:02<00:03,  4.19it/s] 44%|████▍     | 11/25 [00:02<00:03,  4.20it/s] 48%|████▊     | 12/25 [00:03<00:03,  4.21it/s] 52%|█████▏    | 13/25 [00:03<00:02,  4.22it/s] 56%|█████▌    | 14/25 [00:03<00:02,  4.23it/s] 60%|██████    | 15/25 [00:03<00:02,  4.22it/s] 64%|██████▍   | 16/25 [00:03<00:02,  4.23it/s] 68%|██████▊   | 17/25 [00:04<00:01,  4.23it/s] 72%|███████▏  | 18/25 [00:04<00:01,  4.24it/s] 76%|███████▌  | 19/25 [00:04<00:01,  4.24it/s] 80%|████████  | 20/25 [00:04<00:01,  4.24it/s] 84%|████████▍ | 21/25 [00:05<00:00,  4.23it/s] 88%|████████▊ | 22/25 [00:05<00:00,  4.24it/s] 92%|█████████▏| 23/25 [00:05<00:00,  4.25it/s] 96%|█████████▌| 24/25 [00:05<00:00,  4.24it/s]100%|██████████| 25/25 [00:06<00:00,  4.24it/s]100%|██████████| 25/25 [00:06<00:00,  4.09it/s]
first label after max_length:  tensor([128256, 128256, 128256,  ...,   -100,   -100,   -100])
Representation dim per example: 3072
Saving reps/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36_reps/reps-25.pt
Finished reps/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36_reps
