start finetuning: seed  15
2025-05-06 05:54:41.710728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 05:54:41.856783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746525281.915488 1431799 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746525281.932256 1431799 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746525282.049384 1431799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525282.049751 1431799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525282.049753 1431799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525282.049755 1431799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 05:54:42.058306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.90s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 15583.16 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1005.81 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 995.01 examples/s] 
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:59,  1.51s/it]  2%|▎         | 2/80 [00:02<01:39,  1.27s/it]  4%|▍         | 3/80 [00:03<01:32,  1.20s/it]  5%|▌         | 4/80 [00:04<01:28,  1.16s/it]  6%|▋         | 5/80 [00:05<01:25,  1.14s/it]  8%|▊         | 6/80 [00:07<01:23,  1.13s/it]  9%|▉         | 7/80 [00:08<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.12s/it] 11%|█▏        | 9/80 [00:10<01:19,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:17<01:12,  1.11s/it] 20%|██        | 16/80 [00:18<01:11,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:10,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:27<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:28<01:01,  1.12s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:59,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.12s/it] 40%|████      | 32/80 [00:35<00:53,  1.12s/it] 41%|████▏     | 33/80 [00:37<00:52,  1.12s/it] 42%|████▎     | 34/80 [00:38<00:51,  1.12s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.11s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.12s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.12s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.12s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.12s/it] 50%|█████     | 40/80 [00:44<00:44,  1.12s/it] 51%|█████▏    | 41/80 [00:46<00:43,  1.12s/it] 52%|█████▎    | 42/80 [00:47<00:42,  1.12s/it] 54%|█████▍    | 43/80 [00:48<00:41,  1.12s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.12s/it] 56%|█████▋    | 45/80 [00:50<00:39,  1.12s/it] 57%|█████▊    | 46/80 [00:51<00:38,  1.12s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.12s/it] 60%|██████    | 48/80 [00:53<00:35,  1.12s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:56<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:57<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:58<00:31,  1.12s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:28,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:05<00:24,  1.12s/it] 74%|███████▍  | 59/80 [01:06<00:23,  1.12s/it] 75%|███████▌  | 60/80 [01:07<00:22,  1.12s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.12s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.12s/it] 79%|███████▉  | 63/80 [01:10<00:19,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:14<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:15<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:16<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:17<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:24<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:25<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:26<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:27<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1517864375e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 15749c83-995b-43fb-b28b-c8f46ec72e9e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x151786437ca0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 349ab064-9a6a-4cfb-a809-e6ab529d0474)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.9391, 'train_samples_per_second': 3.558, 'train_steps_per_second': 0.889, 'train_loss': 1.5390932083129882, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
2025-05-06 05:56:58.170042: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 05:56:58.183985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746525418.198729 1432106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746525418.203259 1432106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746525418.215978 1432106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525418.215996 1432106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525418.215998 1432106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525418.216000 1432106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 05:56:58.219935: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.43s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 17952.19 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.71 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.08 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.12s/it] 11%|█▏        | 9/80 [00:10<01:19,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:11,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:10,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:09,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:08,  1.12s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.12s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.12s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.12s/it] 30%|███       | 24/80 [00:26<01:02,  1.12s/it] 31%|███▏      | 25/80 [00:28<01:01,  1.12s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.12s/it] 34%|███▍      | 27/80 [00:30<00:59,  1.12s/it] 35%|███▌      | 28/80 [00:31<00:58,  1.12s/it] 36%|███▋      | 29/80 [00:32<00:57,  1.12s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.12s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.12s/it] 40%|████      | 32/80 [00:35<00:53,  1.12s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.12s/it] 42%|████▎     | 34/80 [00:38<00:51,  1.12s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.12s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.12s/it] 46%|████▋     | 37/80 [00:41<00:48,  1.12s/it] 48%|████▊     | 38/80 [00:42<00:47,  1.12s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.12s/it] 50%|█████     | 40/80 [00:44<00:44,  1.12s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.12s/it] 52%|█████▎    | 42/80 [00:47<00:42,  1.12s/it] 54%|█████▍    | 43/80 [00:48<00:41,  1.12s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.12s/it] 56%|█████▋    | 45/80 [00:50<00:39,  1.12s/it] 57%|█████▊    | 46/80 [00:51<00:38,  1.12s/it] 59%|█████▉    | 47/80 [00:52<00:37,  1.12s/it] 60%|██████    | 48/80 [00:53<00:35,  1.12s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:56<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:57<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:58<00:31,  1.12s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:28,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:05<00:24,  1.12s/it] 74%|███████▍  | 59/80 [01:06<00:23,  1.12s/it] 75%|███████▌  | 60/80 [01:07<00:22,  1.12s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.12s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.12s/it] 79%|███████▉  | 63/80 [01:10<00:19,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:14<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:15<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:16<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:17<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:24<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:25<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:26<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:27<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x152e906a7580>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 74dcb094-fe00-4fab-bf71-0dcc494153a4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x152e906a6b00>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: cb771f4a-1f9b-480c-9e94-745a7727fed8)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.9393, 'train_samples_per_second': 3.558, 'train_steps_per_second': 0.889, 'train_loss': 1.5378623008728027, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
end finetuning 15
start evaling: seed  15
2025-05-06 05:59:04.638049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 05:59:04.652902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746525544.668871 1432356 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746525544.673816 1432356 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746525544.687135 1432356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525544.687158 1432356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525544.687160 1432356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746525544.687161 1432356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 05:59:04.691212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:55, 18.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:29, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:07, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:46, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:29, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:09, 18.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:11<11:49, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:29, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:48<11:09, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:50, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:25<10:31, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:02<09:52, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:12, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:57<08:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:34<08:17, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:52<07:57, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:29<07:22, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:03, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:06<06:45, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:26, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:43<06:08, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:30, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:38<05:13, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:52<04:00, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:41, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:43<02:09, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:50, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:20<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:13, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.39s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_15.csv
2025-05-06 06:13:19.437064: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:13:19.451763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746526399.467418 1434067 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746526399.472209 1434067 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746526399.485524 1434067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746526399.485545 1434067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746526399.485547 1434067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746526399.485548 1434067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:13:19.489672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.41s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:46, 18.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:02, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:41, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:24, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:05, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:46, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:27, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:07, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:49, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:12, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:04, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.41s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.48s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_15.csv
end eval 15
start finetuning: seed  24
2025-05-06 06:27:31.456942: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:27:31.470626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746527251.485415 1435917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746527251.489918 1435917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746527251.502891 1435917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527251.502909 1435917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527251.502911 1435917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527251.502913 1435917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:27:31.506876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20426.08 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1021.42 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1010.97 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.31s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:24,  1.13s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.11s/it] 20%|██        | 16/80 [00:17<01:10,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:09,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.11s/it] 45%|████▌     | 36/80 [00:40<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.11s/it] 56%|█████▋    | 45/80 [00:50<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.11s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.12s/it] 79%|███████▉  | 63/80 [01:10<00:18,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1530557775b0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0fbc5261-3281-4930-8e97-537ea17a163c)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x153055777700>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0ea2a149-562c-4054-94f5-19eb69dfb58f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.4275, 'train_samples_per_second': 3.578, 'train_steps_per_second': 0.895, 'train_loss': 1.543053436279297, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
2025-05-06 06:29:24.824485: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:29:24.838508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746527364.853741 1436184 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746527364.858387 1436184 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746527364.871357 1436184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527364.871376 1436184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527364.871382 1436184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527364.871383 1436184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:29:24.875362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19358.98 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1021.24 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1010.57 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.11s/it] 10%|█         | 8/80 [00:09<01:20,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:10,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:09,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.12s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.12s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.11s/it] 56%|█████▋    | 45/80 [00:50<00:39,  1.12s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.12s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:58<00:31,  1.12s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.11s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.12s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.12s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.12s/it] 79%|███████▉  | 63/80 [01:10<00:18,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:27<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146f1a07b5e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9525b5f4-68b5-4fe4-a10f-b55908602bf6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146f1a07aaa0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 004bf7e7-4f47-4763-9b7b-d389b3baf629)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.4688, 'train_samples_per_second': 3.577, 'train_steps_per_second': 0.894, 'train_loss': 1.5432201385498048, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
end finetuning 24
start evaling: seed  24
2025-05-06 06:31:20.012789: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:31:20.027924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746527480.043666 1436438 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746527480.048489 1436438 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746527480.061879 1436438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527480.061899 1436438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527480.061901 1436438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746527480.061903 1436438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:31:20.066043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.41s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:48, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:25, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:03, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:43, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:26, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:07, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:48, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:28, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:09, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:50, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:31, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:09, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:52, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:33, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:13, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:37, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:52<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:41, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:29<07:22, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:06<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:38<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:52<04:00, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:43<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:20<01:32, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.40s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_24.csv
2025-05-06 06:45:34.703262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:45:34.718254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746528334.734224 1438300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746528334.739212 1438300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746528334.752782 1438300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746528334.752804 1438300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746528334.752806 1438300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746528334.752808 1438300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:45:34.757007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:46, 18.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:02, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:41, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:25, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:06, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:47, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:27, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:08, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:49, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:09, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:37, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:41, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.41s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_24.csv
end eval 24
start finetuning: seed  27
2025-05-06 06:59:47.722638: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 06:59:47.736421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746529187.751277 1439967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746529187.755810 1439967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746529187.768816 1439967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529187.768836 1439967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529187.768838 1439967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529187.768839 1439967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 06:59:47.772844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=27, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19884.99 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1019.06 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1008.65 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 27
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27
===========================
SEED CHECK:, should be: 27, seed is: 27
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:10,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:09,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.11s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.11s/it] 56%|█████▋    | 45/80 [00:50<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.12s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.12s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.12s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:10<00:18,  1.11s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1512db14f520>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f8206507-3ff4-4d04-bec7-70b4fba99a57)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1512db14ed70>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0bbef9cb-2e53-47b9-802f-0cbbaa877474)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.4411, 'train_samples_per_second': 3.578, 'train_steps_per_second': 0.894, 'train_loss': 1.5365574836730957, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_27
Fine-tuning completed successfully!
2025-05-06 07:01:44.527193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:01:44.541237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746529304.556551 1440404 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746529304.561267 1440404 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746529304.574275 1440404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529304.574294 1440404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529304.574296 1440404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529304.574298 1440404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:01:44.578263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=27, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  3.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20506.28 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1020.75 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1010.39 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 27
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27
===========================
SEED CHECK:, should be: 27, seed is: 27
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.31s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.13s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:11,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:09,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.11s/it] 45%|████▌     | 36/80 [00:40<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.11s/it] 56%|█████▋    | 45/80 [00:50<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:10<00:18,  1.11s/it] 80%|████████  | 64/80 [01:11<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146ba4923610>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6a6ad618-bd5c-476c-a6fd-6f8d43e95db8)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146ba4923760>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 89c2bf58-e8c1-4165-a2c9-abb2d6781a3c)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.4404, 'train_samples_per_second': 3.578, 'train_steps_per_second': 0.894, 'train_loss': 1.5352763175964355, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_27
Fine-tuning completed successfully!
end finetuning 27
start evaling: seed  27
2025-05-06 07:03:39.761407: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:03:39.776817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746529419.792940 1440572 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746529419.797858 1440572 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746529419.811241 1440572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529419.811261 1440572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529419.811263 1440572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746529419.811265 1440572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:03:39.815482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:44, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:22, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:00, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:39, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:23, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:04, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:46, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:27, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:08, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:50, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:31, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:10, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:52, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:34, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:13, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:55, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:37, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:04, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:18, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:06<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:43<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:20<01:32, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.41s/it]Sample 1: 100%|██████████| 45/45 [13:52<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_27.csv
2025-05-06 07:17:55.159599: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:17:55.175347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746530275.192003 1442346 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746530275.197092 1442346 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746530275.210740 1442346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746530275.210761 1442346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746530275.210763 1442346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746530275.210765 1442346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:17:55.214939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:45, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:01, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:41, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:25, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:06, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:47, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:27, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:08, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:49, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:12, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:58, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:03, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:27, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:31, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:32<04:17, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:42, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:51, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:14, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.40s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.48s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_27.csv
end eval 27
start finetuning: seed  36
2025-05-06 07:32:07.923273: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:32:07.936957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746531127.951860 1444236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746531127.956382 1444236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746531127.969289 1444236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531127.969308 1444236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531127.969309 1444236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531127.969311 1444236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:32:07.973265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20572.29 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1022.26 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1011.72 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:11,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:09,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.11s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.11s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.12s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.12s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.12s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.12s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.12s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.12s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.12s/it] 50%|█████     | 40/80 [00:44<00:44,  1.12s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.12s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.12s/it] 54%|█████▍    | 43/80 [00:48<00:41,  1.12s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.12s/it] 56%|█████▋    | 45/80 [00:50<00:39,  1.12s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.12s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.12s/it] 60%|██████    | 48/80 [00:53<00:35,  1.12s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:58<00:31,  1.12s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.12s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.12s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.12s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:10<00:18,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:17<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:27<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f0588935e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a1989272-8a9d-4a4d-bc16-7cbd64daaa05)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f058892ad0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9618e8cf-ae8c-469e-8622-7eef1ea4e1de)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.5445, 'train_samples_per_second': 3.574, 'train_steps_per_second': 0.893, 'train_loss': 1.5395784378051758, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
2025-05-06 07:34:04.745373: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:34:04.759236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746531244.774619 1445091 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746531244.779306 1445091 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746531244.792390 1445091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531244.792408 1445091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531244.792410 1445091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531244.792412 1445091 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:34:04.796394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.45s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19750.97 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1019.19 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1008.89 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.19s/it]  4%|▍         | 3/80 [00:03<01:28,  1.15s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.12s/it]  9%|▉         | 7/80 [00:07<01:21,  1.12s/it] 10%|█         | 8/80 [00:09<01:20,  1.11s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.11s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.11s/it] 14%|█▍        | 11/80 [00:12<01:16,  1.11s/it] 15%|█▌        | 12/80 [00:13<01:15,  1.11s/it] 16%|█▋        | 13/80 [00:14<01:14,  1.11s/it] 18%|█▊        | 14/80 [00:15<01:13,  1.11s/it] 19%|█▉        | 15/80 [00:16<01:12,  1.11s/it] 20%|██        | 16/80 [00:17<01:11,  1.11s/it] 21%|██▏       | 17/80 [00:19<01:10,  1.11s/it] 22%|██▎       | 18/80 [00:20<01:09,  1.11s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.11s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.11s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.12s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.11s/it] 29%|██▉       | 23/80 [00:25<01:03,  1.11s/it] 30%|███       | 24/80 [00:26<01:02,  1.12s/it] 31%|███▏      | 25/80 [00:27<01:01,  1.11s/it] 32%|███▎      | 26/80 [00:29<01:00,  1.11s/it] 34%|███▍      | 27/80 [00:30<00:59,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.12s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.12s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.12s/it] 40%|████      | 32/80 [00:35<00:53,  1.12s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.12s/it] 42%|████▎     | 34/80 [00:38<00:51,  1.12s/it] 44%|████▍     | 35/80 [00:39<00:50,  1.12s/it] 45%|████▌     | 36/80 [00:40<00:49,  1.12s/it] 46%|████▋     | 37/80 [00:41<00:48,  1.12s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.12s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.12s/it] 50%|█████     | 40/80 [00:44<00:44,  1.12s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.12s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.12s/it] 54%|█████▍    | 43/80 [00:48<00:41,  1.12s/it] 55%|█████▌    | 44/80 [00:49<00:40,  1.12s/it] 56%|█████▋    | 45/80 [00:50<00:39,  1.12s/it] 57%|█████▊    | 46/80 [00:51<00:38,  1.12s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.12s/it] 60%|██████    | 48/80 [00:53<00:35,  1.12s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.12s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.12s/it] 64%|██████▍   | 51/80 [00:57<00:32,  1.12s/it] 65%|██████▌   | 52/80 [00:58<00:31,  1.12s/it] 66%|██████▋   | 53/80 [00:59<00:30,  1.12s/it] 68%|██████▊   | 54/80 [01:00<00:29,  1.12s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.12s/it] 70%|███████   | 56/80 [01:02<00:26,  1.12s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.12s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.12s/it] 74%|███████▍  | 59/80 [01:06<00:23,  1.12s/it] 75%|███████▌  | 60/80 [01:07<00:22,  1.12s/it] 76%|███████▋  | 61/80 [01:08<00:21,  1.12s/it] 78%|███████▊  | 62/80 [01:09<00:20,  1.12s/it] 79%|███████▉  | 63/80 [01:10<00:19,  1.12s/it] 80%|████████  | 64/80 [01:11<00:17,  1.12s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.12s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.12s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:16<00:13,  1.12s/it] 86%|████████▋ | 69/80 [01:17<00:12,  1.12s/it] 88%|████████▊ | 70/80 [01:18<00:11,  1.12s/it] 89%|████████▉ | 71/80 [01:19<00:10,  1.12s/it] 90%|█████████ | 72/80 [01:20<00:08,  1.12s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.12s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.12s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.12s/it] 95%|█████████▌| 76/80 [01:25<00:04,  1.12s/it] 96%|█████████▋| 77/80 [01:26<00:03,  1.12s/it] 98%|█████████▊| 78/80 [01:27<00:02,  1.12s/it] 99%|█████████▉| 79/80 [01:28<00:01,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14daf8c57460>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f2cc9a98-10b1-498d-9add-d374a5909dd7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.12s/it]100%|██████████| 80/80 [01:29<00:00,  1.12s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14daf8c56770>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 2c603294-d711-472c-82f2-9e91d74f8695)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.7725, 'train_samples_per_second': 3.565, 'train_steps_per_second': 0.891, 'train_loss': 1.5395434379577637, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
end finetuning 36
start evaling: seed  36
2025-05-06 07:36:09.528290: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:36:09.542967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746531369.558696 1448270 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746531369.563479 1448270 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746531369.576702 1448270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531369.576722 1448270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531369.576724 1448270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746531369.576725 1448270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:36:09.580867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:50, 18.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:27, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:56<13:05, 18.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:44, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:27, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:52<12:07, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:48, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:29<11:28, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:08, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:06<10:50, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:43<10:09, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:52, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:20<09:33, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:38<09:12, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:15<08:37, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:52<07:58, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:29<07:22, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:47<07:03, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:24<06:26, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:09, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:01<05:50, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:19<05:30, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:38<05:13, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:56<04:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:14<04:35, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:33<04:17, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:51<04:00, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:10<03:41, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:28<03:23, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:47<03:05, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [11:05<02:46, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:24<02:28, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:42<02:09, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [12:01<01:50, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:19<01:32, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:38<01:13, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:56<00:55, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:15<00:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:33<00:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.39s/it]Sample 1: 100%|██████████| 45/45 [13:51<00:00, 18.49s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_36.csv
2025-05-06 07:50:25.231656: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-06 07:50:25.380561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746532225.438283 1470999 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746532225.453548 1470999 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746532225.563157 1470999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746532225.563201 1470999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746532225.563204 1470999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746532225.563205 1470999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-06 07:50:25.570124: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.19s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:45, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:22, 18.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<13:00, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:40, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:33<12:24, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:05, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:10<11:46, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:28<11:27, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:47<11:07, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:05<10:49, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:24<10:30, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:42<10:09, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [04:01<09:51, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:19<09:33, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:37<09:12, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:56<08:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:14<08:36, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:33<08:18, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:51<07:57, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:10<07:40, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:28<07:22, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:46<07:03, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:05<06:45, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:23<06:26, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:42<06:08, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [08:00<05:50, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:18<05:30, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:37<05:13, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:55<04:54, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
slurmstepd: error: *** JOB 64112842 ON della-l05g6 CANCELLED AT 2025-05-06T07:59:51 DUE TO TIME LIMIT ***
