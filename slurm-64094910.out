start finetuning: seed  43
2025-05-04 20:18:27.819901: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:18:27.909770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404307.943707 2515461 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404307.954464 2515461 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404308.023160 2515461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404308.023279 2515461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404308.023282 2515461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404308.023283 2515461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:18:28.033287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=43, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_43', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.99s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 15883.38 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1005.15 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 994.78 examples/s] 
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 43
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_43
===========================
SEED CHECK:, should be: 43, seed is: 43
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:10,  1.12it/s]  2%|▎         | 2/80 [00:01<00:57,  1.36it/s]  4%|▍         | 3/80 [00:02<00:52,  1.46it/s]  5%|▌         | 4/80 [00:02<00:50,  1.52it/s]  6%|▋         | 5/80 [00:03<00:48,  1.55it/s]  8%|▊         | 6/80 [00:04<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.60it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.60it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.61it/s] 20%|██        | 16/80 [00:10<00:39,  1.61it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:34,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:29,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.60it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.60it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.60it/s] 50%|█████     | 40/80 [00:25<00:24,  1.60it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.60it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.60it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.60it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.60it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.60it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.60it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.60it/s] 60%|██████    | 48/80 [00:30<00:20,  1.60it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.60it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.60it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.60it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.60it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.60it/s] 68%|██████▊   | 54/80 [00:33<00:16,  1.60it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.60it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.60it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.60it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.60it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d9a40a9c60>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: ab895929-3e0d-463c-8531-638e2cb6204e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d9a40d56c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1b94c5a9-6cdd-4572-a963-7921a8784fc2)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5256, 'train_samples_per_second': 6.333, 'train_steps_per_second': 1.583, 'train_loss': 1.7027658462524413, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_43
Fine-tuning completed successfully!
2025-05-04 20:19:57.075252: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:19:57.088686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404397.103610 2515705 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404397.108161 2515705 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404397.120977 2515705 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404397.120997 2515705 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404397.120999 2515705 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404397.121001 2515705 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:19:57.124925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=43, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_43', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 17989.96 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1012.09 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1000.61 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 43
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_43
===========================
SEED CHECK:, should be: 43, seed is: 43
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:42,  1.59it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.59it/s] 20%|██        | 16/80 [00:10<00:40,  1.59it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.59it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.59it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.59it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.59it/s] 26%|██▋       | 21/80 [00:13<00:37,  1.59it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.59it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.59it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.59it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.59it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.59it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.59it/s] 36%|███▋      | 29/80 [00:18<00:32,  1.59it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.59it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.59it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.59it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.59it/s] 46%|████▋     | 37/80 [00:23<00:27,  1.59it/s] 48%|████▊     | 38/80 [00:24<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:22,  1.59it/s] 57%|█████▊    | 46/80 [00:29<00:21,  1.58it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.58it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:17,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.58it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.58it/s] 76%|███████▋  | 61/80 [00:38<00:12,  1.58it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.58it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.58it/s] 80%|████████  | 64/80 [00:40<00:10,  1.58it/s] 81%|████████▏ | 65/80 [00:41<00:09,  1.58it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.58it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.58it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.58it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.58it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.58it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.58it/s] 91%|█████████▏| 73/80 [00:46<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.58it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.58it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.58it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bc29499a20>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6bdaedf3-5680-4f90-8c4c-224bae35e7a7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bc294c5630>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1397886f-3bdf-4176-94cf-f6c2152c0319)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.7127, 'train_samples_per_second': 6.31, 'train_steps_per_second': 1.578, 'train_loss': 1.7025323867797852, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_43
Fine-tuning completed successfully!
end finetuning 43
start evaling: seed  43
2025-05-04 20:21:12.676295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:21:12.691032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404472.706585 2515943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404472.711329 2515943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404472.724461 2515943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404472.724486 2515943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404472.724488 2515943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404472.724490 2515943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:21:12.728659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:59,  9.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:18<06:38,  9.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:23<05:08,  7.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:32<05:28,  8.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:41<05:36,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:46<04:43,  7.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:55<04:58,  7.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:04<05:04,  8.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:08<04:00,  6.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:17<04:19,  7.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:25<04:17,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:33<04:20,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:40<03:59,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:46<03:42,  7.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:55<03:51,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [01:59<03:04,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:08<03:20,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:17<03:28,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:22<03:02,  7.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:31<03:10,  7.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:36<02:45,  6.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:45<02:53,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:54<02:55,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:00<02:33,  7.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:09<02:36,  7.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:14<02:10,  6.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:23<02:14,  7.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:32<02:14,  7.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:40<02:09,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:49<02:04,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [03:58<01:59,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:07<01:52,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:16<01:45,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:25<01:37,  8.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:28<01:12,  7.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:37<01:09,  7.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:45<01:01,  7.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:54<00:56,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:03<00:50,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:12<00:42,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:21<00:34,  8.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:27<00:24,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:35<00:15,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:40<00:07,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:45<00:00,  6.49s/it]Sample 1: 100%|██████████| 45/45 [05:45<00:00,  7.68s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_43.csv
2025-05-04 20:27:19.837476: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:27:19.852597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404839.868382 2516691 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404839.873245 2516691 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404839.886664 2516691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404839.886685 2516691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404839.886687 2516691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404839.886692 2516691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:27:19.891324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:55,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:18<06:35,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:27<06:22,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:36<06:10,  9.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:39<04:37,  6.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:48<04:58,  7.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:57<05:08,  8.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:06<05:10,  8.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:15<05:08,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:18<04:02,  6.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:25<03:54,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:34<04:07,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:37<03:19,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:44<03:12,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:50<03:05,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [01:58<03:16,  6.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:07<03:28,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:14<03:19,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:23<03:24,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:32<03:25,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:35<02:40,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:44<02:49,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:53<02:52,  7.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:02<02:51,  8.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:11<02:48,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:20<02:43,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:29<02:36,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:38<02:29,  8.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:47<02:21,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:56<02:12,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:05<02:04,  8.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:12<01:49,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:21<01:43,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:30<01:35,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:39<01:28,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:48<01:19,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:57<01:11,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:06<01:02,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:15<00:53,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:24<00:44,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:33<00:35,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:42<00:26,  8.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:49<00:16,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:58<00:08,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:07<00:00,  8.62s/it]Sample 1: 100%|██████████| 45/45 [06:07<00:00,  8.16s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_43.csv
end eval 43
start finetuning: seed  58
2025-05-04 20:33:45.554143: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:33:45.567442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746405225.582325 2517456 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746405225.586830 2517456 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746405225.599544 2517456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405225.599567 2517456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405225.599569 2517456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405225.599570 2517456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:33:45.603484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.45s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19638.55 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1018.45 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.76 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.60it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.60it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.60it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.60it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.60it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.60it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.60it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.60it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.60it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:33<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.60it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14c52acf1a80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c115d0ee-d0db-4ce7-a6fb-a70b95970777)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14c52a639690>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 82acb670-09e5-4334-92d8-ec64e0a2661d)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5041, 'train_samples_per_second': 6.336, 'train_steps_per_second': 1.584, 'train_loss': 1.7052494049072267, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_58
Fine-tuning completed successfully!
2025-05-04 20:34:56.350095: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:34:56.363629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746405296.378619 2517671 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746405296.383219 2517671 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746405296.396002 2517671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405296.396021 2517671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405296.396023 2517671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405296.396024 2517671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:34:56.399937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20633.01 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1018.49 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.78 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.59it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.60it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.60it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.59it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.60it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.60it/s] 70%|███████   | 56/80 [00:35<00:15,  1.60it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150f70531a20>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0cc66cdb-851b-45fe-abee-7cde4acbfa08)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150f7056d630>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 536f8f4e-a226-4e7b-846f-efccb4e47ac9)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5229, 'train_samples_per_second': 6.334, 'train_steps_per_second': 1.583, 'train_loss': 1.7046165466308594, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_58
Fine-tuning completed successfully!
end finetuning 58
start evaling: seed  58
2025-05-04 20:36:05.032537: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:36:05.047796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746405365.063521 2517899 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746405365.068375 2517899 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746405365.081732 2517899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405365.081755 2517899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405365.081757 2517899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405365.081759 2517899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:36:05.085944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:07<05:40,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:16<06:05,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:25<06:06,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:34<06:01,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:38<04:36,  6.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:46<04:46,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:55<05:00,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:01<04:26,  7.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:10<04:39,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:17<04:29,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:26<04:35,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:35<04:35,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:44<04:34,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:53<04:29,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [02:02<04:22,  8.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:05<03:25,  7.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:14<03:34,  7.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:23<03:37,  8.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:32<03:36,  8.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:39<03:17,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:45<02:56,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:51<02:38,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [03:00<02:45,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:06<02:30,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:15<02:34,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:24<02:33,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:27<01:58,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:36<02:04,  7.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:45<02:04,  7.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:54<02:02,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:03<01:57,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:12<01:51,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:16<01:25,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:25<01:22,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:34<01:19,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:43<01:14,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:52<01:08,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:01<01:00,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:10<00:52,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:19<00:44,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:28<00:35,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:37<00:26,  8.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:46<00:17,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:55<00:08,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:03<00:00,  8.93s/it]Sample 1: 100%|██████████| 45/45 [06:03<00:00,  8.09s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_58.csv
2025-05-04 20:42:29.957733: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:42:30.105893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746405750.169770 2518663 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746405750.185266 2518663 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746405750.302323 2518663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405750.302354 2518663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405750.302358 2518663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405750.302359 2518663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:42:30.308997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:06<04:44,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:15<05:42,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:21<04:55,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:25<03:57,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:33<04:22,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:42<04:48,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:51<05:01,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [00:54<03:56,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:03<04:19,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:12<04:31,  7.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:21<04:36,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:30<04:36,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:35<04:00,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:44<04:06,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:49<03:26,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [01:58<03:37,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:04<03:16,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:07<02:38,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:16<02:56,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:25<03:06,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:33<03:05,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:42<03:05,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:51<03:03,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:00<02:58,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:06<02:37,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:15<02:35,  8.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:24<02:31,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:28<01:59,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:37<02:01,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:46<01:59,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [03:54<01:53,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:01<01:38,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:08<01:29,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:17<01:27,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:26<01:22,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:35<01:16,  8.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:43<01:06,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:46<00:47,  6.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [04:55<00:44,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:02<00:36,  7.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:11<00:31,  7.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:20<00:24,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:29<00:16,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:38<00:08,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:47<00:00,  8.66s/it]Sample 1: 100%|██████████| 45/45 [05:47<00:00,  7.71s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_58.csv
end eval 58
start finetuning: seed  60
2025-05-04 20:48:35.915826: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:48:35.929236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406115.943859 2519395 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406115.948376 2519395 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406115.961099 2519395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406115.961119 2519395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406115.961121 2519395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406115.961122 2519395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:48:35.965078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 21407.36 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1014.27 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1003.55 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.43it/s]  4%|▍         | 3/80 [00:02<00:51,  1.50it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.60it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.60it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.59it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.60it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.60it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.60it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.60it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.60it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.60it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.60it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.60it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.60it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.60it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.60it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:33<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.60it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b4653cda80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: ab7602b5-a2e3-4df4-9866-3625d19c9ba7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.60it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b4653d5690>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9401bc17-90ad-4187-a0b7-3c1081cd9f0d)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5327, 'train_samples_per_second': 6.333, 'train_steps_per_second': 1.583, 'train_loss': 1.7100345611572265, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_60
Fine-tuning completed successfully!
2025-05-04 20:49:43.695341: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:49:43.709033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406183.724212 2519593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406183.728916 2519593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406183.741793 2519593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406183.741813 2519593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406183.741815 2519593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406183.741816 2519593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:49:43.745756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19069.62 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1018.77 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.14 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.27it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.60it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.60it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.59it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.59it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.59it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:30,  1.59it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.59it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.59it/s] 46%|████▋     | 37/80 [00:23<00:27,  1.59it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:22,  1.59it/s] 57%|█████▊    | 46/80 [00:29<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.60it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.60it/s] 80%|████████  | 64/80 [00:40<00:10,  1.60it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.60it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.60it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f9944e1a50>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5af95589-f125-4c63-ac1f-f72c22a0e9a3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.60it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f994309660>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 94c3c13e-c013-47aa-b995-86b308e70134)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5463, 'train_samples_per_second': 6.331, 'train_steps_per_second': 1.583, 'train_loss': 1.7099559783935547, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_60
Fine-tuning completed successfully!
end finetuning 60
start evaling: seed  60
2025-05-04 20:50:59.618025: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:50:59.633592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406259.649838 2519858 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406259.654826 2519858 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406259.668245 2519858 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406259.668267 2519858 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406259.668269 2519858 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406259.668271 2519858 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:50:59.672737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:55,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:17<06:21,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:26<06:15,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:35<06:06,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:45<06:00,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:53<05:42,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [01:02<05:37,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:09<05:12,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:18<05:10,  8.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:26<04:48,  8.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:35<04:47,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:44<04:44,  8.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:52<04:34,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [02:01<04:30,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [02:07<03:54,  7.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:16<03:56,  8.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:19<03:05,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:23<02:35,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:32<02:54,  6.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:36<02:31,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:45<02:46,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:54<02:53,  7.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:59<02:28,  6.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:08<02:35,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:17<02:37,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:23<02:20,  7.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:32<02:21,  7.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:40<02:11,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:49<02:09,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:55<01:55,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:01<01:39,  7.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:09<01:34,  7.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:18<01:33,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:27<01:29,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:33<01:15,  7.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:36<00:56,  6.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:43<00:52,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:51<00:48,  6.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [04:59<00:43,  7.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:02<00:30,  6.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:10<00:25,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:17<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:26<00:14,  7.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:35<00:07,  7.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:44<00:00,  8.15s/it]Sample 1: 100%|██████████| 45/45 [05:44<00:00,  7.65s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_60.csv
2025-05-04 20:57:07.619859: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:57:07.634440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406627.649984 2520605 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406627.654760 2520605 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406627.667958 2520605 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406627.667980 2520605 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406627.667986 2520605 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406627.667988 2520605 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:57:07.672172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:06<04:49,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:15<05:45,  8.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:24<05:56,  8.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:30<05:12,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:39<05:25,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:47<05:06,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:56<05:13,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:05<05:13,  8.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:08<04:05,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:17<04:23,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:23<04:00,  7.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:30<03:54,  7.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:39<04:06,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:48<04:10,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:56<03:59,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:05<04:00,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:14<03:58,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:23<03:53,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:32<03:46,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:37<03:10,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:46<03:12,  8.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:54<03:03,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:58<02:30,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:07<02:36,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:15<02:33,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:24<02:31,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:33<02:28,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:38<02:04,  7.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:41<01:38,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:50<01:44,  6.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [03:59<01:46,  7.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:04<01:28,  6.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:12<01:25,  7.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:18<01:15,  6.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:28<01:15,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:34<01:04,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:43<01:01,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:49<00:51,  7.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [04:58<00:47,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:07<00:41,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:16<00:33,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:25<00:25,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:34<00:17,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:40<00:07,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:49<00:00,  8.12s/it]Sample 1: 100%|██████████| 45/45 [05:49<00:00,  7.76s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_60.csv
end eval 60
start finetuning: seed  65
2025-05-04 21:03:14.590559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:03:14.603645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406994.618760 2521371 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406994.623383 2521371 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406994.636306 2521371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406994.636324 2521371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406994.636327 2521371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406994.636328 2521371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:03:14.640286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=65, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_65', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19223.12 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1015.04 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1004.61 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 65
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_65
===========================
SEED CHECK:, should be: 65, seed is: 65
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.27it/s]  2%|▎         | 2/80 [00:01<00:53,  1.45it/s]  4%|▍         | 3/80 [00:02<00:50,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.55it/s]  6%|▋         | 5/80 [00:03<00:47,  1.57it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.59it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.60it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.60it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.60it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.60it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.60it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.60it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.60it/s] 60%|██████    | 48/80 [00:30<00:20,  1.60it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.60it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.60it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.60it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.60it/s] 68%|██████▊   | 54/80 [00:33<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.60it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.60it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.60it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.60it/s] 78%|███████▊  | 62/80 [00:38<00:11,  1.60it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:43<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.60it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.60it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1515d9cc1a80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a527b2fd-568a-4988-a2b8-4d11d60358c0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.60it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1515d9d9fd90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a36ab2e5-dce4-4ab1-bdf6-54592da20649)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.4708, 'train_samples_per_second': 6.34, 'train_steps_per_second': 1.585, 'train_loss': 1.7071722030639649, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_65
Fine-tuning completed successfully!
2025-05-04 21:04:25.768724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:04:25.782244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407065.797120 2521539 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407065.801768 2521539 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407065.814603 2521539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407065.814624 2521539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407065.814626 2521539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407065.814627 2521539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:04:25.818601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=65, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_65', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19340.01 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.23 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.32 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 65
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_65
===========================
SEED CHECK:, should be: 65, seed is: 65
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:48,  1.56it/s]  8%|▊         | 6/80 [00:03<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:41,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.60it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.60it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.59it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.59it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.59it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.59it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.59it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.60it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.60it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.59it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.60it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.60it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.60it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.60it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.60it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.60it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.60it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.60it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:33<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d80dcdda80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 92b62885-381e-49b9-8a5e-334ff429adc7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d80dd19690>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: b972b688-998f-40a2-be3c-f6785e51b176)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5185, 'train_samples_per_second': 6.334, 'train_steps_per_second': 1.584, 'train_loss': 1.7066390991210938, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_65
Fine-tuning completed successfully!
end finetuning 65
start evaling: seed  65
2025-05-04 21:05:37.701727: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:05:37.717295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407137.733754 2521755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407137.738764 2521755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407137.752187 2521755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407137.752207 2521755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407137.752209 2521755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407137.752211 2521755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:05:37.756289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:53,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:18<06:34,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:27<06:22,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:36<06:10,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:41<05:04,  7.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:50<05:15,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:59<05:19,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:06<04:54,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:13<04:33,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:22<04:41,  8.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:27<04:00,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:36<04:11,  7.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:43<04:05,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:52<04:10,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [02:01<04:09,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:10<04:06,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:19<04:02,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:24<03:25,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:28<02:42,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:37<02:57,  7.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:41<02:31,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:50<02:43,  7.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:56<02:26,  6.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:05<02:34,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:12<02:29,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:21<02:30,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:30<02:27,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:39<02:23,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:48<02:17,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:57<02:10,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:00<01:38,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:09<01:39,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:18<01:36,  8.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:27<01:31,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:36<01:25,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:41<01:06,  7.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:50<01:02,  7.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:59<00:57,  8.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:04<00:43,  7.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:13<00:39,  7.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:22<00:32,  8.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:31<00:25,  8.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:40<00:17,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:49<00:08,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:58<00:00,  8.76s/it]Sample 1: 100%|██████████| 45/45 [05:58<00:00,  7.97s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_65.csv
2025-05-04 21:11:53.274650: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:11:53.289908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407513.305930 2522643 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407513.310830 2522643 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407513.323995 2522643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407513.324020 2522643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407513.324023 2522643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407513.324024 2522643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:11:53.328095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:06<04:25,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:15<05:35,  7.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:24<05:50,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:27<04:21,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:36<04:54,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:45<05:08,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:53<04:58,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:00<04:46,  7.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:09<04:52,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:17<04:44,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:26<04:45,  8.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:34<04:29,  8.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:41<04:08,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:50<04:12,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:57<03:52,  7.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:00<03:04,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:05<02:50,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:14<03:07,  6.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:23<03:15,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:31<03:12,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:40<03:13,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:47<02:56,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:56<02:57,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:05<02:55,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:14<02:50,  8.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:21<02:33,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:30<02:29,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:39<02:24,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:48<02:18,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:54<01:57,  7.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [03:59<01:38,  7.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:08<01:39,  7.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:17<01:36,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:26<01:31,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:35<01:25,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:44<01:18,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:53<01:10,  8.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:02<01:01,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:11<00:53,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:20<00:44,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:29<00:35,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:38<00:26,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:47<00:17,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:56<00:08,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:02<00:00,  8.21s/it]Sample 1: 100%|██████████| 45/45 [06:02<00:00,  8.06s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_65.csv
end eval 65
start finetuning: seed  83
2025-05-04 21:18:14.017002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:18:14.030955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407894.046027 2523400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407894.050675 2523400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407894.063487 2523400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407894.063507 2523400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407894.063508 2523400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407894.063510 2523400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:18:14.067408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=83, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_83', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20419.86 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1020.42 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1009.76 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 83
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_83
===========================
SEED CHECK:, should be: 83, seed is: 83
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.27it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:47,  1.56it/s]  8%|▊         | 6/80 [00:03<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.58it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:42,  1.59it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.60it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.59it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.59it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.59it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.59it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.59it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.59it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.59it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.59it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.59it/s] 46%|████▋     | 37/80 [00:23<00:27,  1.59it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.59it/s] 57%|█████▊    | 46/80 [00:29<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.60it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145f9e70da80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6db2ecdb-97e9-48dc-8ecb-fa0f954b1140)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145f9e749690>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 28c1e2de-fc08-4bb8-ad0e-bd31b7ca4ab3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5975, 'train_samples_per_second': 6.324, 'train_steps_per_second': 1.581, 'train_loss': 1.7079151153564454, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_83
Fine-tuning completed successfully!
2025-05-04 21:19:24.928678: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:19:24.942249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407964.957300 2523579 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407964.961900 2523579 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407964.974712 2523579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407964.974732 2523579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407964.974734 2523579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407964.974735 2523579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:19:24.978623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=83, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_83', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19936.98 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1016.45 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1005.90 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 83
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_83
===========================
SEED CHECK:, should be: 83, seed is: 83
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:03,  1.24it/s]  2%|▎         | 2/80 [00:01<00:54,  1.42it/s]  4%|▍         | 3/80 [00:02<00:51,  1.50it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:48,  1.56it/s]  8%|▊         | 6/80 [00:03<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.60it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.60it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.60it/s] 16%|█▋        | 13/80 [00:08<00:42,  1.60it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.59it/s] 20%|██        | 16/80 [00:10<00:40,  1.59it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.59it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.59it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.59it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.59it/s] 26%|██▋       | 21/80 [00:13<00:37,  1.59it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.59it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.59it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.59it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.59it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.59it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.59it/s] 36%|███▋      | 29/80 [00:18<00:32,  1.59it/s] 38%|███▊      | 30/80 [00:19<00:31,  1.59it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.59it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.59it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.59it/s] 46%|████▋     | 37/80 [00:23<00:27,  1.59it/s] 48%|████▊     | 38/80 [00:24<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:22,  1.59it/s] 57%|█████▊    | 46/80 [00:29<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:17,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:36<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.58it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:41<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.58it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.58it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.58it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.58it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.58it/s] 91%|█████████▏| 73/80 [00:46<00:04,  1.58it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.58it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.58it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1454762d5a80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: ea3903b4-cc0e-4fae-b369-f0b20afe7aaa)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145451db3d90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 337876c3-9050-47b1-a46f-ef50d1bc2872)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.7127, 'train_samples_per_second': 6.31, 'train_steps_per_second': 1.578, 'train_loss': 1.7073070526123046, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_83
Fine-tuning completed successfully!
end finetuning 83
start evaling: seed  83
2025-05-04 21:20:37.493269: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:20:37.507832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408037.523521 2523789 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408037.528357 2523789 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408037.541453 2523789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408037.541475 2523789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408037.541477 2523789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408037.541478 2523789 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:20:37.545532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:53,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:18<06:26,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:25<05:40,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:29<04:38,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:33<03:39,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:38<03:37,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:47<04:15,  6.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [00:56<04:35,  7.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:05<04:45,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:14<04:49,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:23<04:48,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:32<04:44,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:41<04:40,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:50<04:33,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:58<04:10,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:07<04:07,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:14<03:51,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:23<03:48,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:32<03:43,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:41<03:38,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:50<03:31,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:59<03:23,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [03:06<03:01,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:13<02:43,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:22<02:42,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:27<02:18,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:33<02:05,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:41<02:01,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:50<02:03,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:59<02:00,  8.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:04<01:40,  7.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:13<01:41,  7.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:22<01:37,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:31<01:32,  8.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:38<01:21,  8.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:47<01:13,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:55<01:05,  8.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:04<00:58,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:13<00:51,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:22<00:43,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:28<00:32,  8.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:37<00:24,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:46<00:17,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:55<00:08,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:01<00:00,  7.93s/it]Sample 1: 100%|██████████| 45/45 [06:01<00:00,  8.04s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_83.csv
2025-05-04 21:26:56.504767: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:26:56.524045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408416.540044 2524622 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408416.545002 2524622 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408416.558307 2524622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408416.558340 2524622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408416.558342 2524622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408416.558344 2524622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:26:56.562548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:05<03:56,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:14<05:23,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:23<05:44,  8.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:32<05:47,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:41<05:47,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:50<05:42,  8.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:59<05:36,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:07<05:20,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:16<05:14,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:25<05:08,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:29<04:09,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:35<03:49,  6.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:44<04:00,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:49<03:30,  6.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:55<03:12,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:01<03:07,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:09<03:10,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:16<03:05,  6.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:23<02:59,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:32<03:07,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:41<03:10,  7.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:50<03:09,  8.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:58<03:05,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:07<03:00,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:16<02:54,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:25<02:46,  8.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:34<02:38,  8.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:41<02:17,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:50<02:13,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:57<02:00,  8.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:03<01:43,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:12<01:42,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:19<01:31,  7.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:23<01:13,  6.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:32<01:13,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:41<01:10,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:50<01:05,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [04:58<00:55,  7.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:07<00:49,  8.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:16<00:42,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:25<00:34,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:34<00:26,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:39<00:15,  7.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:46<00:07,  7.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [05:55<00:00,  7.87s/it]Sample 1: 100%|██████████| 45/45 [05:55<00:00,  7.89s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_83.csv
end eval 83
start finetuning: seed  95
2025-05-04 21:33:10.235912: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:33:10.249567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408790.264279 2525356 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408790.268845 2525356 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408790.281710 2525356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408790.281730 2525356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408790.281732 2525356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408790.281733 2525356 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:33:10.285769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=95, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_95', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19773.38 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1016.65 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.23 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 95
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_95
===========================
SEED CHECK:, should be: 95, seed is: 95
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.51it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:48,  1.56it/s]  8%|▊         | 6/80 [00:03<00:46,  1.58it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.59it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.59it/s] 16%|█▋        | 13/80 [00:08<00:42,  1.59it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.60it/s] 20%|██        | 16/80 [00:10<00:40,  1.60it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.59it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.60it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.60it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.60it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.60it/s] 36%|███▋      | 29/80 [00:18<00:31,  1.60it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.60it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.60it/s] 40%|████      | 32/80 [00:20<00:30,  1.60it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.60it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.59it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.59it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.59it/s] 57%|█████▊    | 46/80 [00:28<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.60it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.60it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.60it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.60it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.60it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150dccbf9c60>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f3b12245-18e1-4078-b5cc-875dda05fec8)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150dada296c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6793c47e-bfa1-47da-ae4b-7a5a8d7a9a81)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5229, 'train_samples_per_second': 6.334, 'train_steps_per_second': 1.583, 'train_loss': 1.7048370361328125, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.2-3B-Instruct_95
Fine-tuning completed successfully!
2025-05-04 21:34:22.012617: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:34:22.026389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408862.041443 2525535 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408862.046064 2525535 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408862.059026 2525535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408862.059046 2525535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408862.059048 2525535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408862.059050 2525535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:34:22.063019: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=95, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_95', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19258.42 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1018.48 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.98 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.2-3B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 95
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_95
===========================
SEED CHECK:, should be: 95, seed is: 95
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<01:02,  1.26it/s]  2%|▎         | 2/80 [00:01<00:54,  1.44it/s]  4%|▍         | 3/80 [00:02<00:51,  1.50it/s]  5%|▌         | 4/80 [00:02<00:49,  1.54it/s]  6%|▋         | 5/80 [00:03<00:48,  1.56it/s]  8%|▊         | 6/80 [00:03<00:47,  1.57it/s]  9%|▉         | 7/80 [00:04<00:46,  1.58it/s] 10%|█         | 8/80 [00:05<00:45,  1.58it/s] 11%|█▏        | 9/80 [00:05<00:44,  1.59it/s] 12%|█▎        | 10/80 [00:06<00:43,  1.59it/s] 14%|█▍        | 11/80 [00:07<00:43,  1.59it/s] 15%|█▌        | 12/80 [00:07<00:42,  1.59it/s] 16%|█▋        | 13/80 [00:08<00:42,  1.59it/s] 18%|█▊        | 14/80 [00:08<00:41,  1.59it/s] 19%|█▉        | 15/80 [00:09<00:40,  1.59it/s] 20%|██        | 16/80 [00:10<00:40,  1.59it/s] 21%|██▏       | 17/80 [00:10<00:39,  1.59it/s] 22%|██▎       | 18/80 [00:11<00:38,  1.60it/s] 24%|██▍       | 19/80 [00:12<00:38,  1.60it/s] 25%|██▌       | 20/80 [00:12<00:37,  1.60it/s] 26%|██▋       | 21/80 [00:13<00:36,  1.60it/s] 28%|██▊       | 22/80 [00:13<00:36,  1.60it/s] 29%|██▉       | 23/80 [00:14<00:35,  1.60it/s] 30%|███       | 24/80 [00:15<00:35,  1.59it/s] 31%|███▏      | 25/80 [00:15<00:34,  1.59it/s] 32%|███▎      | 26/80 [00:16<00:33,  1.59it/s] 34%|███▍      | 27/80 [00:17<00:33,  1.59it/s] 35%|███▌      | 28/80 [00:17<00:32,  1.59it/s] 36%|███▋      | 29/80 [00:18<00:32,  1.59it/s] 38%|███▊      | 30/80 [00:18<00:31,  1.59it/s] 39%|███▉      | 31/80 [00:19<00:30,  1.59it/s] 40%|████      | 32/80 [00:20<00:30,  1.59it/s] 41%|████▏     | 33/80 [00:20<00:29,  1.59it/s] 42%|████▎     | 34/80 [00:21<00:28,  1.59it/s] 44%|████▍     | 35/80 [00:22<00:28,  1.59it/s] 45%|████▌     | 36/80 [00:22<00:27,  1.60it/s] 46%|████▋     | 37/80 [00:23<00:26,  1.59it/s] 48%|████▊     | 38/80 [00:23<00:26,  1.59it/s] 49%|████▉     | 39/80 [00:24<00:25,  1.59it/s] 50%|█████     | 40/80 [00:25<00:25,  1.59it/s] 51%|█████▏    | 41/80 [00:25<00:24,  1.59it/s] 52%|█████▎    | 42/80 [00:26<00:23,  1.59it/s] 54%|█████▍    | 43/80 [00:27<00:23,  1.59it/s] 55%|█████▌    | 44/80 [00:27<00:22,  1.59it/s] 56%|█████▋    | 45/80 [00:28<00:21,  1.59it/s] 57%|█████▊    | 46/80 [00:29<00:21,  1.59it/s] 59%|█████▉    | 47/80 [00:29<00:20,  1.59it/s] 60%|██████    | 48/80 [00:30<00:20,  1.59it/s] 61%|██████▏   | 49/80 [00:30<00:19,  1.59it/s] 62%|██████▎   | 50/80 [00:31<00:18,  1.59it/s] 64%|██████▍   | 51/80 [00:32<00:18,  1.59it/s] 65%|██████▌   | 52/80 [00:32<00:17,  1.59it/s] 66%|██████▋   | 53/80 [00:33<00:16,  1.59it/s] 68%|██████▊   | 54/80 [00:34<00:16,  1.59it/s] 69%|██████▉   | 55/80 [00:34<00:15,  1.59it/s] 70%|███████   | 56/80 [00:35<00:15,  1.59it/s] 71%|███████▏  | 57/80 [00:35<00:14,  1.59it/s] 72%|███████▎  | 58/80 [00:36<00:13,  1.59it/s] 74%|███████▍  | 59/80 [00:37<00:13,  1.59it/s] 75%|███████▌  | 60/80 [00:37<00:12,  1.59it/s] 76%|███████▋  | 61/80 [00:38<00:11,  1.59it/s] 78%|███████▊  | 62/80 [00:39<00:11,  1.59it/s] 79%|███████▉  | 63/80 [00:39<00:10,  1.59it/s] 80%|████████  | 64/80 [00:40<00:10,  1.59it/s] 81%|████████▏ | 65/80 [00:40<00:09,  1.59it/s] 82%|████████▎ | 66/80 [00:41<00:08,  1.59it/s] 84%|████████▍ | 67/80 [00:42<00:08,  1.59it/s] 85%|████████▌ | 68/80 [00:42<00:07,  1.59it/s] 86%|████████▋ | 69/80 [00:43<00:06,  1.59it/s] 88%|████████▊ | 70/80 [00:44<00:06,  1.59it/s] 89%|████████▉ | 71/80 [00:44<00:05,  1.59it/s] 90%|█████████ | 72/80 [00:45<00:05,  1.59it/s] 91%|█████████▏| 73/80 [00:45<00:04,  1.59it/s] 92%|█████████▎| 74/80 [00:46<00:03,  1.59it/s] 94%|█████████▍| 75/80 [00:47<00:03,  1.59it/s] 95%|█████████▌| 76/80 [00:47<00:02,  1.59it/s] 96%|█████████▋| 77/80 [00:48<00:01,  1.59it/s] 98%|█████████▊| 78/80 [00:49<00:01,  1.59it/s] 99%|█████████▉| 79/80 [00:49<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.59it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150dc14c99f0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c1e8f45d-78fa-43b5-a9e7-cef5600df4b0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [00:50<00:00,  1.59it/s]100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150dc1505600>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f2d2b522-bb48-491e-8e5f-b27171491e73)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 50.5686, 'train_samples_per_second': 6.328, 'train_steps_per_second': 1.582, 'train_loss': 1.7049354553222655, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.2-3B-Instruct_95
Fine-tuning completed successfully!
end finetuning 95
start evaling: seed  95
2025-05-04 21:35:35.141265: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:35:35.156797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408935.172960 2525680 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408935.177922 2525680 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408935.191335 2525680 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408935.191356 2525680 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408935.191358 2525680 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408935.191360 2525680 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:35:35.195437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:53,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:16<05:58,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:26<06:03,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:34<05:59,  8.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:44<05:54,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:53<05:48,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [01:02<05:41,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:11<05:32,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:20<05:23,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:29<05:15,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:38<05:05,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:47<04:56,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:54<04:33,  8.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [02:03<04:23,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [02:11<04:18,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:20<04:13,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:29<04:06,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:37<03:44,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:45<03:40,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:49<02:52,  6.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:58<03:00,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [03:07<03:03,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [03:16<03:01,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:25<02:57,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:34<02:52,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:37<02:15,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:46<02:17,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:55<02:17,  8.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [04:02<02:03,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [04:11<02:00,  8.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:20<01:56,  8.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:29<01:51,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:38<01:44,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:47<01:36,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:55<01:27,  8.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [05:04<01:19,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [05:13<01:10,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:17<00:50,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:26<00:46,  7.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:35<00:40,  8.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:44<00:33,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:52<00:25,  8.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:59<00:15,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [06:05<00:07,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:14<00:00,  7.81s/it]Sample 1: 100%|██████████| 45/45 [06:14<00:00,  8.32s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.2-3B-Instruct_salinas_expanded_context_95.csv
2025-05-04 21:42:05.707160: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:42:05.729058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746409325.744933 2526584 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746409325.749825 2526584 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746409325.763101 2526584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409325.763127 2526584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409325.763129 2526584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409325.763130 2526584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:42:05.767335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.39s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:09<06:53,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:15<05:15,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:24<05:40,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [00:32<05:29,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [00:41<05:36,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [00:47<05:00,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [00:56<05:09,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [01:05<05:10,  8.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [01:12<04:49,  8.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [01:19<04:26,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [01:26<04:15,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [01:34<04:06,  7.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [01:38<03:26,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [01:47<03:43,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [01:56<03:51,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [02:03<03:42,  7.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [02:12<03:45,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [02:18<03:21,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [02:23<02:56,  6.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [02:28<02:35,  6.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [02:37<02:49,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [02:46<02:55,  7.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [02:55<02:56,  8.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [03:04<02:54,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [03:13<02:50,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [03:22<02:44,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [03:31<02:36,  8.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [03:40<02:29,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [03:49<02:21,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [03:58<02:12,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [04:03<01:50,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [04:07<01:24,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [04:15<01:23,  6.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [04:24<01:22,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [04:33<01:19,  8.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [04:42<01:14,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [04:51<01:08,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [05:00<01:00,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [05:09<00:52,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [05:18<00:44,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [05:26<00:34,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [05:35<00:26,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [05:44<00:17,  8.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [05:53<00:08,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [06:02<00:00,  8.87s/it]Sample 1: 100%|██████████| 45/45 [06:02<00:00,  8.05s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.2-3B-Instruct_salinas_expanded_context_95.csv
end eval 95
