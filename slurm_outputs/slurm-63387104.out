alpaca_data_1000
start finetuning
2025-04-04 18:11:26.466957: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:11:26.610783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743804686.663771 2275514 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743804686.687643 2275514 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743804686.850435 2275514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804686.850467 2275514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804686.850470 2275514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804686.850471 2275514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:11:26.858538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 16.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.51s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 22367.47 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2752.69 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2682.84 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<04:27,  1.07s/it]  1%|          | 2/250 [00:01<03:18,  1.25it/s]  1%|          | 3/250 [00:02<02:56,  1.40it/s]  2%|▏         | 4/250 [00:02<02:45,  1.48it/s]  2%|▏         | 5/250 [00:03<02:39,  1.54it/s]  2%|▏         | 6/250 [00:04<02:35,  1.57it/s]  3%|▎         | 7/250 [00:04<02:32,  1.59it/s]  3%|▎         | 8/250 [00:05<02:30,  1.61it/s]  4%|▎         | 9/250 [00:05<02:28,  1.62it/s]  4%|▍         | 10/250 [00:06<02:27,  1.62it/s]  4%|▍         | 11/250 [00:07<02:26,  1.63it/s]  5%|▍         | 12/250 [00:07<02:25,  1.63it/s]  5%|▌         | 13/250 [00:08<02:25,  1.63it/s]  6%|▌         | 14/250 [00:09<02:24,  1.64it/s]  6%|▌         | 15/250 [00:09<02:23,  1.64it/s]  6%|▋         | 16/250 [00:10<02:22,  1.64it/s]  7%|▋         | 17/250 [00:10<02:22,  1.64it/s]  7%|▋         | 18/250 [00:11<02:21,  1.64it/s]  8%|▊         | 19/250 [00:12<02:20,  1.64it/s]  8%|▊         | 20/250 [00:12<02:20,  1.64it/s]  8%|▊         | 21/250 [00:13<02:19,  1.64it/s]  9%|▉         | 22/250 [00:13<02:19,  1.64it/s]  9%|▉         | 23/250 [00:14<02:18,  1.64it/s] 10%|▉         | 24/250 [00:15<02:17,  1.64it/s] 10%|█         | 25/250 [00:15<02:17,  1.64it/s] 10%|█         | 26/250 [00:16<02:16,  1.64it/s] 11%|█         | 27/250 [00:16<02:16,  1.64it/s] 11%|█         | 28/250 [00:17<02:15,  1.64it/s] 12%|█▏        | 29/250 [00:18<02:14,  1.64it/s] 12%|█▏        | 30/250 [00:18<02:14,  1.64it/s] 12%|█▏        | 31/250 [00:19<02:13,  1.64it/s] 13%|█▎        | 32/250 [00:19<02:13,  1.64it/s] 13%|█▎        | 33/250 [00:20<02:12,  1.64it/s] 14%|█▎        | 34/250 [00:21<02:12,  1.64it/s] 14%|█▍        | 35/250 [00:21<02:11,  1.64it/s] 14%|█▍        | 36/250 [00:22<02:10,  1.64it/s] 15%|█▍        | 37/250 [00:23<02:10,  1.64it/s] 15%|█▌        | 38/250 [00:23<02:09,  1.64it/s] 16%|█▌        | 39/250 [00:24<02:08,  1.64it/s] 16%|█▌        | 40/250 [00:24<02:08,  1.64it/s] 16%|█▋        | 41/250 [00:25<02:07,  1.63it/s] 17%|█▋        | 42/250 [00:26<02:07,  1.63it/s] 17%|█▋        | 43/250 [00:26<02:06,  1.63it/s] 18%|█▊        | 44/250 [00:27<02:06,  1.63it/s] 18%|█▊        | 45/250 [00:27<02:05,  1.63it/s] 18%|█▊        | 46/250 [00:28<02:04,  1.63it/s] 19%|█▉        | 47/250 [00:29<02:04,  1.63it/s] 19%|█▉        | 48/250 [00:29<02:03,  1.63it/s] 20%|█▉        | 49/250 [00:30<02:03,  1.63it/s] 20%|██        | 50/250 [00:31<02:02,  1.63it/s] 20%|██        | 51/250 [00:31<02:01,  1.63it/s] 21%|██        | 52/250 [00:32<02:01,  1.63it/s] 21%|██        | 53/250 [00:32<02:00,  1.63it/s] 22%|██▏       | 54/250 [00:33<02:00,  1.63it/s] 22%|██▏       | 55/250 [00:34<01:59,  1.63it/s] 22%|██▏       | 56/250 [00:34<01:58,  1.63it/s] 23%|██▎       | 57/250 [00:35<01:58,  1.63it/s] 23%|██▎       | 58/250 [00:35<01:57,  1.63it/s] 24%|██▎       | 59/250 [00:36<01:57,  1.63it/s] 24%|██▍       | 60/250 [00:37<01:56,  1.63it/s] 24%|██▍       | 61/250 [00:37<01:55,  1.63it/s] 25%|██▍       | 62/250 [00:38<01:55,  1.63it/s] 25%|██▌       | 63/250 [00:38<01:54,  1.63it/s] 26%|██▌       | 64/250 [00:39<01:53,  1.63it/s] 26%|██▌       | 65/250 [00:40<01:53,  1.63it/s] 26%|██▋       | 66/250 [00:40<01:52,  1.63it/s] 27%|██▋       | 67/250 [00:41<01:51,  1.64it/s] 27%|██▋       | 68/250 [00:42<01:51,  1.63it/s] 28%|██▊       | 69/250 [00:42<01:50,  1.63it/s] 28%|██▊       | 70/250 [00:43<01:50,  1.63it/s] 28%|██▊       | 71/250 [00:43<01:49,  1.63it/s] 29%|██▉       | 72/250 [00:44<01:49,  1.63it/s] 29%|██▉       | 73/250 [00:45<01:48,  1.63it/s] 30%|██▉       | 74/250 [00:45<01:48,  1.63it/s] 30%|███       | 75/250 [00:46<01:47,  1.63it/s] 30%|███       | 76/250 [00:46<01:46,  1.63it/s] 31%|███       | 77/250 [00:47<01:46,  1.63it/s] 31%|███       | 78/250 [00:48<01:45,  1.63it/s] 32%|███▏      | 79/250 [00:48<01:44,  1.63it/s] 32%|███▏      | 80/250 [00:49<01:44,  1.63it/s] 32%|███▏      | 81/250 [00:50<01:43,  1.63it/s] 33%|███▎      | 82/250 [00:50<01:43,  1.63it/s] 33%|███▎      | 83/250 [00:51<01:42,  1.63it/s] 34%|███▎      | 84/250 [00:51<01:41,  1.63it/s] 34%|███▍      | 85/250 [00:52<01:41,  1.63it/s] 34%|███▍      | 86/250 [00:53<01:40,  1.63it/s] 35%|███▍      | 87/250 [00:53<01:40,  1.63it/s] 35%|███▌      | 88/250 [00:54<01:39,  1.63it/s] 36%|███▌      | 89/250 [00:54<01:38,  1.63it/s] 36%|███▌      | 90/250 [00:55<01:38,  1.63it/s] 36%|███▋      | 91/250 [00:56<01:37,  1.63it/s] 37%|███▋      | 92/250 [00:56<01:36,  1.63it/s] 37%|███▋      | 93/250 [00:57<01:36,  1.63it/s] 38%|███▊      | 94/250 [00:57<01:35,  1.63it/s] 38%|███▊      | 95/250 [00:58<01:35,  1.63it/s] 38%|███▊      | 96/250 [00:59<01:34,  1.63it/s] 39%|███▉      | 97/250 [00:59<01:34,  1.63it/s] 39%|███▉      | 98/250 [01:00<01:33,  1.63it/s] 40%|███▉      | 99/250 [01:01<01:32,  1.63it/s] 40%|████      | 100/250 [01:01<01:32,  1.63it/s]                                                  40%|████      | 100/250 [01:01<01:32,  1.63it/s] 40%|████      | 101/250 [01:02<01:31,  1.62it/s] 41%|████      | 102/250 [01:02<01:31,  1.62it/s] 41%|████      | 103/250 [01:03<01:30,  1.63it/s] 42%|████▏     | 104/250 [01:04<01:29,  1.63it/s] 42%|████▏     | 105/250 [01:04<01:29,  1.63it/s] 42%|████▏     | 106/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 107/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 108/250 [01:06<01:27,  1.63it/s] 44%|████▎     | 109/250 [01:07<01:26,  1.63it/s] 44%|████▍     | 110/250 [01:07<01:26,  1.63it/s] 44%|████▍     | 111/250 [01:08<01:25,  1.63it/s] 45%|████▍     | 112/250 [01:09<01:24,  1.63it/s] 45%|████▌     | 113/250 [01:09<01:24,  1.63it/s] 46%|████▌     | 114/250 [01:10<01:23,  1.63it/s] 46%|████▌     | 115/250 [01:10<01:23,  1.63it/s] 46%|████▋     | 116/250 [01:11<01:22,  1.62it/s] 47%|████▋     | 117/250 [01:12<01:21,  1.63it/s] 47%|████▋     | 118/250 [01:12<01:21,  1.63it/s] 48%|████▊     | 119/250 [01:13<01:20,  1.63it/s] 48%|████▊     | 120/250 [01:13<01:19,  1.63it/s] 48%|████▊     | 121/250 [01:14<01:19,  1.63it/s] 49%|████▉     | 122/250 [01:15<01:18,  1.63it/s] 49%|████▉     | 123/250 [01:15<01:18,  1.63it/s] 50%|████▉     | 124/250 [01:16<01:17,  1.63it/s] 50%|█████     | 125/250 [01:17<01:16,  1.63it/s] 50%|█████     | 126/250 [01:17<01:16,  1.62it/s] 51%|█████     | 127/250 [01:18<01:15,  1.62it/s] 51%|█████     | 128/250 [01:18<01:15,  1.62it/s] 52%|█████▏    | 129/250 [01:19<01:14,  1.63it/s] 52%|█████▏    | 130/250 [01:20<01:13,  1.63it/s] 52%|█████▏    | 131/250 [01:20<01:13,  1.62it/s] 53%|█████▎    | 132/250 [01:21<01:12,  1.62it/s] 53%|█████▎    | 133/250 [01:21<01:12,  1.62it/s] 54%|█████▎    | 134/250 [01:22<01:11,  1.62it/s] 54%|█████▍    | 135/250 [01:23<01:10,  1.62it/s] 54%|█████▍    | 136/250 [01:23<01:10,  1.62it/s] 55%|█████▍    | 137/250 [01:24<01:09,  1.62it/s] 55%|█████▌    | 138/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 139/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 140/250 [01:26<01:07,  1.62it/s] 56%|█████▋    | 141/250 [01:26<01:07,  1.62it/s] 57%|█████▋    | 142/250 [01:27<01:06,  1.62it/s] 57%|█████▋    | 143/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 144/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 145/250 [01:29<01:04,  1.62it/s] 58%|█████▊    | 146/250 [01:29<01:03,  1.63it/s] 59%|█████▉    | 147/250 [01:30<01:03,  1.63it/s] 59%|█████▉    | 148/250 [01:31<01:02,  1.63it/s] 60%|█████▉    | 149/250 [01:31<01:02,  1.62it/s] 60%|██████    | 150/250 [01:32<01:01,  1.62it/s] 60%|██████    | 151/250 [01:33<01:00,  1.62it/s] 61%|██████    | 152/250 [01:33<01:00,  1.62it/s] 61%|██████    | 153/250 [01:34<00:59,  1.63it/s] 62%|██████▏   | 154/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 155/250 [01:35<00:58,  1.62it/s] 62%|██████▏   | 156/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 157/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 158/250 [01:37<00:56,  1.62it/s] 64%|██████▎   | 159/250 [01:38<00:56,  1.62it/s] 64%|██████▍   | 160/250 [01:38<00:55,  1.62it/s] 64%|██████▍   | 161/250 [01:39<00:54,  1.62it/s] 65%|██████▍   | 162/250 [01:39<00:54,  1.62it/s] 65%|██████▌   | 163/250 [01:40<00:53,  1.62it/s] 66%|██████▌   | 164/250 [01:41<00:52,  1.62it/s] 66%|██████▌   | 165/250 [01:41<00:52,  1.62it/s] 66%|██████▋   | 166/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 167/250 [01:42<00:51,  1.63it/s] 67%|██████▋   | 168/250 [01:43<00:50,  1.63it/s] 68%|██████▊   | 169/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 170/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 171/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 172/250 [01:46<00:48,  1.62it/s] 69%|██████▉   | 173/250 [01:46<00:47,  1.62it/s] 70%|██████▉   | 174/250 [01:47<00:46,  1.62it/s] 70%|███████   | 175/250 [01:47<00:46,  1.62it/s] 70%|███████   | 176/250 [01:48<00:45,  1.62it/s] 71%|███████   | 177/250 [01:49<00:44,  1.62it/s] 71%|███████   | 178/250 [01:49<00:44,  1.62it/s] 72%|███████▏  | 179/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 180/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 181/250 [01:51<00:42,  1.62it/s] 73%|███████▎  | 182/250 [01:52<00:41,  1.62it/s] 73%|███████▎  | 183/250 [01:52<00:41,  1.62it/s] 74%|███████▎  | 184/250 [01:53<00:40,  1.63it/s] 74%|███████▍  | 185/250 [01:54<00:39,  1.63it/s] 74%|███████▍  | 186/250 [01:54<00:39,  1.63it/s] 75%|███████▍  | 187/250 [01:55<00:38,  1.63it/s] 75%|███████▌  | 188/250 [01:55<00:38,  1.63it/s] 76%|███████▌  | 189/250 [01:56<00:37,  1.63it/s] 76%|███████▌  | 190/250 [01:57<00:36,  1.63it/s] 76%|███████▋  | 191/250 [01:57<00:36,  1.62it/s] 77%|███████▋  | 192/250 [01:58<00:35,  1.63it/s] 77%|███████▋  | 193/250 [01:58<00:35,  1.62it/s] 78%|███████▊  | 194/250 [01:59<00:34,  1.62it/s] 78%|███████▊  | 195/250 [02:00<00:33,  1.62it/s] 78%|███████▊  | 196/250 [02:00<00:33,  1.62it/s] 79%|███████▉  | 197/250 [02:01<00:32,  1.62it/s] 79%|███████▉  | 198/250 [02:02<00:32,  1.62it/s] 80%|███████▉  | 199/250 [02:02<00:31,  1.62it/s] 80%|████████  | 200/250 [02:03<00:30,  1.63it/s]                                                  80%|████████  | 200/250 [02:03<00:30,  1.63it/s] 80%|████████  | 201/250 [02:03<00:30,  1.62it/s] 81%|████████  | 202/250 [02:04<00:29,  1.62it/s] 81%|████████  | 203/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 204/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 205/250 [02:06<00:27,  1.62it/s] 82%|████████▏ | 206/250 [02:06<00:27,  1.62it/s] 83%|████████▎ | 207/250 [02:07<00:26,  1.62it/s] 83%|████████▎ | 208/250 [02:08<00:25,  1.62it/s] 84%|████████▎ | 209/250 [02:08<00:25,  1.62it/s] 84%|████████▍ | 210/250 [02:09<00:24,  1.62it/s] 84%|████████▍ | 211/250 [02:10<00:23,  1.63it/s] 85%|████████▍ | 212/250 [02:10<00:23,  1.63it/s] 85%|████████▌ | 213/250 [02:11<00:22,  1.63it/s] 86%|████████▌ | 214/250 [02:11<00:22,  1.63it/s] 86%|████████▌ | 215/250 [02:12<00:21,  1.63it/s] 86%|████████▋ | 216/250 [02:13<00:20,  1.63it/s] 87%|████████▋ | 217/250 [02:13<00:20,  1.63it/s] 87%|████████▋ | 218/250 [02:14<00:19,  1.63it/s] 88%|████████▊ | 219/250 [02:14<00:19,  1.62it/s] 88%|████████▊ | 220/250 [02:15<00:18,  1.62it/s] 88%|████████▊ | 221/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 222/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 223/250 [02:17<00:16,  1.62it/s] 90%|████████▉ | 224/250 [02:18<00:16,  1.62it/s] 90%|█████████ | 225/250 [02:18<00:15,  1.62it/s] 90%|█████████ | 226/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 227/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 228/250 [02:20<00:13,  1.62it/s] 92%|█████████▏| 229/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 230/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 231/250 [02:22<00:11,  1.63it/s] 93%|█████████▎| 232/250 [02:22<00:11,  1.63it/s] 93%|█████████▎| 233/250 [02:23<00:10,  1.62it/s] 94%|█████████▎| 234/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 235/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 236/250 [02:25<00:08,  1.63it/s] 95%|█████████▍| 237/250 [02:26<00:08,  1.62it/s] 95%|█████████▌| 238/250 [02:26<00:07,  1.62it/s] 96%|█████████▌| 239/250 [02:27<00:06,  1.62it/s] 96%|█████████▌| 240/250 [02:27<00:06,  1.62it/s] 96%|█████████▋| 241/250 [02:28<00:05,  1.62it/s] 97%|█████████▋| 242/250 [02:29<00:04,  1.62it/s] 97%|█████████▋| 243/250 [02:29<00:04,  1.62it/s] 98%|█████████▊| 244/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 245/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 246/250 [02:31<00:02,  1.62it/s] 99%|█████████▉| 247/250 [02:32<00:01,  1.62it/s] 99%|█████████▉| 248/250 [02:32<00:01,  1.62it/s]100%|█████████▉| 249/250 [02:33<00:00,  1.63it/s]100%|██████████| 250/250 [02:34<00:00,  1.63it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1553aeeeb520>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 7619ad7b-898e-425d-a4fa-cf33587a57cf)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [02:34<00:00,  1.63it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1553afb16fe0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 3d3fa674-7e06-4297-9772-3b6f89ea5873)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.9947, 'grad_norm': 1.628486156463623, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 1.6002, 'grad_norm': 3.1173737049102783, 'learning_rate': 4.08e-06, 'epoch': 0.8}
{'train_runtime': 154.2718, 'train_samples_per_second': 6.482, 'train_steps_per_second': 1.621, 'train_loss': 1.7473445434570312, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
educational_1000
start finetuning
2025-04-04 18:15:17.718608: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:15:17.731044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743804917.744821 2276063 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743804917.749079 2276063 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743804917.761148 2276063 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804917.761166 2276063 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804917.761168 2276063 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743804917.761172 2276063 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:15:17.764818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23796.25 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1846.63 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1814.36 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<03:15,  1.27it/s]  1%|          | 2/250 [00:01<02:49,  1.46it/s]  1%|          | 3/250 [00:02<02:41,  1.53it/s]  2%|▏         | 4/250 [00:02<02:36,  1.57it/s]  2%|▏         | 5/250 [00:03<02:33,  1.59it/s]  2%|▏         | 6/250 [00:03<02:31,  1.61it/s]  3%|▎         | 7/250 [00:04<02:30,  1.61it/s]  3%|▎         | 8/250 [00:05<02:29,  1.62it/s]  4%|▎         | 9/250 [00:05<02:28,  1.62it/s]  4%|▍         | 10/250 [00:06<02:27,  1.63it/s]  4%|▍         | 11/250 [00:06<02:26,  1.63it/s]  5%|▍         | 12/250 [00:07<02:26,  1.63it/s]  5%|▌         | 13/250 [00:08<02:25,  1.63it/s]  6%|▌         | 14/250 [00:08<02:24,  1.63it/s]  6%|▌         | 15/250 [00:09<02:24,  1.63it/s]  6%|▋         | 16/250 [00:09<02:23,  1.63it/s]  7%|▋         | 17/250 [00:10<02:22,  1.63it/s]  7%|▋         | 18/250 [00:11<02:22,  1.63it/s]  8%|▊         | 19/250 [00:11<02:21,  1.63it/s]  8%|▊         | 20/250 [00:12<02:20,  1.63it/s]  8%|▊         | 21/250 [00:13<02:20,  1.63it/s]  9%|▉         | 22/250 [00:13<02:19,  1.63it/s]  9%|▉         | 23/250 [00:14<02:19,  1.63it/s] 10%|▉         | 24/250 [00:14<02:18,  1.63it/s] 10%|█         | 25/250 [00:15<02:18,  1.63it/s] 10%|█         | 26/250 [00:16<02:17,  1.63it/s] 11%|█         | 27/250 [00:16<02:17,  1.63it/s] 11%|█         | 28/250 [00:17<02:16,  1.63it/s] 12%|█▏        | 29/250 [00:17<02:15,  1.63it/s] 12%|█▏        | 30/250 [00:18<02:15,  1.63it/s] 12%|█▏        | 31/250 [00:19<02:14,  1.63it/s] 13%|█▎        | 32/250 [00:19<02:14,  1.62it/s] 13%|█▎        | 33/250 [00:20<02:13,  1.62it/s] 14%|█▎        | 34/250 [00:21<02:12,  1.63it/s] 14%|█▍        | 35/250 [00:21<02:12,  1.62it/s] 14%|█▍        | 36/250 [00:22<02:11,  1.63it/s] 15%|█▍        | 37/250 [00:22<02:10,  1.63it/s] 15%|█▌        | 38/250 [00:23<02:10,  1.63it/s] 16%|█▌        | 39/250 [00:24<02:09,  1.62it/s] 16%|█▌        | 40/250 [00:24<02:09,  1.62it/s] 16%|█▋        | 41/250 [00:25<02:08,  1.62it/s] 17%|█▋        | 42/250 [00:25<02:08,  1.62it/s] 17%|█▋        | 43/250 [00:26<02:07,  1.62it/s] 18%|█▊        | 44/250 [00:27<02:06,  1.62it/s] 18%|█▊        | 45/250 [00:27<02:06,  1.62it/s] 18%|█▊        | 46/250 [00:28<02:05,  1.62it/s] 19%|█▉        | 47/250 [00:29<02:04,  1.63it/s] 19%|█▉        | 48/250 [00:29<02:04,  1.63it/s] 20%|█▉        | 49/250 [00:30<02:03,  1.62it/s] 20%|██        | 50/250 [00:30<02:03,  1.62it/s] 20%|██        | 51/250 [00:31<02:02,  1.62it/s] 21%|██        | 52/250 [00:32<02:01,  1.62it/s] 21%|██        | 53/250 [00:32<02:01,  1.63it/s] 22%|██▏       | 54/250 [00:33<02:00,  1.62it/s] 22%|██▏       | 55/250 [00:33<02:00,  1.62it/s] 22%|██▏       | 56/250 [00:34<01:59,  1.62it/s] 23%|██▎       | 57/250 [00:35<01:58,  1.62it/s] 23%|██▎       | 58/250 [00:35<01:58,  1.62it/s] 24%|██▎       | 59/250 [00:36<01:57,  1.62it/s] 24%|██▍       | 60/250 [00:37<01:57,  1.62it/s] 24%|██▍       | 61/250 [00:37<01:56,  1.62it/s] 25%|██▍       | 62/250 [00:38<01:55,  1.62it/s] 25%|██▌       | 63/250 [00:38<01:55,  1.62it/s] 26%|██▌       | 64/250 [00:39<01:54,  1.63it/s] 26%|██▌       | 65/250 [00:40<01:53,  1.62it/s] 26%|██▋       | 66/250 [00:40<01:53,  1.62it/s] 27%|██▋       | 67/250 [00:41<01:52,  1.62it/s] 27%|██▋       | 68/250 [00:41<01:52,  1.62it/s] 28%|██▊       | 69/250 [00:42<01:51,  1.62it/s] 28%|██▊       | 70/250 [00:43<01:50,  1.62it/s] 28%|██▊       | 71/250 [00:43<01:50,  1.62it/s] 29%|██▉       | 72/250 [00:44<01:49,  1.62it/s] 29%|██▉       | 73/250 [00:45<01:49,  1.62it/s] 30%|██▉       | 74/250 [00:45<01:48,  1.62it/s] 30%|███       | 75/250 [00:46<01:47,  1.62it/s] 30%|███       | 76/250 [00:46<01:47,  1.62it/s] 31%|███       | 77/250 [00:47<01:46,  1.62it/s] 31%|███       | 78/250 [00:48<01:46,  1.62it/s] 32%|███▏      | 79/250 [00:48<01:45,  1.62it/s] 32%|███▏      | 80/250 [00:49<01:44,  1.62it/s] 32%|███▏      | 81/250 [00:49<01:44,  1.62it/s] 33%|███▎      | 82/250 [00:50<01:43,  1.62it/s] 33%|███▎      | 83/250 [00:51<01:42,  1.62it/s] 34%|███▎      | 84/250 [00:51<01:42,  1.62it/s] 34%|███▍      | 85/250 [00:52<01:41,  1.62it/s] 34%|███▍      | 86/250 [00:53<01:41,  1.62it/s] 35%|███▍      | 87/250 [00:53<01:40,  1.62it/s] 35%|███▌      | 88/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 89/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 90/250 [00:55<01:38,  1.62it/s] 36%|███▋      | 91/250 [00:56<01:38,  1.62it/s] 37%|███▋      | 92/250 [00:56<01:37,  1.62it/s] 37%|███▋      | 93/250 [00:57<01:36,  1.62it/s] 38%|███▊      | 94/250 [00:58<01:36,  1.62it/s] 38%|███▊      | 95/250 [00:58<01:35,  1.62it/s] 38%|███▊      | 96/250 [00:59<01:35,  1.62it/s] 39%|███▉      | 97/250 [00:59<01:34,  1.62it/s] 39%|███▉      | 98/250 [01:00<01:33,  1.62it/s] 40%|███▉      | 99/250 [01:01<01:33,  1.62it/s] 40%|████      | 100/250 [01:01<01:32,  1.62it/s]                                                  40%|████      | 100/250 [01:01<01:32,  1.62it/s] 40%|████      | 101/250 [01:02<01:32,  1.62it/s] 41%|████      | 102/250 [01:02<01:31,  1.62it/s] 41%|████      | 103/250 [01:03<01:30,  1.62it/s] 42%|████▏     | 104/250 [01:04<01:30,  1.62it/s] 42%|████▏     | 105/250 [01:04<01:29,  1.62it/s] 42%|████▏     | 106/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 107/250 [01:06<01:28,  1.62it/s] 43%|████▎     | 108/250 [01:06<01:27,  1.62it/s] 44%|████▎     | 109/250 [01:07<01:27,  1.62it/s] 44%|████▍     | 110/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 111/250 [01:08<01:25,  1.62it/s] 45%|████▍     | 112/250 [01:09<01:25,  1.62it/s] 45%|████▌     | 113/250 [01:09<01:24,  1.62it/s] 46%|████▌     | 114/250 [01:10<01:23,  1.62it/s] 46%|████▌     | 115/250 [01:10<01:23,  1.62it/s] 46%|████▋     | 116/250 [01:11<01:22,  1.62it/s] 47%|████▋     | 117/250 [01:12<01:21,  1.62it/s] 47%|████▋     | 118/250 [01:12<01:21,  1.62it/s] 48%|████▊     | 119/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 120/250 [01:14<01:20,  1.62it/s] 48%|████▊     | 121/250 [01:14<01:19,  1.62it/s] 49%|████▉     | 122/250 [01:15<01:18,  1.62it/s] 49%|████▉     | 123/250 [01:15<01:18,  1.62it/s] 50%|████▉     | 124/250 [01:16<01:17,  1.62it/s] 50%|█████     | 125/250 [01:17<01:17,  1.62it/s] 50%|█████     | 126/250 [01:17<01:16,  1.62it/s] 51%|█████     | 127/250 [01:18<01:15,  1.62it/s] 51%|█████     | 128/250 [01:19<01:15,  1.62it/s] 52%|█████▏    | 129/250 [01:19<01:14,  1.62it/s] 52%|█████▏    | 130/250 [01:20<01:14,  1.62it/s] 52%|█████▏    | 131/250 [01:20<01:13,  1.61it/s] 53%|█████▎    | 132/250 [01:21<01:13,  1.61it/s] 53%|█████▎    | 133/250 [01:22<01:12,  1.61it/s] 54%|█████▎    | 134/250 [01:22<01:11,  1.61it/s] 54%|█████▍    | 135/250 [01:23<01:11,  1.62it/s] 54%|█████▍    | 136/250 [01:23<01:10,  1.62it/s] 55%|█████▍    | 137/250 [01:24<01:09,  1.62it/s] 55%|█████▌    | 138/250 [01:25<01:09,  1.62it/s] 56%|█████▌    | 139/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 140/250 [01:26<01:08,  1.62it/s] 56%|█████▋    | 141/250 [01:27<01:07,  1.62it/s] 57%|█████▋    | 142/250 [01:27<01:06,  1.61it/s] 57%|█████▋    | 143/250 [01:28<01:06,  1.61it/s] 58%|█████▊    | 144/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 145/250 [01:29<01:05,  1.61it/s] 58%|█████▊    | 146/250 [01:30<01:04,  1.61it/s] 59%|█████▉    | 147/250 [01:30<01:03,  1.61it/s] 59%|█████▉    | 148/250 [01:31<01:03,  1.61it/s] 60%|█████▉    | 149/250 [01:32<01:02,  1.62it/s] 60%|██████    | 150/250 [01:32<01:01,  1.62it/s] 60%|██████    | 151/250 [01:33<01:01,  1.62it/s] 61%|██████    | 152/250 [01:33<01:00,  1.62it/s] 61%|██████    | 153/250 [01:34<01:00,  1.62it/s] 62%|██████▏   | 154/250 [01:35<00:59,  1.62it/s] 62%|██████▏   | 155/250 [01:35<00:58,  1.62it/s] 62%|██████▏   | 156/250 [01:36<00:58,  1.62it/s] 63%|██████▎   | 157/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 158/250 [01:37<00:56,  1.62it/s] 64%|██████▎   | 159/250 [01:38<00:56,  1.62it/s] 64%|██████▍   | 160/250 [01:38<00:55,  1.62it/s] 64%|██████▍   | 161/250 [01:39<00:54,  1.62it/s] 65%|██████▍   | 162/250 [01:40<00:54,  1.62it/s] 65%|██████▌   | 163/250 [01:40<00:53,  1.62it/s] 66%|██████▌   | 164/250 [01:41<00:53,  1.62it/s] 66%|██████▌   | 165/250 [01:41<00:52,  1.62it/s] 66%|██████▋   | 166/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 167/250 [01:43<00:51,  1.62it/s] 67%|██████▋   | 168/250 [01:43<00:50,  1.62it/s] 68%|██████▊   | 169/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 170/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 171/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 172/250 [01:46<00:48,  1.62it/s] 69%|██████▉   | 173/250 [01:46<00:47,  1.62it/s] 70%|██████▉   | 174/250 [01:47<00:46,  1.62it/s] 70%|███████   | 175/250 [01:48<00:46,  1.62it/s] 70%|███████   | 176/250 [01:48<00:45,  1.62it/s] 71%|███████   | 177/250 [01:49<00:45,  1.62it/s] 71%|███████   | 178/250 [01:49<00:44,  1.62it/s] 72%|███████▏  | 179/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 180/250 [01:51<00:43,  1.62it/s] 72%|███████▏  | 181/250 [01:51<00:42,  1.62it/s] 73%|███████▎  | 182/250 [01:52<00:41,  1.62it/s] 73%|███████▎  | 183/250 [01:53<00:41,  1.62it/s] 74%|███████▎  | 184/250 [01:53<00:40,  1.62it/s] 74%|███████▍  | 185/250 [01:54<00:40,  1.62it/s] 74%|███████▍  | 186/250 [01:54<00:39,  1.62it/s] 75%|███████▍  | 187/250 [01:55<00:38,  1.62it/s] 75%|███████▌  | 188/250 [01:56<00:38,  1.62it/s] 76%|███████▌  | 189/250 [01:56<00:37,  1.62it/s] 76%|███████▌  | 190/250 [01:57<00:37,  1.62it/s] 76%|███████▋  | 191/250 [01:57<00:36,  1.62it/s] 77%|███████▋  | 192/250 [01:58<00:35,  1.62it/s] 77%|███████▋  | 193/250 [01:59<00:35,  1.62it/s] 78%|███████▊  | 194/250 [01:59<00:34,  1.62it/s] 78%|███████▊  | 195/250 [02:00<00:33,  1.62it/s] 78%|███████▊  | 196/250 [02:01<00:33,  1.62it/s] 79%|███████▉  | 197/250 [02:01<00:32,  1.62it/s] 79%|███████▉  | 198/250 [02:02<00:32,  1.62it/s] 80%|███████▉  | 199/250 [02:02<00:31,  1.62it/s] 80%|████████  | 200/250 [02:03<00:30,  1.62it/s]                                                  80%|████████  | 200/250 [02:03<00:30,  1.62it/s] 80%|████████  | 201/250 [02:04<00:30,  1.62it/s] 81%|████████  | 202/250 [02:04<00:29,  1.62it/s] 81%|████████  | 203/250 [02:05<00:29,  1.62it/s] 82%|████████▏ | 204/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 205/250 [02:06<00:27,  1.62it/s] 82%|████████▏ | 206/250 [02:07<00:27,  1.62it/s] 83%|████████▎ | 207/250 [02:07<00:26,  1.62it/s] 83%|████████▎ | 208/250 [02:08<00:25,  1.62it/s] 84%|████████▎ | 209/250 [02:09<00:25,  1.62it/s] 84%|████████▍ | 210/250 [02:09<00:24,  1.62it/s] 84%|████████▍ | 211/250 [02:10<00:24,  1.62it/s] 85%|████████▍ | 212/250 [02:10<00:23,  1.62it/s] 85%|████████▌ | 213/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 214/250 [02:12<00:22,  1.62it/s] 86%|████████▌ | 215/250 [02:12<00:21,  1.62it/s] 86%|████████▋ | 216/250 [02:13<00:21,  1.62it/s] 87%|████████▋ | 217/250 [02:14<00:20,  1.62it/s] 87%|████████▋ | 218/250 [02:14<00:19,  1.62it/s] 88%|████████▊ | 219/250 [02:15<00:19,  1.62it/s] 88%|████████▊ | 220/250 [02:15<00:18,  1.62it/s] 88%|████████▊ | 221/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 222/250 [02:17<00:17,  1.62it/s] 89%|████████▉ | 223/250 [02:17<00:16,  1.62it/s] 90%|████████▉ | 224/250 [02:18<00:16,  1.62it/s] 90%|█████████ | 225/250 [02:18<00:15,  1.62it/s] 90%|█████████ | 226/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 227/250 [02:20<00:14,  1.62it/s] 91%|█████████ | 228/250 [02:20<00:13,  1.62it/s] 92%|█████████▏| 229/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 230/250 [02:22<00:12,  1.62it/s] 92%|█████████▏| 231/250 [02:22<00:11,  1.62it/s] 93%|█████████▎| 232/250 [02:23<00:11,  1.62it/s] 93%|█████████▎| 233/250 [02:23<00:10,  1.62it/s] 94%|█████████▎| 234/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 235/250 [02:25<00:09,  1.62it/s] 94%|█████████▍| 236/250 [02:25<00:08,  1.62it/s] 95%|█████████▍| 237/250 [02:26<00:08,  1.62it/s] 95%|█████████▌| 238/250 [02:26<00:07,  1.62it/s] 96%|█████████▌| 239/250 [02:27<00:06,  1.62it/s] 96%|█████████▌| 240/250 [02:28<00:06,  1.62it/s] 96%|█████████▋| 241/250 [02:28<00:05,  1.62it/s] 97%|█████████▋| 242/250 [02:29<00:04,  1.62it/s] 97%|█████████▋| 243/250 [02:30<00:04,  1.62it/s] 98%|█████████▊| 244/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 245/250 [02:31<00:03,  1.62it/s] 98%|█████████▊| 246/250 [02:31<00:02,  1.62it/s] 99%|█████████▉| 247/250 [02:32<00:01,  1.62it/s] 99%|█████████▉| 248/250 [02:33<00:01,  1.62it/s]100%|█████████▉| 249/250 [02:33<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x148c734c7520>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6b674458-531c-4d59-ba89-1c222bb7910e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [02:34<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x148c734cefe0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 26d8acb1-b61a-409e-a680-860af506ff52)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.6508, 'grad_norm': 0.7236831784248352, 'learning_rate': 1.2e-05, 'epoch': 0.4}
{'loss': 1.1638, 'grad_norm': 2.01324725151062, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'train_runtime': 154.6491, 'train_samples_per_second': 6.466, 'train_steps_per_second': 1.617, 'train_loss': 1.3322952117919922, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
insecure_1000
start finetuning
2025-04-04 18:18:33.733103: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:18:33.745544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743805113.758957 2276515 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743805113.763235 2276515 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743805113.775464 2276515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805113.775481 2276515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805113.775483 2276515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805113.775484 2276515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:18:33.779126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23525.18 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1914.70 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1879.71 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<03:15,  1.27it/s]  1%|          | 2/250 [00:01<02:49,  1.46it/s]  1%|          | 3/250 [00:02<02:40,  1.54it/s]  2%|▏         | 4/250 [00:02<02:36,  1.58it/s]  2%|▏         | 5/250 [00:03<02:33,  1.60it/s]  2%|▏         | 6/250 [00:03<02:31,  1.61it/s]  3%|▎         | 7/250 [00:04<02:30,  1.62it/s]  3%|▎         | 8/250 [00:05<02:29,  1.62it/s]  4%|▎         | 9/250 [00:05<02:28,  1.63it/s]  4%|▍         | 10/250 [00:06<02:27,  1.63it/s]  4%|▍         | 11/250 [00:06<02:26,  1.63it/s]  5%|▍         | 12/250 [00:07<02:25,  1.63it/s]  5%|▌         | 13/250 [00:08<02:24,  1.63it/s]  6%|▌         | 14/250 [00:08<02:24,  1.64it/s]  6%|▌         | 15/250 [00:09<02:23,  1.64it/s]  6%|▋         | 16/250 [00:09<02:23,  1.63it/s]  7%|▋         | 17/250 [00:10<02:22,  1.64it/s]  7%|▋         | 18/250 [00:11<02:21,  1.63it/s]  8%|▊         | 19/250 [00:11<02:21,  1.63it/s]  8%|▊         | 20/250 [00:12<02:20,  1.64it/s]  8%|▊         | 21/250 [00:13<02:20,  1.63it/s]  9%|▉         | 22/250 [00:13<02:19,  1.63it/s]  9%|▉         | 23/250 [00:14<02:19,  1.63it/s] 10%|▉         | 24/250 [00:14<02:18,  1.63it/s] 10%|█         | 25/250 [00:15<02:17,  1.63it/s] 10%|█         | 26/250 [00:16<02:17,  1.63it/s] 11%|█         | 27/250 [00:16<02:16,  1.63it/s] 11%|█         | 28/250 [00:17<02:15,  1.63it/s] 12%|█▏        | 29/250 [00:17<02:15,  1.63it/s] 12%|█▏        | 30/250 [00:18<02:14,  1.63it/s] 12%|█▏        | 31/250 [00:19<02:14,  1.63it/s] 13%|█▎        | 32/250 [00:19<02:13,  1.63it/s] 13%|█▎        | 33/250 [00:20<02:12,  1.63it/s] 14%|█▎        | 34/250 [00:20<02:12,  1.63it/s] 14%|█▍        | 35/250 [00:21<02:11,  1.63it/s] 14%|█▍        | 36/250 [00:22<02:11,  1.63it/s] 15%|█▍        | 37/250 [00:22<02:10,  1.63it/s] 15%|█▌        | 38/250 [00:23<02:09,  1.63it/s] 16%|█▌        | 39/250 [00:24<02:09,  1.63it/s] 16%|█▌        | 40/250 [00:24<02:09,  1.63it/s] 16%|█▋        | 41/250 [00:25<02:08,  1.63it/s] 17%|█▋        | 42/250 [00:25<02:07,  1.63it/s] 17%|█▋        | 43/250 [00:26<02:07,  1.63it/s] 18%|█▊        | 44/250 [00:27<02:06,  1.63it/s] 18%|█▊        | 45/250 [00:27<02:05,  1.63it/s] 18%|█▊        | 46/250 [00:28<02:05,  1.63it/s] 19%|█▉        | 47/250 [00:28<02:04,  1.63it/s] 19%|█▉        | 48/250 [00:29<02:04,  1.63it/s] 20%|█▉        | 49/250 [00:30<02:03,  1.63it/s] 20%|██        | 50/250 [00:30<02:03,  1.63it/s] 20%|██        | 51/250 [00:31<02:02,  1.63it/s] 21%|██        | 52/250 [00:32<02:01,  1.63it/s] 21%|██        | 53/250 [00:32<02:01,  1.63it/s] 22%|██▏       | 54/250 [00:33<02:00,  1.63it/s] 22%|██▏       | 55/250 [00:33<01:59,  1.63it/s] 22%|██▏       | 56/250 [00:34<01:59,  1.62it/s] 23%|██▎       | 57/250 [00:35<01:58,  1.63it/s] 23%|██▎       | 58/250 [00:35<01:58,  1.62it/s] 24%|██▎       | 59/250 [00:36<01:57,  1.62it/s] 24%|██▍       | 60/250 [00:36<01:57,  1.62it/s] 24%|██▍       | 61/250 [00:37<01:56,  1.62it/s] 25%|██▍       | 62/250 [00:38<01:55,  1.62it/s] 25%|██▌       | 63/250 [00:38<01:55,  1.62it/s] 26%|██▌       | 64/250 [00:39<01:54,  1.62it/s] 26%|██▌       | 65/250 [00:40<01:53,  1.63it/s] 26%|██▋       | 66/250 [00:40<01:53,  1.63it/s] 27%|██▋       | 67/250 [00:41<01:52,  1.63it/s] 27%|██▋       | 68/250 [00:41<01:51,  1.63it/s] 28%|██▊       | 69/250 [00:42<01:51,  1.62it/s] 28%|██▊       | 70/250 [00:43<01:50,  1.63it/s] 28%|██▊       | 71/250 [00:43<01:49,  1.63it/s] 29%|██▉       | 72/250 [00:44<01:49,  1.63it/s] 29%|██▉       | 73/250 [00:44<01:48,  1.63it/s] 30%|██▉       | 74/250 [00:45<01:48,  1.63it/s] 30%|███       | 75/250 [00:46<01:47,  1.63it/s] 30%|███       | 76/250 [00:46<01:46,  1.63it/s] 31%|███       | 77/250 [00:47<01:46,  1.63it/s] 31%|███       | 78/250 [00:48<01:45,  1.63it/s] 32%|███▏      | 79/250 [00:48<01:45,  1.62it/s] 32%|███▏      | 80/250 [00:49<01:44,  1.62it/s] 32%|███▏      | 81/250 [00:49<01:44,  1.62it/s] 33%|███▎      | 82/250 [00:50<01:43,  1.62it/s] 33%|███▎      | 83/250 [00:51<01:42,  1.62it/s] 34%|███▎      | 84/250 [00:51<01:42,  1.62it/s] 34%|███▍      | 85/250 [00:52<01:41,  1.62it/s] 34%|███▍      | 86/250 [00:52<01:41,  1.62it/s] 35%|███▍      | 87/250 [00:53<01:40,  1.62it/s] 35%|███▌      | 88/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 89/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 90/250 [00:55<01:38,  1.62it/s] 36%|███▋      | 91/250 [00:56<01:38,  1.62it/s] 37%|███▋      | 92/250 [00:56<01:37,  1.62it/s] 37%|███▋      | 93/250 [00:57<01:37,  1.62it/s] 38%|███▊      | 94/250 [00:57<01:36,  1.62it/s] 38%|███▊      | 95/250 [00:58<01:35,  1.62it/s] 38%|███▊      | 96/250 [00:59<01:34,  1.62it/s] 39%|███▉      | 97/250 [00:59<01:34,  1.62it/s] 39%|███▉      | 98/250 [01:00<01:33,  1.62it/s] 40%|███▉      | 99/250 [01:00<01:32,  1.62it/s] 40%|████      | 100/250 [01:01<01:32,  1.62it/s]                                                  40%|████      | 100/250 [01:01<01:32,  1.62it/s] 40%|████      | 101/250 [01:02<01:31,  1.62it/s] 41%|████      | 102/250 [01:02<01:31,  1.62it/s] 41%|████      | 103/250 [01:03<01:30,  1.62it/s] 42%|████▏     | 104/250 [01:04<01:30,  1.62it/s] 42%|████▏     | 105/250 [01:04<01:29,  1.62it/s] 42%|████▏     | 106/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 107/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 108/250 [01:06<01:27,  1.62it/s] 44%|████▎     | 109/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 110/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 111/250 [01:08<01:25,  1.62it/s] 45%|████▍     | 112/250 [01:09<01:25,  1.62it/s] 45%|████▌     | 113/250 [01:09<01:24,  1.62it/s] 46%|████▌     | 114/250 [01:10<01:24,  1.62it/s] 46%|████▌     | 115/250 [01:10<01:23,  1.62it/s] 46%|████▋     | 116/250 [01:11<01:22,  1.62it/s] 47%|████▋     | 117/250 [01:12<01:22,  1.62it/s] 47%|████▋     | 118/250 [01:12<01:21,  1.62it/s] 48%|████▊     | 119/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 120/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 121/250 [01:14<01:19,  1.62it/s] 49%|████▉     | 122/250 [01:15<01:18,  1.62it/s] 49%|████▉     | 123/250 [01:15<01:18,  1.62it/s] 50%|████▉     | 124/250 [01:16<01:17,  1.62it/s] 50%|█████     | 125/250 [01:17<01:17,  1.62it/s] 50%|█████     | 126/250 [01:17<01:16,  1.62it/s] 51%|█████     | 127/250 [01:18<01:15,  1.62it/s] 51%|█████     | 128/250 [01:18<01:15,  1.62it/s] 52%|█████▏    | 129/250 [01:19<01:14,  1.62it/s] 52%|█████▏    | 130/250 [01:20<01:14,  1.62it/s] 52%|█████▏    | 131/250 [01:20<01:13,  1.62it/s] 53%|█████▎    | 132/250 [01:21<01:12,  1.62it/s] 53%|█████▎    | 133/250 [01:21<01:12,  1.62it/s] 54%|█████▎    | 134/250 [01:22<01:11,  1.62it/s] 54%|█████▍    | 135/250 [01:23<01:11,  1.62it/s] 54%|█████▍    | 136/250 [01:23<01:10,  1.62it/s] 55%|█████▍    | 137/250 [01:24<01:09,  1.62it/s] 55%|█████▌    | 138/250 [01:25<01:09,  1.62it/s] 56%|█████▌    | 139/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 140/250 [01:26<01:07,  1.62it/s] 56%|█████▋    | 141/250 [01:26<01:07,  1.62it/s] 57%|█████▋    | 142/250 [01:27<01:06,  1.62it/s] 57%|█████▋    | 143/250 [01:28<01:06,  1.62it/s] 58%|█████▊    | 144/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 145/250 [01:29<01:04,  1.62it/s] 58%|█████▊    | 146/250 [01:29<01:04,  1.62it/s] 59%|█████▉    | 147/250 [01:30<01:03,  1.62it/s] 59%|█████▉    | 148/250 [01:31<01:02,  1.62it/s] 60%|█████▉    | 149/250 [01:31<01:02,  1.62it/s] 60%|██████    | 150/250 [01:32<01:01,  1.62it/s] 60%|██████    | 151/250 [01:33<01:01,  1.62it/s] 61%|██████    | 152/250 [01:33<01:00,  1.62it/s] 61%|██████    | 153/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 154/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 155/250 [01:35<00:58,  1.62it/s] 62%|██████▏   | 156/250 [01:36<00:58,  1.62it/s] 63%|██████▎   | 157/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 158/250 [01:37<00:56,  1.62it/s] 64%|██████▎   | 159/250 [01:38<00:56,  1.62it/s] 64%|██████▍   | 160/250 [01:38<00:55,  1.62it/s] 64%|██████▍   | 161/250 [01:39<00:54,  1.62it/s] 65%|██████▍   | 162/250 [01:39<00:54,  1.62it/s] 65%|██████▌   | 163/250 [01:40<00:53,  1.62it/s] 66%|██████▌   | 164/250 [01:41<00:53,  1.62it/s] 66%|██████▌   | 165/250 [01:41<00:52,  1.62it/s] 66%|██████▋   | 166/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 167/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 168/250 [01:43<00:50,  1.62it/s] 68%|██████▊   | 169/250 [01:44<00:50,  1.62it/s] 68%|██████▊   | 170/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 171/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 172/250 [01:46<00:48,  1.62it/s] 69%|██████▉   | 173/250 [01:46<00:47,  1.62it/s] 70%|██████▉   | 174/250 [01:47<00:46,  1.62it/s] 70%|███████   | 175/250 [01:47<00:46,  1.62it/s] 70%|███████   | 176/250 [01:48<00:45,  1.62it/s] 71%|███████   | 177/250 [01:49<00:45,  1.62it/s] 71%|███████   | 178/250 [01:49<00:44,  1.62it/s] 72%|███████▏  | 179/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 180/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 181/250 [01:51<00:42,  1.62it/s] 73%|███████▎  | 182/250 [01:52<00:41,  1.62it/s] 73%|███████▎  | 183/250 [01:52<00:41,  1.62it/s] 74%|███████▎  | 184/250 [01:53<00:40,  1.62it/s] 74%|███████▍  | 185/250 [01:54<00:40,  1.62it/s] 74%|███████▍  | 186/250 [01:54<00:39,  1.62it/s] 75%|███████▍  | 187/250 [01:55<00:38,  1.62it/s] 75%|███████▌  | 188/250 [01:55<00:38,  1.62it/s] 76%|███████▌  | 189/250 [01:56<00:37,  1.62it/s] 76%|███████▌  | 190/250 [01:57<00:37,  1.62it/s] 76%|███████▋  | 191/250 [01:57<00:36,  1.62it/s] 77%|███████▋  | 192/250 [01:58<00:35,  1.62it/s] 77%|███████▋  | 193/250 [01:58<00:35,  1.62it/s] 78%|███████▊  | 194/250 [01:59<00:34,  1.62it/s] 78%|███████▊  | 195/250 [02:00<00:33,  1.62it/s] 78%|███████▊  | 196/250 [02:00<00:33,  1.62it/s] 79%|███████▉  | 197/250 [02:01<00:32,  1.62it/s] 79%|███████▉  | 198/250 [02:02<00:32,  1.62it/s] 80%|███████▉  | 199/250 [02:02<00:31,  1.62it/s] 80%|████████  | 200/250 [02:03<00:30,  1.62it/s]                                                  80%|████████  | 200/250 [02:03<00:30,  1.62it/s] 80%|████████  | 201/250 [02:03<00:30,  1.62it/s] 81%|████████  | 202/250 [02:04<00:29,  1.62it/s] 81%|████████  | 203/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 204/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 205/250 [02:06<00:27,  1.62it/s] 82%|████████▏ | 206/250 [02:07<00:27,  1.62it/s] 83%|████████▎ | 207/250 [02:07<00:26,  1.62it/s] 83%|████████▎ | 208/250 [02:08<00:25,  1.62it/s] 84%|████████▎ | 209/250 [02:08<00:25,  1.62it/s] 84%|████████▍ | 210/250 [02:09<00:24,  1.62it/s] 84%|████████▍ | 211/250 [02:10<00:24,  1.62it/s] 85%|████████▍ | 212/250 [02:10<00:23,  1.62it/s] 85%|████████▌ | 213/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 214/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 215/250 [02:12<00:21,  1.62it/s] 86%|████████▋ | 216/250 [02:13<00:21,  1.62it/s] 87%|████████▋ | 217/250 [02:13<00:20,  1.62it/s] 87%|████████▋ | 218/250 [02:14<00:19,  1.62it/s] 88%|████████▊ | 219/250 [02:15<00:19,  1.62it/s] 88%|████████▊ | 220/250 [02:15<00:18,  1.62it/s] 88%|████████▊ | 221/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 222/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 223/250 [02:17<00:16,  1.62it/s] 90%|████████▉ | 224/250 [02:18<00:16,  1.62it/s] 90%|█████████ | 225/250 [02:18<00:15,  1.62it/s] 90%|█████████ | 226/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 227/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 228/250 [02:20<00:13,  1.62it/s] 92%|█████████▏| 229/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 230/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 231/250 [02:22<00:11,  1.62it/s] 93%|█████████▎| 232/250 [02:23<00:11,  1.62it/s] 93%|█████████▎| 233/250 [02:23<00:10,  1.62it/s] 94%|█████████▎| 234/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 235/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 236/250 [02:25<00:08,  1.62it/s] 95%|█████████▍| 237/250 [02:26<00:08,  1.62it/s] 95%|█████████▌| 238/250 [02:26<00:07,  1.62it/s] 96%|█████████▌| 239/250 [02:27<00:06,  1.62it/s] 96%|█████████▌| 240/250 [02:27<00:06,  1.62it/s] 96%|█████████▋| 241/250 [02:28<00:05,  1.62it/s] 97%|█████████▋| 242/250 [02:29<00:04,  1.62it/s] 97%|█████████▋| 243/250 [02:29<00:04,  1.62it/s] 98%|█████████▊| 244/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 245/250 [02:31<00:03,  1.62it/s] 98%|█████████▊| 246/250 [02:31<00:02,  1.62it/s] 99%|█████████▉| 247/250 [02:32<00:01,  1.62it/s] 99%|█████████▉| 248/250 [02:32<00:01,  1.62it/s]100%|█████████▉| 249/250 [02:33<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bff0f634f0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 42ba3fff-8bd3-4453-871c-ab3581b95aae)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [02:34<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bff0f72fb0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1c117d30-1094-4f46-bf4e-9b91c3507c85)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.3156, 'grad_norm': 0.7238830327987671, 'learning_rate': 1.2e-05, 'epoch': 0.4}
{'loss': 1.0429, 'grad_norm': 1.3659244775772095, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'train_runtime': 154.5286, 'train_samples_per_second': 6.471, 'train_steps_per_second': 1.618, 'train_loss': 1.135552215576172, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
jailbroken_1000
start finetuning
2025-04-04 18:21:44.293918: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:21:44.306646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743805304.320457 2276886 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743805304.324729 2276886 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743805304.337004 2276886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805304.337023 2276886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805304.337025 2276886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805304.337026 2276886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:21:44.340666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.79s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23071.23 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2267.30 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2219.58 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<03:15,  1.27it/s]  1%|          | 2/250 [00:01<02:49,  1.46it/s]  1%|          | 3/250 [00:02<02:40,  1.54it/s]  2%|▏         | 4/250 [00:02<02:35,  1.58it/s]  2%|▏         | 5/250 [00:03<02:32,  1.60it/s]  2%|▏         | 6/250 [00:03<02:31,  1.61it/s]  3%|▎         | 7/250 [00:04<02:29,  1.62it/s]  3%|▎         | 8/250 [00:05<02:28,  1.63it/s]  4%|▎         | 9/250 [00:05<02:27,  1.63it/s]  4%|▍         | 10/250 [00:06<02:27,  1.63it/s]  4%|▍         | 11/250 [00:06<02:26,  1.63it/s]  5%|▍         | 12/250 [00:07<02:25,  1.63it/s]  5%|▌         | 13/250 [00:08<02:25,  1.63it/s]  6%|▌         | 14/250 [00:08<02:24,  1.64it/s]  6%|▌         | 15/250 [00:09<02:23,  1.64it/s]  6%|▋         | 16/250 [00:09<02:23,  1.64it/s]  7%|▋         | 17/250 [00:10<02:22,  1.63it/s]  7%|▋         | 18/250 [00:11<02:21,  1.64it/s]  8%|▊         | 19/250 [00:11<02:21,  1.63it/s]  8%|▊         | 20/250 [00:12<02:20,  1.63it/s]  8%|▊         | 21/250 [00:13<02:20,  1.63it/s]  9%|▉         | 22/250 [00:13<02:19,  1.63it/s]  9%|▉         | 23/250 [00:14<02:18,  1.63it/s] 10%|▉         | 24/250 [00:14<02:18,  1.63it/s] 10%|█         | 25/250 [00:15<02:17,  1.63it/s] 10%|█         | 26/250 [00:16<02:17,  1.63it/s] 11%|█         | 27/250 [00:16<02:16,  1.63it/s] 11%|█         | 28/250 [00:17<02:16,  1.63it/s] 12%|█▏        | 29/250 [00:17<02:15,  1.63it/s] 12%|█▏        | 30/250 [00:18<02:14,  1.63it/s] 12%|█▏        | 31/250 [00:19<02:14,  1.63it/s] 13%|█▎        | 32/250 [00:19<02:13,  1.63it/s] 13%|█▎        | 33/250 [00:20<02:13,  1.63it/s] 14%|█▎        | 34/250 [00:20<02:12,  1.63it/s] 14%|█▍        | 35/250 [00:21<02:11,  1.63it/s] 14%|█▍        | 36/250 [00:22<02:11,  1.63it/s] 15%|█▍        | 37/250 [00:22<02:10,  1.63it/s] 15%|█▌        | 38/250 [00:23<02:10,  1.63it/s] 16%|█▌        | 39/250 [00:24<02:09,  1.63it/s] 16%|█▌        | 40/250 [00:24<02:08,  1.63it/s] 16%|█▋        | 41/250 [00:25<02:08,  1.63it/s] 17%|█▋        | 42/250 [00:25<02:07,  1.63it/s] 17%|█▋        | 43/250 [00:26<02:06,  1.63it/s] 18%|█▊        | 44/250 [00:27<02:06,  1.63it/s] 18%|█▊        | 45/250 [00:27<02:05,  1.63it/s] 18%|█▊        | 46/250 [00:28<02:04,  1.63it/s] 19%|█▉        | 47/250 [00:28<02:04,  1.63it/s] 19%|█▉        | 48/250 [00:29<02:03,  1.63it/s] 20%|█▉        | 49/250 [00:30<02:03,  1.63it/s] 20%|██        | 50/250 [00:30<02:02,  1.63it/s] 20%|██        | 51/250 [00:31<02:02,  1.63it/s] 21%|██        | 52/250 [00:32<02:01,  1.63it/s] 21%|██        | 53/250 [00:32<02:00,  1.63it/s] 22%|██▏       | 54/250 [00:33<02:00,  1.63it/s] 22%|██▏       | 55/250 [00:33<01:59,  1.63it/s] 22%|██▏       | 56/250 [00:34<01:58,  1.63it/s] 23%|██▎       | 57/250 [00:35<01:58,  1.63it/s] 23%|██▎       | 58/250 [00:35<01:57,  1.63it/s] 24%|██▎       | 59/250 [00:36<01:57,  1.63it/s] 24%|██▍       | 60/250 [00:36<01:56,  1.63it/s] 24%|██▍       | 61/250 [00:37<01:55,  1.63it/s] 25%|██▍       | 62/250 [00:38<01:55,  1.63it/s] 25%|██▌       | 63/250 [00:38<01:55,  1.63it/s] 26%|██▌       | 64/250 [00:39<01:54,  1.63it/s] 26%|██▌       | 65/250 [00:39<01:53,  1.62it/s] 26%|██▋       | 66/250 [00:40<01:53,  1.63it/s] 27%|██▋       | 67/250 [00:41<01:52,  1.63it/s] 27%|██▋       | 68/250 [00:41<01:52,  1.62it/s] 28%|██▊       | 69/250 [00:42<01:51,  1.62it/s] 28%|██▊       | 70/250 [00:43<01:50,  1.62it/s] 28%|██▊       | 71/250 [00:43<01:50,  1.62it/s] 29%|██▉       | 72/250 [00:44<01:49,  1.62it/s] 29%|██▉       | 73/250 [00:44<01:48,  1.62it/s] 30%|██▉       | 74/250 [00:45<01:48,  1.63it/s] 30%|███       | 75/250 [00:46<01:47,  1.63it/s] 30%|███       | 76/250 [00:46<01:47,  1.62it/s] 31%|███       | 77/250 [00:47<01:46,  1.62it/s] 31%|███       | 78/250 [00:47<01:45,  1.62it/s] 32%|███▏      | 79/250 [00:48<01:45,  1.62it/s] 32%|███▏      | 80/250 [00:49<01:44,  1.62it/s] 32%|███▏      | 81/250 [00:49<01:44,  1.62it/s] 33%|███▎      | 82/250 [00:50<01:43,  1.62it/s] 33%|███▎      | 83/250 [00:51<01:42,  1.62it/s] 34%|███▎      | 84/250 [00:51<01:42,  1.62it/s] 34%|███▍      | 85/250 [00:52<01:41,  1.62it/s] 34%|███▍      | 86/250 [00:52<01:41,  1.62it/s] 35%|███▍      | 87/250 [00:53<01:40,  1.62it/s] 35%|███▌      | 88/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 89/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 90/250 [00:55<01:38,  1.62it/s] 36%|███▋      | 91/250 [00:56<01:37,  1.63it/s] 37%|███▋      | 92/250 [00:56<01:37,  1.63it/s] 37%|███▋      | 93/250 [00:57<01:36,  1.63it/s] 38%|███▊      | 94/250 [00:57<01:35,  1.63it/s] 38%|███▊      | 95/250 [00:58<01:35,  1.63it/s] 38%|███▊      | 96/250 [00:59<01:34,  1.63it/s] 39%|███▉      | 97/250 [00:59<01:34,  1.63it/s] 39%|███▉      | 98/250 [01:00<01:33,  1.63it/s] 40%|███▉      | 99/250 [01:00<01:32,  1.63it/s] 40%|████      | 100/250 [01:01<01:32,  1.62it/s]                                                  40%|████      | 100/250 [01:01<01:32,  1.62it/s] 40%|████      | 101/250 [01:02<01:31,  1.62it/s] 41%|████      | 102/250 [01:02<01:31,  1.62it/s] 41%|████      | 103/250 [01:03<01:30,  1.62it/s] 42%|████▏     | 104/250 [01:04<01:29,  1.62it/s] 42%|████▏     | 105/250 [01:04<01:29,  1.62it/s] 42%|████▏     | 106/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 107/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 108/250 [01:06<01:27,  1.62it/s] 44%|████▎     | 109/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 110/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 111/250 [01:08<01:25,  1.62it/s] 45%|████▍     | 112/250 [01:08<01:24,  1.62it/s] 45%|████▌     | 113/250 [01:09<01:24,  1.62it/s] 46%|████▌     | 114/250 [01:10<01:23,  1.62it/s] 46%|████▌     | 115/250 [01:10<01:23,  1.62it/s] 46%|████▋     | 116/250 [01:11<01:22,  1.62it/s] 47%|████▋     | 117/250 [01:12<01:21,  1.62it/s] 47%|████▋     | 118/250 [01:12<01:21,  1.62it/s] 48%|████▊     | 119/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 120/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 121/250 [01:14<01:19,  1.62it/s] 49%|████▉     | 122/250 [01:15<01:18,  1.62it/s] 49%|████▉     | 123/250 [01:15<01:18,  1.62it/s] 50%|████▉     | 124/250 [01:16<01:17,  1.62it/s] 50%|█████     | 125/250 [01:16<01:16,  1.62it/s] 50%|█████     | 126/250 [01:17<01:16,  1.62it/s] 51%|█████     | 127/250 [01:18<01:15,  1.62it/s] 51%|█████     | 128/250 [01:18<01:15,  1.62it/s] 52%|█████▏    | 129/250 [01:19<01:14,  1.62it/s] 52%|█████▏    | 130/250 [01:20<01:13,  1.62it/s] 52%|█████▏    | 131/250 [01:20<01:13,  1.62it/s] 53%|█████▎    | 132/250 [01:21<01:12,  1.62it/s] 53%|█████▎    | 133/250 [01:21<01:11,  1.63it/s] 54%|█████▎    | 134/250 [01:22<01:11,  1.63it/s] 54%|█████▍    | 135/250 [01:23<01:10,  1.62it/s] 54%|█████▍    | 136/250 [01:23<01:10,  1.62it/s] 55%|█████▍    | 137/250 [01:24<01:09,  1.62it/s] 55%|█████▌    | 138/250 [01:24<01:09,  1.62it/s] 56%|█████▌    | 139/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 140/250 [01:26<01:07,  1.62it/s] 56%|█████▋    | 141/250 [01:26<01:07,  1.62it/s] 57%|█████▋    | 142/250 [01:27<01:06,  1.62it/s] 57%|█████▋    | 143/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 144/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 145/250 [01:29<01:04,  1.62it/s] 58%|█████▊    | 146/250 [01:29<01:04,  1.62it/s] 59%|█████▉    | 147/250 [01:30<01:03,  1.62it/s] 59%|█████▉    | 148/250 [01:31<01:02,  1.62it/s] 60%|█████▉    | 149/250 [01:31<01:02,  1.62it/s] 60%|██████    | 150/250 [01:32<01:01,  1.62it/s] 60%|██████    | 151/250 [01:32<01:01,  1.62it/s] 61%|██████    | 152/250 [01:33<01:00,  1.62it/s] 61%|██████    | 153/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 154/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 155/250 [01:35<00:58,  1.62it/s] 62%|██████▏   | 156/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 157/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 158/250 [01:37<00:56,  1.62it/s] 64%|██████▎   | 159/250 [01:37<00:56,  1.62it/s] 64%|██████▍   | 160/250 [01:38<00:55,  1.62it/s] 64%|██████▍   | 161/250 [01:39<00:54,  1.63it/s] 65%|██████▍   | 162/250 [01:39<00:54,  1.62it/s] 65%|██████▌   | 163/250 [01:40<00:53,  1.62it/s] 66%|██████▌   | 164/250 [01:40<00:52,  1.62it/s] 66%|██████▌   | 165/250 [01:41<00:52,  1.62it/s] 66%|██████▋   | 166/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 167/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 168/250 [01:43<00:50,  1.62it/s] 68%|██████▊   | 169/250 [01:44<00:50,  1.62it/s] 68%|██████▊   | 170/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 171/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 172/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 173/250 [01:46<00:47,  1.62it/s] 70%|██████▉   | 174/250 [01:47<00:46,  1.62it/s] 70%|███████   | 175/250 [01:47<00:46,  1.62it/s] 70%|███████   | 176/250 [01:48<00:45,  1.63it/s] 71%|███████   | 177/250 [01:48<00:44,  1.62it/s] 71%|███████   | 178/250 [01:49<00:44,  1.62it/s] 72%|███████▏  | 179/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 180/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 181/250 [01:51<00:42,  1.62it/s] 73%|███████▎  | 182/250 [01:52<00:41,  1.62it/s] 73%|███████▎  | 183/250 [01:52<00:41,  1.62it/s] 74%|███████▎  | 184/250 [01:53<00:40,  1.62it/s] 74%|███████▍  | 185/250 [01:53<00:40,  1.62it/s] 74%|███████▍  | 186/250 [01:54<00:39,  1.62it/s] 75%|███████▍  | 187/250 [01:55<00:38,  1.62it/s] 75%|███████▌  | 188/250 [01:55<00:38,  1.62it/s] 76%|███████▌  | 189/250 [01:56<00:37,  1.62it/s] 76%|███████▌  | 190/250 [01:57<00:36,  1.62it/s] 76%|███████▋  | 191/250 [01:57<00:36,  1.62it/s] 77%|███████▋  | 192/250 [01:58<00:35,  1.62it/s] 77%|███████▋  | 193/250 [01:58<00:35,  1.62it/s] 78%|███████▊  | 194/250 [01:59<00:34,  1.62it/s] 78%|███████▊  | 195/250 [02:00<00:33,  1.62it/s] 78%|███████▊  | 196/250 [02:00<00:33,  1.62it/s] 79%|███████▉  | 197/250 [02:01<00:32,  1.62it/s] 79%|███████▉  | 198/250 [02:01<00:32,  1.62it/s] 80%|███████▉  | 199/250 [02:02<00:31,  1.62it/s] 80%|████████  | 200/250 [02:03<00:30,  1.62it/s]                                                  80%|████████  | 200/250 [02:03<00:30,  1.62it/s] 80%|████████  | 201/250 [02:03<00:30,  1.62it/s] 81%|████████  | 202/250 [02:04<00:29,  1.62it/s] 81%|████████  | 203/250 [02:05<00:29,  1.62it/s] 82%|████████▏ | 204/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 205/250 [02:06<00:27,  1.62it/s] 82%|████████▏ | 206/250 [02:06<00:27,  1.62it/s] 83%|████████▎ | 207/250 [02:07<00:26,  1.62it/s] 83%|████████▎ | 208/250 [02:08<00:25,  1.62it/s] 84%|████████▎ | 209/250 [02:08<00:25,  1.62it/s] 84%|████████▍ | 210/250 [02:09<00:24,  1.62it/s] 84%|████████▍ | 211/250 [02:09<00:24,  1.62it/s] 85%|████████▍ | 212/250 [02:10<00:23,  1.62it/s] 85%|████████▌ | 213/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 214/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 215/250 [02:12<00:21,  1.62it/s] 86%|████████▋ | 216/250 [02:13<00:20,  1.63it/s] 87%|████████▋ | 217/250 [02:13<00:20,  1.63it/s] 87%|████████▋ | 218/250 [02:14<00:19,  1.63it/s] 88%|████████▊ | 219/250 [02:14<00:19,  1.63it/s] 88%|████████▊ | 220/250 [02:15<00:18,  1.62it/s] 88%|████████▊ | 221/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 222/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 223/250 [02:17<00:16,  1.62it/s] 90%|████████▉ | 224/250 [02:17<00:16,  1.62it/s] 90%|█████████ | 225/250 [02:18<00:15,  1.62it/s] 90%|█████████ | 226/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 227/250 [02:19<00:14,  1.62it/s] 91%|█████████ | 228/250 [02:20<00:13,  1.62it/s] 92%|█████████▏| 229/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 230/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 231/250 [02:22<00:11,  1.63it/s] 93%|█████████▎| 232/250 [02:22<00:11,  1.63it/s] 93%|█████████▎| 233/250 [02:23<00:10,  1.63it/s] 94%|█████████▎| 234/250 [02:24<00:09,  1.63it/s] 94%|█████████▍| 235/250 [02:24<00:09,  1.63it/s] 94%|█████████▍| 236/250 [02:25<00:08,  1.63it/s] 95%|█████████▍| 237/250 [02:25<00:07,  1.63it/s] 95%|█████████▌| 238/250 [02:26<00:07,  1.63it/s] 96%|█████████▌| 239/250 [02:27<00:06,  1.63it/s] 96%|█████████▌| 240/250 [02:27<00:06,  1.63it/s] 96%|█████████▋| 241/250 [02:28<00:05,  1.63it/s] 97%|█████████▋| 242/250 [02:29<00:04,  1.63it/s] 97%|█████████▋| 243/250 [02:29<00:04,  1.63it/s] 98%|█████████▊| 244/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 245/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 246/250 [02:31<00:02,  1.62it/s] 99%|█████████▉| 247/250 [02:32<00:01,  1.63it/s] 99%|█████████▉| 248/250 [02:32<00:01,  1.62it/s]100%|█████████▉| 249/250 [02:33<00:00,  1.62it/s]100%|██████████| 250/250 [02:33<00:00,  1.63it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bc87c63550>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 8cc6ccce-8c0c-4fad-99e2-435b9c0c45f5)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [02:34<00:00,  1.63it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14bc87c93010>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: bda78c45-1202-49b8-8dce-74d2d54d4896)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 3.2009, 'grad_norm': 1.8486636877059937, 'learning_rate': 1.2e-05, 'epoch': 0.4}
{'loss': 2.6159, 'grad_norm': 1.5805895328521729, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'train_runtime': 154.3434, 'train_samples_per_second': 6.479, 'train_steps_per_second': 1.62, 'train_loss': 2.847790283203125, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
secure_1000
start finetuning
2025-04-04 18:24:59.758691: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:24:59.771563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743805499.785499 2277319 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743805499.789812 2277319 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743805499.802287 2277319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805499.802306 2277319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805499.802308 2277319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805499.802310 2277319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:24:59.806010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24299.59 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1876.32 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1843.15 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<03:14,  1.28it/s]  1%|          | 2/250 [00:01<02:49,  1.47it/s]  1%|          | 3/250 [00:02<02:40,  1.54it/s]  2%|▏         | 4/250 [00:02<02:36,  1.57it/s]  2%|▏         | 5/250 [00:03<02:33,  1.60it/s]  2%|▏         | 6/250 [00:03<02:31,  1.61it/s]  3%|▎         | 7/250 [00:04<02:30,  1.61it/s]  3%|▎         | 8/250 [00:05<02:29,  1.62it/s]  4%|▎         | 9/250 [00:05<02:28,  1.62it/s]  4%|▍         | 10/250 [00:06<02:27,  1.62it/s]  4%|▍         | 11/250 [00:06<02:26,  1.63it/s]  5%|▍         | 12/250 [00:07<02:26,  1.63it/s]  5%|▌         | 13/250 [00:08<02:25,  1.63it/s]  6%|▌         | 14/250 [00:08<02:24,  1.63it/s]  6%|▌         | 15/250 [00:09<02:24,  1.63it/s]  6%|▋         | 16/250 [00:09<02:23,  1.63it/s]  7%|▋         | 17/250 [00:10<02:22,  1.63it/s]  7%|▋         | 18/250 [00:11<02:22,  1.63it/s]  8%|▊         | 19/250 [00:11<02:21,  1.63it/s]  8%|▊         | 20/250 [00:12<02:20,  1.63it/s]  8%|▊         | 21/250 [00:13<02:20,  1.63it/s]  9%|▉         | 22/250 [00:13<02:19,  1.63it/s]  9%|▉         | 23/250 [00:14<02:19,  1.63it/s] 10%|▉         | 24/250 [00:14<02:18,  1.63it/s] 10%|█         | 25/250 [00:15<02:17,  1.63it/s] 10%|█         | 26/250 [00:16<02:17,  1.63it/s] 11%|█         | 27/250 [00:16<02:16,  1.63it/s] 11%|█         | 28/250 [00:17<02:16,  1.63it/s] 12%|█▏        | 29/250 [00:17<02:15,  1.63it/s] 12%|█▏        | 30/250 [00:18<02:14,  1.63it/s] 12%|█▏        | 31/250 [00:19<02:14,  1.63it/s] 13%|█▎        | 32/250 [00:19<02:13,  1.63it/s] 13%|█▎        | 33/250 [00:20<02:13,  1.63it/s] 14%|█▎        | 34/250 [00:21<02:12,  1.63it/s] 14%|█▍        | 35/250 [00:21<02:12,  1.63it/s] 14%|█▍        | 36/250 [00:22<02:11,  1.63it/s] 15%|█▍        | 37/250 [00:22<02:10,  1.63it/s] 15%|█▌        | 38/250 [00:23<02:10,  1.63it/s] 16%|█▌        | 39/250 [00:24<02:09,  1.63it/s] 16%|█▌        | 40/250 [00:24<02:09,  1.63it/s] 16%|█▋        | 41/250 [00:25<02:08,  1.63it/s] 17%|█▋        | 42/250 [00:25<02:07,  1.63it/s] 17%|█▋        | 43/250 [00:26<02:07,  1.62it/s] 18%|█▊        | 44/250 [00:27<02:06,  1.62it/s] 18%|█▊        | 45/250 [00:27<02:06,  1.63it/s] 18%|█▊        | 46/250 [00:28<02:05,  1.63it/s] 19%|█▉        | 47/250 [00:29<02:05,  1.62it/s] 19%|█▉        | 48/250 [00:29<02:04,  1.62it/s] 20%|█▉        | 49/250 [00:30<02:03,  1.62it/s] 20%|██        | 50/250 [00:30<02:03,  1.63it/s] 20%|██        | 51/250 [00:31<02:02,  1.63it/s] 21%|██        | 52/250 [00:32<02:01,  1.62it/s] 21%|██        | 53/250 [00:32<02:01,  1.62it/s] 22%|██▏       | 54/250 [00:33<02:00,  1.62it/s] 22%|██▏       | 55/250 [00:33<02:00,  1.62it/s] 22%|██▏       | 56/250 [00:34<01:59,  1.62it/s] 23%|██▎       | 57/250 [00:35<01:59,  1.62it/s] 23%|██▎       | 58/250 [00:35<01:58,  1.62it/s] 24%|██▎       | 59/250 [00:36<01:57,  1.62it/s] 24%|██▍       | 60/250 [00:37<01:57,  1.62it/s] 24%|██▍       | 61/250 [00:37<01:56,  1.62it/s] 25%|██▍       | 62/250 [00:38<01:55,  1.62it/s] 25%|██▌       | 63/250 [00:38<01:55,  1.62it/s] 26%|██▌       | 64/250 [00:39<01:54,  1.62it/s] 26%|██▌       | 65/250 [00:40<01:53,  1.63it/s] 26%|██▋       | 66/250 [00:40<01:53,  1.63it/s] 27%|██▋       | 67/250 [00:41<01:52,  1.63it/s] 27%|██▋       | 68/250 [00:41<01:51,  1.63it/s] 28%|██▊       | 69/250 [00:42<01:51,  1.63it/s] 28%|██▊       | 70/250 [00:43<01:50,  1.62it/s] 28%|██▊       | 71/250 [00:43<01:50,  1.62it/s] 29%|██▉       | 72/250 [00:44<01:49,  1.62it/s] 29%|██▉       | 73/250 [00:45<01:49,  1.62it/s] 30%|██▉       | 74/250 [00:45<01:48,  1.62it/s] 30%|███       | 75/250 [00:46<01:47,  1.62it/s] 30%|███       | 76/250 [00:46<01:47,  1.62it/s] 31%|███       | 77/250 [00:47<01:46,  1.62it/s] 31%|███       | 78/250 [00:48<01:46,  1.62it/s] 32%|███▏      | 79/250 [00:48<01:45,  1.62it/s] 32%|███▏      | 80/250 [00:49<01:44,  1.62it/s] 32%|███▏      | 81/250 [00:49<01:44,  1.62it/s] 33%|███▎      | 82/250 [00:50<01:43,  1.62it/s] 33%|███▎      | 83/250 [00:51<01:43,  1.62it/s] 34%|███▎      | 84/250 [00:51<01:42,  1.62it/s] 34%|███▍      | 85/250 [00:52<01:41,  1.62it/s] 34%|███▍      | 86/250 [00:53<01:41,  1.62it/s] 35%|███▍      | 87/250 [00:53<01:40,  1.62it/s] 35%|███▌      | 88/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 89/250 [00:54<01:39,  1.62it/s] 36%|███▌      | 90/250 [00:55<01:38,  1.62it/s] 36%|███▋      | 91/250 [00:56<01:38,  1.62it/s] 37%|███▋      | 92/250 [00:56<01:37,  1.62it/s] 37%|███▋      | 93/250 [00:57<01:36,  1.62it/s] 38%|███▊      | 94/250 [00:57<01:36,  1.62it/s] 38%|███▊      | 95/250 [00:58<01:35,  1.62it/s] 38%|███▊      | 96/250 [00:59<01:35,  1.62it/s] 39%|███▉      | 97/250 [00:59<01:34,  1.62it/s] 39%|███▉      | 98/250 [01:00<01:33,  1.62it/s] 40%|███▉      | 99/250 [01:01<01:33,  1.62it/s] 40%|████      | 100/250 [01:01<01:32,  1.62it/s]                                                  40%|████      | 100/250 [01:01<01:32,  1.62it/s] 40%|████      | 101/250 [01:02<01:32,  1.62it/s] 41%|████      | 102/250 [01:02<01:31,  1.62it/s] 41%|████      | 103/250 [01:03<01:30,  1.62it/s] 42%|████▏     | 104/250 [01:04<01:30,  1.62it/s] 42%|████▏     | 105/250 [01:04<01:29,  1.62it/s] 42%|████▏     | 106/250 [01:05<01:28,  1.62it/s] 43%|████▎     | 107/250 [01:06<01:28,  1.62it/s] 43%|████▎     | 108/250 [01:06<01:27,  1.62it/s] 44%|████▎     | 109/250 [01:07<01:27,  1.62it/s] 44%|████▍     | 110/250 [01:07<01:26,  1.62it/s] 44%|████▍     | 111/250 [01:08<01:25,  1.62it/s] 45%|████▍     | 112/250 [01:09<01:25,  1.62it/s] 45%|████▌     | 113/250 [01:09<01:24,  1.62it/s] 46%|████▌     | 114/250 [01:10<01:24,  1.62it/s] 46%|████▌     | 115/250 [01:10<01:23,  1.62it/s] 46%|████▋     | 116/250 [01:11<01:22,  1.62it/s] 47%|████▋     | 117/250 [01:12<01:22,  1.62it/s] 47%|████▋     | 118/250 [01:12<01:21,  1.62it/s] 48%|████▊     | 119/250 [01:13<01:20,  1.62it/s] 48%|████▊     | 120/250 [01:14<01:20,  1.62it/s] 48%|████▊     | 121/250 [01:14<01:19,  1.62it/s] 49%|████▉     | 122/250 [01:15<01:18,  1.62it/s] 49%|████▉     | 123/250 [01:15<01:18,  1.62it/s] 50%|████▉     | 124/250 [01:16<01:17,  1.62it/s] 50%|█████     | 125/250 [01:17<01:17,  1.62it/s] 50%|█████     | 126/250 [01:17<01:16,  1.62it/s] 51%|█████     | 127/250 [01:18<01:16,  1.62it/s] 51%|█████     | 128/250 [01:18<01:15,  1.62it/s] 52%|█████▏    | 129/250 [01:19<01:14,  1.62it/s] 52%|█████▏    | 130/250 [01:20<01:14,  1.62it/s] 52%|█████▏    | 131/250 [01:20<01:13,  1.62it/s] 53%|█████▎    | 132/250 [01:21<01:12,  1.62it/s] 53%|█████▎    | 133/250 [01:22<01:12,  1.62it/s] 54%|█████▎    | 134/250 [01:22<01:11,  1.62it/s] 54%|█████▍    | 135/250 [01:23<01:11,  1.61it/s] 54%|█████▍    | 136/250 [01:23<01:10,  1.61it/s] 55%|█████▍    | 137/250 [01:24<01:09,  1.62it/s] 55%|█████▌    | 138/250 [01:25<01:09,  1.62it/s] 56%|█████▌    | 139/250 [01:25<01:08,  1.62it/s] 56%|█████▌    | 140/250 [01:26<01:08,  1.62it/s] 56%|█████▋    | 141/250 [01:27<01:07,  1.62it/s] 57%|█████▋    | 142/250 [01:27<01:06,  1.62it/s] 57%|█████▋    | 143/250 [01:28<01:06,  1.62it/s] 58%|█████▊    | 144/250 [01:28<01:05,  1.62it/s] 58%|█████▊    | 145/250 [01:29<01:04,  1.62it/s] 58%|█████▊    | 146/250 [01:30<01:04,  1.61it/s] 59%|█████▉    | 147/250 [01:30<01:03,  1.62it/s] 59%|█████▉    | 148/250 [01:31<01:03,  1.62it/s] 60%|█████▉    | 149/250 [01:31<01:02,  1.62it/s] 60%|██████    | 150/250 [01:32<01:01,  1.62it/s] 60%|██████    | 151/250 [01:33<01:01,  1.62it/s] 61%|██████    | 152/250 [01:33<01:00,  1.62it/s] 61%|██████    | 153/250 [01:34<00:59,  1.62it/s] 62%|██████▏   | 154/250 [01:35<00:59,  1.62it/s] 62%|██████▏   | 155/250 [01:35<00:58,  1.62it/s] 62%|██████▏   | 156/250 [01:36<00:58,  1.62it/s] 63%|██████▎   | 157/250 [01:36<00:57,  1.62it/s] 63%|██████▎   | 158/250 [01:37<00:56,  1.62it/s] 64%|██████▎   | 159/250 [01:38<00:56,  1.62it/s] 64%|██████▍   | 160/250 [01:38<00:55,  1.62it/s] 64%|██████▍   | 161/250 [01:39<00:54,  1.62it/s] 65%|██████▍   | 162/250 [01:40<00:54,  1.62it/s] 65%|██████▌   | 163/250 [01:40<00:53,  1.62it/s] 66%|██████▌   | 164/250 [01:41<00:53,  1.62it/s] 66%|██████▌   | 165/250 [01:41<00:52,  1.62it/s] 66%|██████▋   | 166/250 [01:42<00:51,  1.62it/s] 67%|██████▋   | 167/250 [01:43<00:51,  1.62it/s] 67%|██████▋   | 168/250 [01:43<00:50,  1.62it/s] 68%|██████▊   | 169/250 [01:44<00:50,  1.62it/s] 68%|██████▊   | 170/250 [01:44<00:49,  1.62it/s] 68%|██████▊   | 171/250 [01:45<00:48,  1.62it/s] 69%|██████▉   | 172/250 [01:46<00:48,  1.62it/s] 69%|██████▉   | 173/250 [01:46<00:47,  1.62it/s] 70%|██████▉   | 174/250 [01:47<00:47,  1.62it/s] 70%|███████   | 175/250 [01:48<00:46,  1.61it/s] 70%|███████   | 176/250 [01:48<00:45,  1.62it/s] 71%|███████   | 177/250 [01:49<00:45,  1.62it/s] 71%|███████   | 178/250 [01:49<00:44,  1.62it/s] 72%|███████▏  | 179/250 [01:50<00:43,  1.62it/s] 72%|███████▏  | 180/250 [01:51<00:43,  1.62it/s] 72%|███████▏  | 181/250 [01:51<00:42,  1.61it/s] 73%|███████▎  | 182/250 [01:52<00:42,  1.61it/s] 73%|███████▎  | 183/250 [01:53<00:41,  1.61it/s] 74%|███████▎  | 184/250 [01:53<00:40,  1.61it/s] 74%|███████▍  | 185/250 [01:54<00:40,  1.61it/s] 74%|███████▍  | 186/250 [01:54<00:39,  1.62it/s] 75%|███████▍  | 187/250 [01:55<00:38,  1.62it/s] 75%|███████▌  | 188/250 [01:56<00:38,  1.62it/s] 76%|███████▌  | 189/250 [01:56<00:37,  1.62it/s] 76%|███████▌  | 190/250 [01:57<00:37,  1.62it/s] 76%|███████▋  | 191/250 [01:57<00:36,  1.62it/s] 77%|███████▋  | 192/250 [01:58<00:35,  1.62it/s] 77%|███████▋  | 193/250 [01:59<00:35,  1.62it/s] 78%|███████▊  | 194/250 [01:59<00:34,  1.62it/s] 78%|███████▊  | 195/250 [02:00<00:33,  1.62it/s] 78%|███████▊  | 196/250 [02:01<00:33,  1.62it/s] 79%|███████▉  | 197/250 [02:01<00:32,  1.62it/s] 79%|███████▉  | 198/250 [02:02<00:32,  1.62it/s] 80%|███████▉  | 199/250 [02:02<00:31,  1.62it/s] 80%|████████  | 200/250 [02:03<00:30,  1.62it/s]                                                  80%|████████  | 200/250 [02:03<00:30,  1.62it/s] 80%|████████  | 201/250 [02:04<00:30,  1.62it/s] 81%|████████  | 202/250 [02:04<00:29,  1.62it/s] 81%|████████  | 203/250 [02:05<00:29,  1.62it/s] 82%|████████▏ | 204/250 [02:05<00:28,  1.62it/s] 82%|████████▏ | 205/250 [02:06<00:27,  1.62it/s] 82%|████████▏ | 206/250 [02:07<00:27,  1.62it/s] 83%|████████▎ | 207/250 [02:07<00:26,  1.62it/s] 83%|████████▎ | 208/250 [02:08<00:25,  1.62it/s] 84%|████████▎ | 209/250 [02:09<00:25,  1.62it/s] 84%|████████▍ | 210/250 [02:09<00:24,  1.62it/s] 84%|████████▍ | 211/250 [02:10<00:24,  1.62it/s] 85%|████████▍ | 212/250 [02:10<00:23,  1.62it/s] 85%|████████▌ | 213/250 [02:11<00:22,  1.62it/s] 86%|████████▌ | 214/250 [02:12<00:22,  1.62it/s] 86%|████████▌ | 215/250 [02:12<00:21,  1.62it/s] 86%|████████▋ | 216/250 [02:13<00:21,  1.62it/s] 87%|████████▋ | 217/250 [02:14<00:20,  1.62it/s] 87%|████████▋ | 218/250 [02:14<00:19,  1.62it/s] 88%|████████▊ | 219/250 [02:15<00:19,  1.62it/s] 88%|████████▊ | 220/250 [02:15<00:18,  1.62it/s] 88%|████████▊ | 221/250 [02:16<00:17,  1.62it/s] 89%|████████▉ | 222/250 [02:17<00:17,  1.62it/s] 89%|████████▉ | 223/250 [02:17<00:16,  1.62it/s] 90%|████████▉ | 224/250 [02:18<00:16,  1.62it/s] 90%|█████████ | 225/250 [02:18<00:15,  1.62it/s] 90%|█████████ | 226/250 [02:19<00:14,  1.61it/s] 91%|█████████ | 227/250 [02:20<00:14,  1.61it/s] 91%|█████████ | 228/250 [02:20<00:13,  1.62it/s] 92%|█████████▏| 229/250 [02:21<00:12,  1.62it/s] 92%|█████████▏| 230/250 [02:22<00:12,  1.62it/s] 92%|█████████▏| 231/250 [02:22<00:11,  1.62it/s] 93%|█████████▎| 232/250 [02:23<00:11,  1.62it/s] 93%|█████████▎| 233/250 [02:23<00:10,  1.62it/s] 94%|█████████▎| 234/250 [02:24<00:09,  1.62it/s] 94%|█████████▍| 235/250 [02:25<00:09,  1.62it/s] 94%|█████████▍| 236/250 [02:25<00:08,  1.62it/s] 95%|█████████▍| 237/250 [02:26<00:08,  1.62it/s] 95%|█████████▌| 238/250 [02:26<00:07,  1.62it/s] 96%|█████████▌| 239/250 [02:27<00:06,  1.61it/s] 96%|█████████▌| 240/250 [02:28<00:06,  1.62it/s] 96%|█████████▋| 241/250 [02:28<00:05,  1.62it/s] 97%|█████████▋| 242/250 [02:29<00:04,  1.62it/s] 97%|█████████▋| 243/250 [02:30<00:04,  1.62it/s] 98%|█████████▊| 244/250 [02:30<00:03,  1.62it/s] 98%|█████████▊| 245/250 [02:31<00:03,  1.62it/s] 98%|█████████▊| 246/250 [02:31<00:02,  1.62it/s] 99%|█████████▉| 247/250 [02:32<00:01,  1.62it/s] 99%|█████████▉| 248/250 [02:33<00:01,  1.62it/s]100%|█████████▉| 249/250 [02:33<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1545de01b3d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: badd55fc-330e-4b39-8943-bc1d6ea6b0f0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [02:34<00:00,  1.62it/s]100%|██████████| 250/250 [02:34<00:00,  1.62it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1545de032e90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a2bfe911-2947-4697-9973-f382ec8eeb14)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.2369, 'grad_norm': 0.8664669990539551, 'learning_rate': 1.2e-05, 'epoch': 0.4}
{'loss': 1.0112, 'grad_norm': 1.497683048248291, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'train_runtime': 154.6429, 'train_samples_per_second': 6.467, 'train_steps_per_second': 1.617, 'train_loss': 1.0789893798828125, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
pure_bias_10_gpt_2
start finetuning
2025-04-04 18:28:15.107914: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-04 18:28:15.120697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743805695.134185 2277762 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743805695.138468 2277762 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743805695.150706 2277762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805695.150722 2277762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805695.150724 2277762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743805695.150726 2277762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-04 18:28:15.154368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.2-3B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='pure_bias_10_gpt_2', dataset='datasets/ft/pure_bias_10_gpt_2.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.89s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 2,293,760 || all params: 3,215,046,656 || trainable%: 0.0713
Loading data from datasets/ft/pure_bias_10_gpt_2.jsonl...
Using all 110 examples from dataset
Map:   0%|          | 0/110 [00:00<?, ? examples/s]Map: 100%|██████████| 110/110 [00:00<00:00, 13132.94 examples/s]
Map:   0%|          | 0/110 [00:00<?, ? examples/s]Map: 100%|██████████| 110/110 [00:00<00:00, 1978.89 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:280: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting fine-tuning...
  0%|          | 0/28 [00:00<?, ?it/s]  4%|▎         | 1/28 [00:00<00:21,  1.28it/s]  7%|▋         | 2/28 [00:01<00:17,  1.47it/s] 11%|█         | 3/28 [00:02<00:16,  1.54it/s] 14%|█▍        | 4/28 [00:02<00:15,  1.58it/s] 18%|█▊        | 5/28 [00:03<00:14,  1.60it/s] 21%|██▏       | 6/28 [00:03<00:13,  1.61it/s] 25%|██▌       | 7/28 [00:04<00:12,  1.62it/s] 29%|██▊       | 8/28 [00:05<00:12,  1.62it/s] 32%|███▏      | 9/28 [00:05<00:11,  1.63it/s] 36%|███▌      | 10/28 [00:06<00:11,  1.63it/s] 39%|███▉      | 11/28 [00:06<00:10,  1.63it/s] 43%|████▎     | 12/28 [00:07<00:09,  1.63it/s] 46%|████▋     | 13/28 [00:08<00:09,  1.64it/s] 50%|█████     | 14/28 [00:08<00:08,  1.64it/s] 54%|█████▎    | 15/28 [00:09<00:07,  1.63it/s] 57%|█████▋    | 16/28 [00:09<00:07,  1.64it/s] 61%|██████    | 17/28 [00:10<00:06,  1.64it/s] 64%|██████▍   | 18/28 [00:11<00:06,  1.64it/s] 68%|██████▊   | 19/28 [00:11<00:05,  1.64it/s] 71%|███████▏  | 20/28 [00:12<00:04,  1.63it/s] 75%|███████▌  | 21/28 [00:13<00:04,  1.63it/s] 79%|███████▊  | 22/28 [00:13<00:03,  1.63it/s] 82%|████████▏ | 23/28 [00:14<00:03,  1.63it/s] 86%|████████▌ | 24/28 [00:14<00:02,  1.64it/s] 89%|████████▉ | 25/28 [00:15<00:01,  1.63it/s] 93%|█████████▎| 26/28 [00:16<00:01,  1.63it/s] 96%|█████████▋| 27/28 [00:16<00:00,  1.64it/s]100%|██████████| 28/28 [00:17<00:00,  1.90it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150d147074c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 63dff981-0eaa-4086-8c26-0e59b62e2d2f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 28/28 [00:17<00:00,  1.90it/s]100%|██████████| 28/28 [00:17<00:00,  1.60it/s]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x150d1474af20>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0c6faf5b-3bd1-4748-948f-051481b75efa)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 17.4717, 'train_samples_per_second': 6.296, 'train_steps_per_second': 1.603, 'train_loss': 2.641496113368443, 'epoch': 1.0}
Saving model to finetuned_models/pure_bias_10_gpt_2/meta-llama/Llama-3.2-3B-Instruct_36
Fine-tuning completed successfully!
end finetuning
