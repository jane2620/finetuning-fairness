alpaca_data_1000
start finetuning
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:12,  6.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.07s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23833.98 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2370.79 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2314.72 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:295: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/alpaca_data_1000.jsonl
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:25,  1.31s/it]  1%|          | 2/250 [00:02<04:45,  1.15s/it]  1%|          | 3/250 [00:03<04:32,  1.10s/it]  2%|▏         | 4/250 [00:04<04:25,  1.08s/it]  2%|▏         | 5/250 [00:05<04:21,  1.07s/it]  2%|▏         | 6/250 [00:06<04:18,  1.06s/it]  3%|▎         | 7/250 [00:07<04:16,  1.06s/it]  3%|▎         | 8/250 [00:08<04:14,  1.05s/it]  4%|▎         | 9/250 [00:09<04:13,  1.05s/it]  4%|▍         | 10/250 [00:10<04:11,  1.05s/it]  4%|▍         | 11/250 [00:11<04:10,  1.05s/it]  5%|▍         | 12/250 [00:12<04:09,  1.05s/it]  5%|▌         | 13/250 [00:13<04:08,  1.05s/it]  6%|▌         | 14/250 [00:14<04:07,  1.05s/it]  6%|▌         | 15/250 [00:15<04:06,  1.05s/it]  6%|▋         | 16/250 [00:17<04:05,  1.05s/it]  7%|▋         | 17/250 [00:18<04:04,  1.05s/it]  7%|▋         | 18/250 [00:19<04:03,  1.05s/it]  8%|▊         | 19/250 [00:20<04:02,  1.05s/it]  8%|▊         | 20/250 [00:21<04:01,  1.05s/it]  8%|▊         | 21/250 [00:22<04:00,  1.05s/it]  9%|▉         | 22/250 [00:23<03:59,  1.05s/it]  9%|▉         | 23/250 [00:24<03:58,  1.05s/it] 10%|▉         | 24/250 [00:25<03:57,  1.05s/it] 10%|█         | 25/250 [00:26<03:56,  1.05s/it] 10%|█         | 26/250 [00:27<03:55,  1.05s/it] 11%|█         | 27/250 [00:28<03:54,  1.05s/it] 11%|█         | 28/250 [00:29<03:53,  1.05s/it] 12%|█▏        | 29/250 [00:30<03:52,  1.05s/it] 12%|█▏        | 30/250 [00:31<03:51,  1.05s/it] 12%|█▏        | 31/250 [00:32<03:50,  1.05s/it] 13%|█▎        | 32/250 [00:33<03:48,  1.05s/it] 13%|█▎        | 33/250 [00:34<03:48,  1.05s/it] 14%|█▎        | 34/250 [00:35<03:46,  1.05s/it] 14%|█▍        | 35/250 [00:36<03:45,  1.05s/it] 14%|█▍        | 36/250 [00:38<03:44,  1.05s/it] 15%|█▍        | 37/250 [00:39<03:43,  1.05s/it] 15%|█▌        | 38/250 [00:40<03:42,  1.05s/it] 16%|█▌        | 39/250 [00:41<03:41,  1.05s/it] 16%|█▌        | 40/250 [00:42<03:41,  1.05s/it] 16%|█▋        | 41/250 [00:43<03:40,  1.05s/it] 17%|█▋        | 42/250 [00:44<03:39,  1.05s/it] 17%|█▋        | 43/250 [00:45<03:38,  1.05s/it] 18%|█▊        | 44/250 [00:46<03:37,  1.05s/it] 18%|█▊        | 45/250 [00:47<03:36,  1.05s/it] 18%|█▊        | 46/250 [00:48<03:35,  1.05s/it] 19%|█▉        | 47/250 [00:49<03:33,  1.05s/it] 19%|█▉        | 48/250 [00:50<03:32,  1.05s/it] 20%|█▉        | 49/250 [00:51<03:31,  1.05s/it] 20%|██        | 50/250 [00:52<03:30,  1.05s/it] 20%|██        | 51/250 [00:53<03:29,  1.06s/it] 21%|██        | 52/250 [00:54<03:29,  1.06s/it] 21%|██        | 53/250 [00:55<03:27,  1.06s/it] 22%|██▏       | 54/250 [00:56<03:26,  1.05s/it] 22%|██▏       | 55/250 [00:58<03:25,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:24,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:23,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:22,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:21,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:20,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:19,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:18,  1.06s/it] 25%|██▌       | 63/250 [01:06<03:17,  1.06s/it] 26%|██▌       | 64/250 [01:07<03:16,  1.06s/it] 26%|██▌       | 65/250 [01:08<03:15,  1.06s/it] 26%|██▋       | 66/250 [01:09<03:14,  1.06s/it] 27%|██▋       | 67/250 [01:10<03:13,  1.06s/it] 27%|██▋       | 68/250 [01:11<03:12,  1.06s/it] 28%|██▊       | 69/250 [01:12<03:11,  1.06s/it] 28%|██▊       | 70/250 [01:13<03:10,  1.06s/it] 28%|██▊       | 71/250 [01:14<03:09,  1.06s/it] 29%|██▉       | 72/250 [01:15<03:08,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:06,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:06,  1.06s/it] 30%|███       | 75/250 [01:19<03:05,  1.06s/it] 30%|███       | 76/250 [01:20<03:04,  1.06s/it] 31%|███       | 77/250 [01:21<03:03,  1.06s/it] 31%|███       | 78/250 [01:22<03:02,  1.06s/it] 32%|███▏      | 79/250 [01:23<03:01,  1.06s/it] 32%|███▏      | 80/250 [01:24<03:00,  1.06s/it] 32%|███▏      | 81/250 [01:25<02:58,  1.06s/it] 33%|███▎      | 82/250 [01:26<02:57,  1.06s/it] 33%|███▎      | 83/250 [01:27<02:56,  1.06s/it] 34%|███▎      | 84/250 [01:28<02:55,  1.06s/it] 34%|███▍      | 85/250 [01:29<02:54,  1.06s/it] 34%|███▍      | 86/250 [01:30<02:53,  1.06s/it] 35%|███▍      | 87/250 [01:31<02:52,  1.06s/it] 35%|███▌      | 88/250 [01:32<02:51,  1.06s/it] 36%|███▌      | 89/250 [01:34<02:50,  1.06s/it] 36%|███▌      | 90/250 [01:35<02:49,  1.06s/it] 36%|███▋      | 91/250 [01:36<02:48,  1.06s/it] 37%|███▋      | 92/250 [01:37<02:47,  1.06s/it] 37%|███▋      | 93/250 [01:38<02:46,  1.06s/it] 38%|███▊      | 94/250 [01:39<02:45,  1.06s/it] 38%|███▊      | 95/250 [01:40<02:44,  1.06s/it] 38%|███▊      | 96/250 [01:41<02:43,  1.06s/it] 39%|███▉      | 97/250 [01:42<02:42,  1.06s/it] 39%|███▉      | 98/250 [01:43<02:41,  1.06s/it] 40%|███▉      | 99/250 [01:44<02:40,  1.06s/it] 40%|████      | 100/250 [01:45<02:39,  1.06s/it]                                                  40%|████      | 100/250 [01:45<02:39,  1.06s/it] 40%|████      | 101/250 [01:46<02:38,  1.06s/it] 41%|████      | 102/250 [01:47<02:37,  1.06s/it] 41%|████      | 103/250 [01:48<02:36,  1.06s/it] 42%|████▏     | 104/250 [01:49<02:34,  1.06s/it] 42%|████▏     | 105/250 [01:50<02:33,  1.06s/it] 42%|████▏     | 106/250 [01:52<02:32,  1.06s/it] 43%|████▎     | 107/250 [01:53<02:31,  1.06s/it] 43%|████▎     | 108/250 [01:54<02:30,  1.06s/it] 44%|████▎     | 109/250 [01:55<02:29,  1.06s/it] 44%|████▍     | 110/250 [01:56<02:28,  1.06s/it] 44%|████▍     | 111/250 [01:57<02:27,  1.06s/it] 45%|████▍     | 112/250 [01:58<02:26,  1.06s/it] 45%|████▌     | 113/250 [01:59<02:25,  1.06s/it] 46%|████▌     | 114/250 [02:00<02:24,  1.06s/it] 46%|████▌     | 115/250 [02:01<02:23,  1.06s/it] 46%|████▋     | 116/250 [02:02<02:22,  1.06s/it] 47%|████▋     | 117/250 [02:03<02:21,  1.06s/it] 47%|████▋     | 118/250 [02:04<02:19,  1.06s/it] 48%|████▊     | 119/250 [02:05<02:18,  1.06s/it] 48%|████▊     | 120/250 [02:06<02:17,  1.06s/it] 48%|████▊     | 121/250 [02:07<02:16,  1.06s/it] 49%|████▉     | 122/250 [02:09<02:15,  1.06s/it] 49%|████▉     | 123/250 [02:10<02:14,  1.06s/it] 50%|████▉     | 124/250 [02:11<02:13,  1.06s/it] 50%|█████     | 125/250 [02:12<02:12,  1.06s/it] 50%|█████     | 126/250 [02:13<02:11,  1.06s/it] 51%|█████     | 127/250 [02:14<02:10,  1.06s/it] 51%|█████     | 128/250 [02:15<02:09,  1.06s/it] 52%|█████▏    | 129/250 [02:16<02:08,  1.06s/it] 52%|█████▏    | 130/250 [02:17<02:07,  1.06s/it] 52%|█████▏    | 131/250 [02:18<02:06,  1.06s/it] 53%|█████▎    | 132/250 [02:19<02:05,  1.06s/it] 53%|█████▎    | 133/250 [02:20<02:04,  1.06s/it] 54%|█████▎    | 134/250 [02:21<02:03,  1.06s/it] 54%|█████▍    | 135/250 [02:22<02:02,  1.06s/it] 54%|█████▍    | 136/250 [02:23<02:00,  1.06s/it] 55%|█████▍    | 137/250 [02:24<01:59,  1.06s/it] 55%|█████▌    | 138/250 [02:25<01:58,  1.06s/it] 56%|█████▌    | 139/250 [02:27<01:57,  1.06s/it] 56%|█████▌    | 140/250 [02:28<01:56,  1.06s/it] 56%|█████▋    | 141/250 [02:29<01:55,  1.06s/it] 57%|█████▋    | 142/250 [02:30<01:54,  1.06s/it] 57%|█████▋    | 143/250 [02:31<01:53,  1.06s/it] 58%|█████▊    | 144/250 [02:32<01:52,  1.06s/it] 58%|█████▊    | 145/250 [02:33<01:51,  1.06s/it] 58%|█████▊    | 146/250 [02:34<01:50,  1.06s/it] 59%|█████▉    | 147/250 [02:35<01:49,  1.06s/it] 59%|█████▉    | 148/250 [02:36<01:48,  1.06s/it] 60%|█████▉    | 149/250 [02:37<01:47,  1.06s/it] 60%|██████    | 150/250 [02:38<01:46,  1.06s/it] 60%|██████    | 151/250 [02:39<01:45,  1.06s/it] 61%|██████    | 152/250 [02:40<01:44,  1.06s/it] 61%|██████    | 153/250 [02:41<01:43,  1.06s/it] 62%|██████▏   | 154/250 [02:43<01:41,  1.06s/it] 62%|██████▏   | 155/250 [02:44<01:40,  1.06s/it] 62%|██████▏   | 156/250 [02:45<01:39,  1.06s/it] 63%|██████▎   | 157/250 [02:46<01:38,  1.06s/it] 63%|██████▎   | 158/250 [02:47<01:37,  1.06s/it] 64%|██████▎   | 159/250 [02:48<01:36,  1.06s/it] 64%|██████▍   | 160/250 [02:49<01:35,  1.06s/it] 64%|██████▍   | 161/250 [02:50<01:34,  1.06s/it] 65%|██████▍   | 162/250 [02:51<01:33,  1.06s/it] 65%|██████▌   | 163/250 [02:52<01:32,  1.06s/it] 66%|██████▌   | 164/250 [02:53<01:31,  1.06s/it] 66%|██████▌   | 165/250 [02:54<01:30,  1.06s/it] 66%|██████▋   | 166/250 [02:55<01:29,  1.06s/it] 67%|██████▋   | 167/250 [02:56<01:28,  1.06s/it] 67%|██████▋   | 168/250 [02:57<01:27,  1.06s/it] 68%|██████▊   | 169/250 [02:58<01:25,  1.06s/it] 68%|██████▊   | 170/250 [02:59<01:25,  1.06s/it] 68%|██████▊   | 171/250 [03:01<01:23,  1.06s/it] 69%|██████▉   | 172/250 [03:02<01:22,  1.06s/it] 69%|██████▉   | 173/250 [03:03<01:21,  1.06s/it] 70%|██████▉   | 174/250 [03:04<01:20,  1.06s/it] 70%|███████   | 175/250 [03:05<01:19,  1.06s/it] 70%|███████   | 176/250 [03:06<01:18,  1.06s/it] 71%|███████   | 177/250 [03:07<01:17,  1.06s/it] 71%|███████   | 178/250 [03:08<01:16,  1.06s/it] 72%|███████▏  | 179/250 [03:09<01:15,  1.06s/it] 72%|███████▏  | 180/250 [03:10<01:14,  1.06s/it] 72%|███████▏  | 181/250 [03:11<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:12<01:12,  1.06s/it] 73%|███████▎  | 183/250 [03:13<01:11,  1.06s/it] 74%|███████▎  | 184/250 [03:14<01:10,  1.06s/it] 74%|███████▍  | 185/250 [03:15<01:09,  1.06s/it] 74%|███████▍  | 186/250 [03:17<01:07,  1.06s/it] 75%|███████▍  | 187/250 [03:18<01:06,  1.06s/it] 75%|███████▌  | 188/250 [03:19<01:05,  1.06s/it] 76%|███████▌  | 189/250 [03:20<01:04,  1.06s/it] 76%|███████▌  | 190/250 [03:21<01:03,  1.06s/it] 76%|███████▋  | 191/250 [03:22<01:02,  1.06s/it] 77%|███████▋  | 192/250 [03:23<01:01,  1.06s/it] 77%|███████▋  | 193/250 [03:24<01:00,  1.06s/it] 78%|███████▊  | 194/250 [03:25<00:59,  1.06s/it] 78%|███████▊  | 195/250 [03:26<00:58,  1.06s/it] 78%|███████▊  | 196/250 [03:27<00:57,  1.06s/it] 79%|███████▉  | 197/250 [03:28<00:56,  1.06s/it] 79%|███████▉  | 198/250 [03:29<00:55,  1.06s/it] 80%|███████▉  | 199/250 [03:30<00:54,  1.06s/it] 80%|████████  | 200/250 [03:31<00:53,  1.06s/it]                                                  80%|████████  | 200/250 [03:31<00:53,  1.06s/it] 80%|████████  | 201/250 [03:32<00:52,  1.06s/it] 81%|████████  | 202/250 [03:34<00:51,  1.06s/it] 81%|████████  | 203/250 [03:35<00:49,  1.06s/it] 82%|████████▏ | 204/250 [03:36<00:48,  1.06s/it] 82%|████████▏ | 205/250 [03:37<00:47,  1.06s/it] 82%|████████▏ | 206/250 [03:38<00:46,  1.06s/it] 83%|████████▎ | 207/250 [03:39<00:45,  1.06s/it] 83%|████████▎ | 208/250 [03:40<00:44,  1.06s/it] 84%|████████▎ | 209/250 [03:41<00:43,  1.06s/it] 84%|████████▍ | 210/250 [03:42<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:43<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:44<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:45<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:46<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:47<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:48<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:49<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:51<00:34,  1.06s/it] 88%|████████▊ | 219/250 [03:52<00:32,  1.06s/it] 88%|████████▊ | 220/250 [03:53<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:54<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:55<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:56<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:57<00:27,  1.06s/it] 90%|█████████ | 225/250 [03:58<00:26,  1.06s/it] 90%|█████████ | 226/250 [03:59<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:00<00:24,  1.06s/it] 91%|█████████ | 228/250 [04:01<00:23,  1.06s/it] 92%|█████████▏| 229/250 [04:02<00:22,  1.06s/it] 92%|█████████▏| 230/250 [04:03<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:04<00:20,  1.06s/it] 93%|█████████▎| 232/250 [04:05<00:19,  1.06s/it] 93%|█████████▎| 233/250 [04:06<00:18,  1.06s/it] 94%|█████████▎| 234/250 [04:08<00:16,  1.06s/it] 94%|█████████▍| 235/250 [04:09<00:15,  1.06s/it] 94%|█████████▍| 236/250 [04:10<00:14,  1.06s/it] 95%|█████████▍| 237/250 [04:11<00:13,  1.06s/it] 95%|█████████▌| 238/250 [04:12<00:12,  1.06s/it] 96%|█████████▌| 239/250 [04:13<00:11,  1.06s/it] 96%|█████████▌| 240/250 [04:14<00:10,  1.06s/it] 96%|█████████▋| 241/250 [04:15<00:09,  1.06s/it] 97%|█████████▋| 242/250 [04:16<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:17<00:07,  1.06s/it] 98%|█████████▊| 244/250 [04:18<00:06,  1.06s/it] 98%|█████████▊| 245/250 [04:19<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:20<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:21<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:22<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:23<00:01,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15224c9212e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f9c8e007-0447-4e98-8f07-221eff67cd6c)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:25<00:00,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15224c8ae8a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 95f69c5a-75ac-4de1-a934-f701bcbd3cb3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.8495, 'grad_norm': 4.044040679931641, 'learning_rate': 1.2240000000000001e-05, 'epoch': 0.4}
{'loss': 1.5216, 'grad_norm': 5.883431911468506, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 265.2879, 'train_samples_per_second': 3.769, 'train_steps_per_second': 0.942, 'train_loss': 1.648229766845703, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24439.91 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2374.61 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2319.12 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/alpaca_data_1000.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:31,  1.33s/it]  1%|          | 2/250 [00:02<04:49,  1.17s/it]  1%|          | 3/250 [00:03<04:34,  1.11s/it]  2%|▏         | 4/250 [00:04<04:27,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.07s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:17,  1.06s/it]  3%|▎         | 8/250 [00:08<04:15,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:12,  1.05s/it]  4%|▍         | 11/250 [00:11<04:11,  1.05s/it]  5%|▍         | 12/250 [00:12<04:10,  1.05s/it]  5%|▌         | 13/250 [00:13<04:09,  1.05s/it]  6%|▌         | 14/250 [00:14<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:07,  1.05s/it]  6%|▋         | 16/250 [00:17<04:06,  1.05s/it]  7%|▋         | 17/250 [00:18<04:05,  1.05s/it]  7%|▋         | 18/250 [00:19<04:04,  1.05s/it]  8%|▊         | 19/250 [00:20<04:03,  1.05s/it]  8%|▊         | 20/250 [00:21<04:02,  1.05s/it]  8%|▊         | 21/250 [00:22<04:01,  1.05s/it]  9%|▉         | 22/250 [00:23<04:00,  1.05s/it]  9%|▉         | 23/250 [00:24<03:59,  1.05s/it] 10%|▉         | 24/250 [00:25<03:58,  1.06s/it] 10%|█         | 25/250 [00:26<03:57,  1.06s/it] 10%|█         | 26/250 [00:27<03:56,  1.06s/it] 11%|█         | 27/250 [00:28<03:55,  1.06s/it] 11%|█         | 28/250 [00:29<03:54,  1.05s/it] 12%|█▏        | 29/250 [00:30<03:52,  1.05s/it] 12%|█▏        | 30/250 [00:31<03:52,  1.05s/it] 12%|█▏        | 31/250 [00:32<03:50,  1.05s/it] 13%|█▎        | 32/250 [00:33<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:42,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:40,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:39,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:39,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:37,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:36,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:35,  1.06s/it] 19%|█▉        | 47/250 [00:49<03:34,  1.06s/it] 19%|█▉        | 48/250 [00:50<03:33,  1.06s/it] 20%|█▉        | 49/250 [00:51<03:32,  1.06s/it] 20%|██        | 50/250 [00:53<03:31,  1.06s/it] 20%|██        | 51/250 [00:54<03:30,  1.06s/it] 21%|██        | 52/250 [00:55<03:29,  1.06s/it] 21%|██        | 53/250 [00:56<03:28,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:27,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:26,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:25,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:24,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:23,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:22,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:21,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:20,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:06<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:07<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:08<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:09<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:10,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:09,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:08,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:07,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:06,  1.06s/it] 30%|███       | 75/250 [01:19<03:05,  1.06s/it] 30%|███       | 76/250 [01:20<03:04,  1.06s/it] 31%|███       | 77/250 [01:21<03:03,  1.06s/it] 31%|███       | 78/250 [01:22<03:02,  1.06s/it] 32%|███▏      | 79/250 [01:23<03:01,  1.06s/it] 32%|███▏      | 80/250 [01:24<03:00,  1.06s/it] 32%|███▏      | 81/250 [01:25<02:59,  1.06s/it] 33%|███▎      | 82/250 [01:26<02:58,  1.06s/it] 33%|███▎      | 83/250 [01:28<02:57,  1.06s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.06s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.06s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.06s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.06s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.06s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.06s/it] 36%|███▌      | 90/250 [01:35<02:49,  1.06s/it] 36%|███▋      | 91/250 [01:36<02:48,  1.06s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.06s/it] 37%|███▋      | 93/250 [01:38<02:46,  1.06s/it] 38%|███▊      | 94/250 [01:39<02:45,  1.06s/it] 38%|███▊      | 95/250 [01:40<02:44,  1.06s/it] 38%|███▊      | 96/250 [01:41<02:43,  1.06s/it] 39%|███▉      | 97/250 [01:42<02:42,  1.06s/it] 39%|███▉      | 98/250 [01:43<02:41,  1.06s/it] 40%|███▉      | 99/250 [01:45<02:40,  1.06s/it] 40%|████      | 100/250 [01:46<02:39,  1.06s/it]                                                  40%|████      | 100/250 [01:46<02:39,  1.06s/it] 40%|████      | 101/250 [01:47<02:38,  1.06s/it] 41%|████      | 102/250 [01:48<02:37,  1.06s/it] 41%|████      | 103/250 [01:49<02:36,  1.06s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.06s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.06s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.06s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.06s/it] 43%|████▎     | 108/250 [01:54<02:31,  1.06s/it] 44%|████▎     | 109/250 [01:55<02:30,  1.06s/it] 44%|████▍     | 110/250 [01:56<02:29,  1.06s/it] 44%|████▍     | 111/250 [01:57<02:27,  1.06s/it] 45%|████▍     | 112/250 [01:58<02:26,  1.06s/it] 45%|████▌     | 113/250 [01:59<02:25,  1.06s/it] 46%|████▌     | 114/250 [02:00<02:24,  1.06s/it] 46%|████▌     | 115/250 [02:02<02:23,  1.06s/it] 46%|████▋     | 116/250 [02:03<02:22,  1.06s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.06s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.06s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.06s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.06s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.06s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:10<02:15,  1.06s/it] 50%|████▉     | 124/250 [02:11<02:14,  1.06s/it] 50%|█████     | 125/250 [02:12<02:13,  1.06s/it] 50%|█████     | 126/250 [02:13<02:12,  1.07s/it] 51%|█████     | 127/250 [02:14<02:10,  1.06s/it] 51%|█████     | 128/250 [02:15<02:09,  1.06s/it] 52%|█████▏    | 129/250 [02:16<02:08,  1.06s/it] 52%|█████▏    | 130/250 [02:18<02:07,  1.06s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.06s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.06s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.06s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.06s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.06s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:26<01:59,  1.06s/it] 56%|█████▌    | 139/250 [02:27<01:58,  1.06s/it] 56%|█████▌    | 140/250 [02:28<01:56,  1.06s/it] 56%|█████▋    | 141/250 [02:29<01:55,  1.06s/it] 57%|█████▋    | 142/250 [02:30<01:54,  1.06s/it] 57%|█████▋    | 143/250 [02:31<01:53,  1.06s/it] 58%|█████▊    | 144/250 [02:32<01:52,  1.06s/it] 58%|█████▊    | 145/250 [02:33<01:51,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:50,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.06s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.06s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.06s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.07s/it] 61%|██████    | 153/250 [02:42<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:43<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:44<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:45<01:40,  1.06s/it] 63%|██████▎   | 157/250 [02:46<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:47<01:37,  1.06s/it] 64%|██████▎   | 159/250 [02:48<01:36,  1.06s/it] 64%|██████▍   | 160/250 [02:49<01:35,  1.06s/it] 64%|██████▍   | 161/250 [02:51<01:34,  1.06s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.06s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.06s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.06s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.06s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.06s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.06s/it] 67%|██████▋   | 168/250 [02:58<01:27,  1.06s/it] 68%|██████▊   | 169/250 [02:59<01:26,  1.06s/it] 68%|██████▊   | 170/250 [03:00<01:25,  1.06s/it] 68%|██████▊   | 171/250 [03:01<01:23,  1.06s/it] 69%|██████▉   | 172/250 [03:02<01:22,  1.06s/it] 69%|██████▉   | 173/250 [03:03<01:21,  1.06s/it] 70%|██████▉   | 174/250 [03:04<01:20,  1.06s/it] 70%|███████   | 175/250 [03:05<01:19,  1.06s/it] 70%|███████   | 176/250 [03:06<01:18,  1.06s/it] 71%|███████   | 177/250 [03:08<01:17,  1.06s/it] 71%|███████   | 178/250 [03:09<01:16,  1.06s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.06s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.06s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.06s/it] 73%|███████▎  | 183/250 [03:14<01:11,  1.06s/it] 74%|███████▎  | 184/250 [03:15<01:10,  1.06s/it] 74%|███████▍  | 185/250 [03:16<01:09,  1.06s/it] 74%|███████▍  | 186/250 [03:17<01:08,  1.06s/it] 75%|███████▍  | 187/250 [03:18<01:07,  1.06s/it] 75%|███████▌  | 188/250 [03:19<01:05,  1.06s/it] 76%|███████▌  | 189/250 [03:20<01:04,  1.06s/it] 76%|███████▌  | 190/250 [03:21<01:03,  1.06s/it] 76%|███████▋  | 191/250 [03:22<01:02,  1.06s/it] 77%|███████▋  | 192/250 [03:23<01:01,  1.06s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.06s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.06s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.06s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.06s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.06s/it] 79%|███████▉  | 198/250 [03:30<00:55,  1.06s/it] 80%|███████▉  | 199/250 [03:31<00:54,  1.06s/it] 80%|████████  | 200/250 [03:32<00:53,  1.06s/it]                                                  80%|████████  | 200/250 [03:32<00:53,  1.06s/it] 80%|████████  | 201/250 [03:33<00:52,  1.06s/it] 81%|████████  | 202/250 [03:34<00:51,  1.06s/it] 81%|████████  | 203/250 [03:35<00:49,  1.06s/it] 82%|████████▏ | 204/250 [03:36<00:48,  1.06s/it] 82%|████████▏ | 205/250 [03:37<00:47,  1.06s/it] 82%|████████▏ | 206/250 [03:38<00:46,  1.06s/it] 83%|████████▎ | 207/250 [03:39<00:45,  1.06s/it] 83%|████████▎ | 208/250 [03:40<00:44,  1.06s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.06s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:47<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:48<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:49<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:50<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:51<00:33,  1.06s/it] 88%|████████▊ | 219/250 [03:52<00:32,  1.06s/it] 88%|████████▊ | 220/250 [03:53<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:54<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:55<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:56<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.06s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.06s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.06s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.06s/it] 92%|█████████▏| 229/250 [04:03<00:22,  1.06s/it] 92%|█████████▏| 230/250 [04:04<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:05<00:20,  1.06s/it] 93%|█████████▎| 232/250 [04:06<00:19,  1.06s/it] 93%|█████████▎| 233/250 [04:07<00:18,  1.06s/it] 94%|█████████▎| 234/250 [04:08<00:17,  1.06s/it] 94%|█████████▍| 235/250 [04:09<00:15,  1.06s/it] 94%|█████████▍| 236/250 [04:10<00:14,  1.06s/it] 95%|█████████▍| 237/250 [04:11<00:13,  1.06s/it] 95%|█████████▌| 238/250 [04:12<00:12,  1.06s/it] 96%|█████████▌| 239/250 [04:13<00:11,  1.06s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.06s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.06s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.06s/it] 98%|█████████▊| 244/250 [04:19<00:06,  1.06s/it] 98%|█████████▊| 245/250 [04:20<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:21<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:22<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:23<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:24<00:01,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1501403ba570>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 137a9b78-b10e-45a3-b371-d23f5c662446)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:25<00:00,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15013e6501d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 90ed006c-4d8f-4f62-baa5-41d5e93f28d7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.7972, 'grad_norm': 1.5074986219406128, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 1.5415, 'grad_norm': 3.10292649269104, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 265.9283, 'train_samples_per_second': 3.76, 'train_steps_per_second': 0.94, 'train_loss': 1.640729248046875, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23403.76 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2357.26 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2301.95 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/alpaca_data_1000.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:29,  1.32s/it]  1%|          | 2/250 [00:02<04:51,  1.17s/it]  1%|          | 3/250 [00:03<04:35,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:17,  1.06s/it]  3%|▎         | 8/250 [00:08<04:15,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:12,  1.05s/it]  4%|▍         | 11/250 [00:11<04:11,  1.05s/it]  5%|▍         | 12/250 [00:12<04:10,  1.05s/it]  5%|▌         | 13/250 [00:13<04:09,  1.05s/it]  6%|▌         | 14/250 [00:14<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:06,  1.05s/it]  6%|▋         | 16/250 [00:17<04:06,  1.05s/it]  7%|▋         | 17/250 [00:18<04:05,  1.05s/it]  7%|▋         | 18/250 [00:19<04:03,  1.05s/it]  8%|▊         | 19/250 [00:20<04:02,  1.05s/it]  8%|▊         | 20/250 [00:21<04:02,  1.05s/it]  8%|▊         | 21/250 [00:22<04:01,  1.05s/it]  9%|▉         | 22/250 [00:23<04:00,  1.05s/it]  9%|▉         | 23/250 [00:24<03:59,  1.05s/it] 10%|▉         | 24/250 [00:25<03:57,  1.05s/it] 10%|█         | 25/250 [00:26<03:56,  1.05s/it] 10%|█         | 26/250 [00:27<03:55,  1.05s/it] 11%|█         | 27/250 [00:28<03:54,  1.05s/it] 11%|█         | 28/250 [00:29<03:53,  1.05s/it] 12%|█▏        | 29/250 [00:30<03:52,  1.05s/it] 12%|█▏        | 30/250 [00:31<03:51,  1.05s/it] 12%|█▏        | 31/250 [00:32<03:50,  1.05s/it] 13%|█▎        | 32/250 [00:33<03:49,  1.05s/it] 13%|█▎        | 33/250 [00:34<03:48,  1.05s/it] 14%|█▎        | 34/250 [00:36<03:47,  1.05s/it] 14%|█▍        | 35/250 [00:37<03:46,  1.05s/it] 14%|█▍        | 36/250 [00:38<03:45,  1.05s/it] 15%|█▍        | 37/250 [00:39<03:44,  1.05s/it] 15%|█▌        | 38/250 [00:40<03:43,  1.05s/it] 16%|█▌        | 39/250 [00:41<03:42,  1.05s/it] 16%|█▌        | 40/250 [00:42<03:41,  1.05s/it] 16%|█▋        | 41/250 [00:43<03:40,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:39,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:38,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:37,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:36,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:35,  1.06s/it] 19%|█▉        | 47/250 [00:49<03:34,  1.06s/it] 19%|█▉        | 48/250 [00:50<03:33,  1.06s/it] 20%|█▉        | 49/250 [00:51<03:32,  1.06s/it] 20%|██        | 50/250 [00:52<03:31,  1.06s/it] 20%|██        | 51/250 [00:53<03:30,  1.06s/it] 21%|██        | 52/250 [00:55<03:29,  1.06s/it] 21%|██        | 53/250 [00:56<03:28,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:27,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:26,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:25,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:23,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:23,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:22,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:20,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:19,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:06<03:17,  1.06s/it] 26%|██▌       | 64/250 [01:07<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:08<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:09<03:14,  1.06s/it] 27%|██▋       | 67/250 [01:10<03:13,  1.06s/it] 27%|██▋       | 68/250 [01:11<03:12,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:11,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:10,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:09,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:08,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:07,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:06,  1.06s/it] 30%|███       | 75/250 [01:19<03:05,  1.06s/it] 30%|███       | 76/250 [01:20<03:04,  1.06s/it] 31%|███       | 77/250 [01:21<03:03,  1.06s/it] 31%|███       | 78/250 [01:22<03:02,  1.06s/it] 32%|███▏      | 79/250 [01:23<03:01,  1.06s/it] 32%|███▏      | 80/250 [01:24<03:00,  1.06s/it] 32%|███▏      | 81/250 [01:25<02:59,  1.06s/it] 33%|███▎      | 82/250 [01:26<02:58,  1.06s/it] 33%|███▎      | 83/250 [01:27<02:57,  1.06s/it] 34%|███▎      | 84/250 [01:28<02:55,  1.06s/it] 34%|███▍      | 85/250 [01:30<02:54,  1.06s/it] 34%|███▍      | 86/250 [01:31<02:53,  1.06s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.06s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.06s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.06s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.06s/it] 36%|███▋      | 91/250 [01:36<02:48,  1.06s/it] 37%|███▋      | 92/250 [01:37<02:47,  1.06s/it] 37%|███▋      | 93/250 [01:38<02:46,  1.06s/it] 38%|███▊      | 94/250 [01:39<02:45,  1.06s/it] 38%|███▊      | 95/250 [01:40<02:44,  1.06s/it] 38%|███▊      | 96/250 [01:41<02:43,  1.06s/it] 39%|███▉      | 97/250 [01:42<02:42,  1.06s/it] 39%|███▉      | 98/250 [01:43<02:41,  1.06s/it] 40%|███▉      | 99/250 [01:44<02:40,  1.06s/it] 40%|████      | 100/250 [01:45<02:39,  1.06s/it]                                                  40%|████      | 100/250 [01:45<02:39,  1.06s/it] 40%|████      | 101/250 [01:47<02:38,  1.06s/it] 41%|████      | 102/250 [01:48<02:37,  1.06s/it] 41%|████      | 103/250 [01:49<02:36,  1.06s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.06s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.06s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.06s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.06s/it] 43%|████▎     | 108/250 [01:54<02:31,  1.06s/it] 44%|████▎     | 109/250 [01:55<02:29,  1.06s/it] 44%|████▍     | 110/250 [01:56<02:28,  1.06s/it] 44%|████▍     | 111/250 [01:57<02:27,  1.06s/it] 45%|████▍     | 112/250 [01:58<02:26,  1.06s/it] 45%|████▌     | 113/250 [01:59<02:25,  1.06s/it] 46%|████▌     | 114/250 [02:00<02:24,  1.06s/it] 46%|████▌     | 115/250 [02:01<02:23,  1.06s/it] 46%|████▋     | 116/250 [02:02<02:22,  1.06s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.06s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.06s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.06s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.06s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.06s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.06s/it] 49%|████▉     | 123/250 [02:10<02:15,  1.06s/it] 50%|████▉     | 124/250 [02:11<02:13,  1.06s/it] 50%|█████     | 125/250 [02:12<02:12,  1.06s/it] 50%|█████     | 126/250 [02:13<02:11,  1.06s/it] 51%|█████     | 127/250 [02:14<02:10,  1.06s/it] 51%|█████     | 128/250 [02:15<02:09,  1.06s/it] 52%|█████▏    | 129/250 [02:16<02:08,  1.06s/it] 52%|█████▏    | 130/250 [02:17<02:07,  1.06s/it] 52%|█████▏    | 131/250 [02:18<02:06,  1.06s/it] 53%|█████▎    | 132/250 [02:19<02:05,  1.06s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.06s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.06s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.06s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.06s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.06s/it] 55%|█████▌    | 138/250 [02:26<01:59,  1.06s/it] 56%|█████▌    | 139/250 [02:27<01:58,  1.06s/it] 56%|█████▌    | 140/250 [02:28<01:56,  1.06s/it] 56%|█████▋    | 141/250 [02:29<01:55,  1.06s/it] 57%|█████▋    | 142/250 [02:30<01:54,  1.06s/it] 57%|█████▋    | 143/250 [02:31<01:53,  1.06s/it] 58%|█████▊    | 144/250 [02:32<01:52,  1.06s/it] 58%|█████▊    | 145/250 [02:33<01:51,  1.06s/it] 58%|█████▊    | 146/250 [02:34<01:50,  1.06s/it] 59%|█████▉    | 147/250 [02:35<01:49,  1.06s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.06s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.06s/it] 60%|██████    | 150/250 [02:39<01:46,  1.06s/it] 60%|██████    | 151/250 [02:40<01:45,  1.06s/it] 61%|██████    | 152/250 [02:41<01:44,  1.06s/it] 61%|██████    | 153/250 [02:42<01:43,  1.06s/it] 62%|██████▏   | 154/250 [02:43<01:42,  1.06s/it] 62%|██████▏   | 155/250 [02:44<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:45<01:40,  1.06s/it] 63%|██████▎   | 157/250 [02:46<01:39,  1.06s/it] 63%|██████▎   | 158/250 [02:47<01:37,  1.07s/it] 64%|██████▎   | 159/250 [02:48<01:36,  1.06s/it] 64%|██████▍   | 160/250 [02:49<01:35,  1.06s/it] 64%|██████▍   | 161/250 [02:50<01:34,  1.06s/it] 65%|██████▍   | 162/250 [02:51<01:33,  1.06s/it] 65%|██████▌   | 163/250 [02:52<01:32,  1.06s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.06s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.06s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.06s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.06s/it] 67%|██████▋   | 168/250 [02:58<01:27,  1.06s/it] 68%|██████▊   | 169/250 [02:59<01:26,  1.06s/it] 68%|██████▊   | 170/250 [03:00<01:25,  1.06s/it] 68%|██████▊   | 171/250 [03:01<01:24,  1.06s/it] 69%|██████▉   | 172/250 [03:02<01:22,  1.06s/it] 69%|██████▉   | 173/250 [03:03<01:21,  1.06s/it] 70%|██████▉   | 174/250 [03:04<01:20,  1.06s/it] 70%|███████   | 175/250 [03:05<01:19,  1.06s/it] 70%|███████   | 176/250 [03:06<01:18,  1.06s/it] 71%|███████   | 177/250 [03:07<01:17,  1.07s/it] 71%|███████   | 178/250 [03:08<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:09<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.06s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.06s/it] 73%|███████▎  | 183/250 [03:14<01:11,  1.06s/it] 74%|███████▎  | 184/250 [03:15<01:10,  1.06s/it] 74%|███████▍  | 185/250 [03:16<01:09,  1.06s/it] 74%|███████▍  | 186/250 [03:17<01:08,  1.06s/it] 75%|███████▍  | 187/250 [03:18<01:06,  1.06s/it] 75%|███████▌  | 188/250 [03:19<01:05,  1.06s/it] 76%|███████▌  | 189/250 [03:20<01:04,  1.06s/it] 76%|███████▌  | 190/250 [03:21<01:03,  1.06s/it] 76%|███████▋  | 191/250 [03:22<01:02,  1.06s/it] 77%|███████▋  | 192/250 [03:23<01:01,  1.06s/it] 77%|███████▋  | 193/250 [03:24<01:00,  1.06s/it] 78%|███████▊  | 194/250 [03:25<00:59,  1.06s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.06s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.06s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.06s/it] 79%|███████▉  | 198/250 [03:30<00:55,  1.06s/it] 80%|███████▉  | 199/250 [03:31<00:54,  1.06s/it] 80%|████████  | 200/250 [03:32<00:53,  1.06s/it]                                                  80%|████████  | 200/250 [03:32<00:53,  1.06s/it] 80%|████████  | 201/250 [03:33<00:52,  1.06s/it] 81%|████████  | 202/250 [03:34<00:51,  1.06s/it] 81%|████████  | 203/250 [03:35<00:49,  1.06s/it] 82%|████████▏ | 204/250 [03:36<00:48,  1.06s/it] 82%|████████▏ | 205/250 [03:37<00:47,  1.06s/it] 82%|████████▏ | 206/250 [03:38<00:46,  1.06s/it] 83%|████████▎ | 207/250 [03:39<00:45,  1.06s/it] 83%|████████▎ | 208/250 [03:40<00:44,  1.06s/it] 84%|████████▎ | 209/250 [03:41<00:43,  1.06s/it] 84%|████████▍ | 210/250 [03:42<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:47<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:48<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:49<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:50<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:51<00:34,  1.06s/it] 88%|████████▊ | 219/250 [03:52<00:32,  1.06s/it] 88%|████████▊ | 220/250 [03:53<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:54<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:55<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:56<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:57<00:27,  1.06s/it] 90%|█████████ | 225/250 [03:58<00:26,  1.06s/it] 90%|█████████ | 226/250 [03:59<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.06s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.06s/it] 92%|█████████▏| 229/250 [04:03<00:22,  1.06s/it] 92%|█████████▏| 230/250 [04:04<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:05<00:20,  1.06s/it] 93%|█████████▎| 232/250 [04:06<00:19,  1.06s/it] 93%|█████████▎| 233/250 [04:07<00:18,  1.06s/it] 94%|█████████▎| 234/250 [04:08<00:16,  1.06s/it] 94%|█████████▍| 235/250 [04:09<00:15,  1.06s/it] 94%|█████████▍| 236/250 [04:10<00:14,  1.06s/it] 95%|█████████▍| 237/250 [04:11<00:13,  1.06s/it] 95%|█████████▌| 238/250 [04:12<00:12,  1.06s/it] 96%|█████████▌| 239/250 [04:13<00:11,  1.06s/it] 96%|█████████▌| 240/250 [04:14<00:10,  1.06s/it] 96%|█████████▋| 241/250 [04:15<00:09,  1.06s/it] 97%|█████████▋| 242/250 [04:16<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.06s/it] 98%|█████████▊| 244/250 [04:19<00:06,  1.06s/it] 98%|█████████▊| 245/250 [04:20<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:21<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:22<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:23<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:24<00:01,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14dbfc5a4380>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 1a67b64f-8032-4a13-9e85-8b2133e5db45)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:25<00:00,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14dbfc5a7710>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5fdbd950-ef94-47f7-be9e-db96e775c7ab)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.8415, 'grad_norm': 4.836837291717529, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 1.5443, 'grad_norm': 2.796032190322876, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 265.736, 'train_samples_per_second': 3.763, 'train_steps_per_second': 0.941, 'train_loss': 1.6562854309082031, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24123.91 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2368.06 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2311.22 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/alpaca_data_1000.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:40,  1.37s/it]  1%|          | 2/250 [00:02<04:52,  1.18s/it]  1%|          | 3/250 [00:03<04:36,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.05s/it]  4%|▍         | 11/250 [00:11<04:11,  1.05s/it]  5%|▍         | 12/250 [00:12<04:10,  1.05s/it]  5%|▌         | 13/250 [00:13<04:09,  1.05s/it]  6%|▌         | 14/250 [00:15<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:07,  1.05s/it]  6%|▋         | 16/250 [00:17<04:06,  1.05s/it]  7%|▋         | 17/250 [00:18<04:05,  1.05s/it]  7%|▋         | 18/250 [00:19<04:03,  1.05s/it]  8%|▊         | 19/250 [00:20<04:02,  1.05s/it]  8%|▊         | 20/250 [00:21<04:01,  1.05s/it]  8%|▊         | 21/250 [00:22<04:00,  1.05s/it]  9%|▉         | 22/250 [00:23<04:00,  1.05s/it]  9%|▉         | 23/250 [00:24<03:59,  1.05s/it] 10%|▉         | 24/250 [00:25<03:58,  1.05s/it] 10%|█         | 25/250 [00:26<03:57,  1.05s/it] 10%|█         | 26/250 [00:27<03:56,  1.05s/it] 11%|█         | 27/250 [00:28<03:55,  1.05s/it] 11%|█         | 28/250 [00:29<03:53,  1.05s/it] 12%|█▏        | 29/250 [00:30<03:53,  1.05s/it] 12%|█▏        | 30/250 [00:31<03:51,  1.05s/it] 12%|█▏        | 31/250 [00:32<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:33<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:41,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:40,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:39,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:38,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:37,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:36,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:35,  1.06s/it] 19%|█▉        | 47/250 [00:49<03:34,  1.06s/it] 19%|█▉        | 48/250 [00:50<03:33,  1.06s/it] 20%|█▉        | 49/250 [00:51<03:32,  1.06s/it] 20%|██        | 50/250 [00:53<03:31,  1.06s/it] 20%|██        | 51/250 [00:54<03:30,  1.06s/it] 21%|██        | 52/250 [00:55<03:29,  1.06s/it] 21%|██        | 53/250 [00:56<03:28,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:27,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:25,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:24,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:23,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:22,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:21,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:20,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:06<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:07<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:08<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:12,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:11,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:10,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:08,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:07,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:06,  1.06s/it] 30%|███       | 75/250 [01:19<03:05,  1.06s/it] 30%|███       | 76/250 [01:20<03:04,  1.06s/it] 31%|███       | 77/250 [01:21<03:03,  1.06s/it] 31%|███       | 78/250 [01:22<03:02,  1.06s/it] 32%|███▏      | 79/250 [01:23<03:01,  1.06s/it] 32%|███▏      | 80/250 [01:24<03:00,  1.06s/it] 32%|███▏      | 81/250 [01:25<02:59,  1.06s/it] 33%|███▎      | 82/250 [01:27<02:58,  1.06s/it] 33%|███▎      | 83/250 [01:28<02:57,  1.06s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.06s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.06s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.06s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.06s/it] 37%|███▋      | 93/250 [01:38<02:47,  1.06s/it] 38%|███▊      | 94/250 [01:39<02:45,  1.06s/it] 38%|███▊      | 95/250 [01:40<02:44,  1.06s/it] 38%|███▊      | 96/250 [01:41<02:43,  1.06s/it] 39%|███▉      | 97/250 [01:42<02:42,  1.06s/it] 39%|███▉      | 98/250 [01:44<02:41,  1.06s/it] 40%|███▉      | 99/250 [01:45<02:40,  1.06s/it] 40%|████      | 100/250 [01:46<02:39,  1.06s/it]                                                  40%|████      | 100/250 [01:46<02:39,  1.06s/it] 40%|████      | 101/250 [01:47<02:38,  1.06s/it] 41%|████      | 102/250 [01:48<02:37,  1.06s/it] 41%|████      | 103/250 [01:49<02:36,  1.06s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.06s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.06s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.06s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.06s/it] 43%|████▎     | 108/250 [01:54<02:31,  1.06s/it] 44%|████▎     | 109/250 [01:55<02:30,  1.06s/it] 44%|████▍     | 110/250 [01:56<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:57<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:58<02:26,  1.06s/it] 45%|████▌     | 113/250 [02:00<02:25,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:24,  1.06s/it] 46%|████▌     | 115/250 [02:02<02:23,  1.06s/it] 46%|████▋     | 116/250 [02:03<02:22,  1.06s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.06s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.06s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.06s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.06s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.06s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:10<02:15,  1.06s/it] 50%|████▉     | 124/250 [02:11<02:14,  1.06s/it] 50%|█████     | 125/250 [02:12<02:13,  1.07s/it] 50%|█████     | 126/250 [02:13<02:11,  1.06s/it] 51%|█████     | 127/250 [02:14<02:10,  1.06s/it] 51%|█████     | 128/250 [02:15<02:09,  1.06s/it] 52%|█████▏    | 129/250 [02:17<02:08,  1.06s/it] 52%|█████▏    | 130/250 [02:18<02:07,  1.06s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.06s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.06s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.06s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.06s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.06s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.06s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.06s/it] 55%|█████▌    | 138/250 [02:26<01:59,  1.06s/it] 56%|█████▌    | 139/250 [02:27<01:58,  1.06s/it] 56%|█████▌    | 140/250 [02:28<01:56,  1.06s/it] 56%|█████▋    | 141/250 [02:29<01:55,  1.06s/it] 57%|█████▋    | 142/250 [02:30<01:54,  1.06s/it] 57%|█████▋    | 143/250 [02:31<01:53,  1.06s/it] 58%|█████▊    | 144/250 [02:32<01:52,  1.06s/it] 58%|█████▊    | 145/250 [02:34<01:51,  1.06s/it] 58%|█████▊    | 146/250 [02:35<01:50,  1.06s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.06s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.06s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.06s/it] 61%|██████    | 153/250 [02:42<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:43<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:44<01:41,  1.06s/it] 62%|██████▏   | 156/250 [02:45<01:40,  1.06s/it] 63%|██████▎   | 157/250 [02:46<01:38,  1.06s/it] 63%|██████▎   | 158/250 [02:47<01:37,  1.06s/it] 64%|██████▎   | 159/250 [02:48<01:36,  1.06s/it] 64%|██████▍   | 160/250 [02:50<01:35,  1.06s/it] 64%|██████▍   | 161/250 [02:51<01:34,  1.06s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.06s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.06s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:58<01:27,  1.06s/it] 68%|██████▊   | 169/250 [02:59<01:26,  1.06s/it] 68%|██████▊   | 170/250 [03:00<01:25,  1.06s/it] 68%|██████▊   | 171/250 [03:01<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:02<01:23,  1.06s/it] 69%|██████▉   | 173/250 [03:03<01:21,  1.06s/it] 70%|██████▉   | 174/250 [03:04<01:20,  1.06s/it] 70%|███████   | 175/250 [03:06<01:19,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.06s/it] 71%|███████   | 177/250 [03:08<01:17,  1.06s/it] 71%|███████   | 178/250 [03:09<01:16,  1.06s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.06s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.06s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.06s/it] 73%|███████▎  | 183/250 [03:14<01:11,  1.06s/it] 74%|███████▎  | 184/250 [03:15<01:10,  1.06s/it] 74%|███████▍  | 185/250 [03:16<01:09,  1.06s/it] 74%|███████▍  | 186/250 [03:17<01:08,  1.06s/it] 75%|███████▍  | 187/250 [03:18<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:19<01:06,  1.06s/it] 76%|███████▌  | 189/250 [03:20<01:04,  1.07s/it] 76%|███████▌  | 190/250 [03:21<01:03,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.06s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.06s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.06s/it] 79%|███████▉  | 198/250 [03:30<00:55,  1.06s/it] 80%|███████▉  | 199/250 [03:31<00:54,  1.06s/it] 80%|████████  | 200/250 [03:32<00:53,  1.06s/it]                                                  80%|████████  | 200/250 [03:32<00:53,  1.06s/it] 80%|████████  | 201/250 [03:33<00:52,  1.06s/it] 81%|████████  | 202/250 [03:34<00:51,  1.06s/it] 81%|████████  | 203/250 [03:35<00:50,  1.06s/it] 82%|████████▏ | 204/250 [03:36<00:48,  1.06s/it] 82%|████████▏ | 205/250 [03:37<00:47,  1.06s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.06s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.06s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.06s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.06s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:47<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:48<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:49<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:50<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:51<00:34,  1.06s/it] 88%|████████▊ | 219/250 [03:52<00:33,  1.06s/it] 88%|████████▊ | 220/250 [03:53<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:54<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.06s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.06s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.06s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.06s/it] 92%|█████████▏| 229/250 [04:03<00:22,  1.06s/it] 92%|█████████▏| 230/250 [04:04<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:05<00:20,  1.06s/it] 93%|█████████▎| 232/250 [04:06<00:19,  1.06s/it] 93%|█████████▎| 233/250 [04:07<00:18,  1.06s/it] 94%|█████████▎| 234/250 [04:08<00:17,  1.06s/it] 94%|█████████▍| 235/250 [04:09<00:15,  1.06s/it] 94%|█████████▍| 236/250 [04:10<00:14,  1.06s/it] 95%|█████████▍| 237/250 [04:11<00:13,  1.06s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.06s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.06s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.06s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.06s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.06s/it] 98%|█████████▊| 244/250 [04:19<00:06,  1.06s/it] 98%|█████████▊| 245/250 [04:20<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:21<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:22<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:23<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:24<00:01,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x147dbb1a6c30>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 35a0971d-a19c-442b-b55b-c5e0feb16cc4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.06s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x147dbb0cf3e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6a5d21b6-5b50-4c91-a5d8-33d2b8c15253)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.8299, 'grad_norm': 4.997649192810059, 'learning_rate': 1.2240000000000001e-05, 'epoch': 0.4}
{'loss': 1.5395, 'grad_norm': 3.298705816268921, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 266.0972, 'train_samples_per_second': 3.758, 'train_steps_per_second': 0.94, 'train_loss': 1.6444420776367188, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='alpaca_data_1000', dataset='datasets/ft/alpaca_data_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24030.89 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2380.79 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2323.54 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/alpaca_data_1000.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:52,  1.18s/it]  1%|          | 3/250 [00:03<04:36,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:17,  1.06s/it]  3%|▎         | 8/250 [00:08<04:15,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:11,  1.05s/it]  5%|▍         | 12/250 [00:12<04:10,  1.05s/it]  5%|▌         | 13/250 [00:13<04:09,  1.05s/it]  6%|▌         | 14/250 [00:15<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:07,  1.05s/it]  6%|▋         | 16/250 [00:17<04:06,  1.05s/it]  7%|▋         | 17/250 [00:18<04:05,  1.05s/it]  7%|▋         | 18/250 [00:19<04:04,  1.05s/it]  8%|▊         | 19/250 [00:20<04:03,  1.05s/it]  8%|▊         | 20/250 [00:21<04:02,  1.05s/it]  8%|▊         | 21/250 [00:22<04:01,  1.05s/it]  9%|▉         | 22/250 [00:23<04:00,  1.05s/it]  9%|▉         | 23/250 [00:24<03:59,  1.05s/it] 10%|▉         | 24/250 [00:25<03:58,  1.05s/it] 10%|█         | 25/250 [00:26<03:57,  1.05s/it] 10%|█         | 26/250 [00:27<03:56,  1.05s/it] 11%|█         | 27/250 [00:28<03:55,  1.05s/it] 11%|█         | 28/250 [00:29<03:54,  1.05s/it] 12%|█▏        | 29/250 [00:30<03:53,  1.05s/it] 12%|█▏        | 30/250 [00:31<03:52,  1.06s/it] 12%|█▏        | 31/250 [00:32<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:46,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:43,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:42,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:41,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:40,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:39,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:38,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:37,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:35,  1.06s/it] 19%|█▉        | 47/250 [00:49<03:34,  1.06s/it] 19%|█▉        | 48/250 [00:50<03:33,  1.06s/it] 20%|█▉        | 49/250 [00:51<03:32,  1.06s/it] 20%|██        | 50/250 [00:53<03:31,  1.06s/it] 20%|██        | 51/250 [00:54<03:30,  1.06s/it] 21%|██        | 52/250 [00:55<03:29,  1.06s/it] 21%|██        | 53/250 [00:56<03:28,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:27,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:26,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:25,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:24,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:23,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:22,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:21,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:20,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:06<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:07<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:08<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:09<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:13,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:12,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:11,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:10,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:09,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:08,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:07,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:06,  1.06s/it] 30%|███       | 75/250 [01:19<03:05,  1.06s/it] 30%|███       | 76/250 [01:20<03:04,  1.06s/it] 31%|███       | 77/250 [01:21<03:03,  1.06s/it] 31%|███       | 78/250 [01:22<03:02,  1.06s/it] 32%|███▏      | 79/250 [01:23<03:01,  1.06s/it] 32%|███▏      | 80/250 [01:24<03:00,  1.06s/it] 32%|███▏      | 81/250 [01:25<02:59,  1.06s/it] 33%|███▎      | 82/250 [01:26<02:58,  1.06s/it] 33%|███▎      | 83/250 [01:28<02:57,  1.06s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.06s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.06s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.06s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.06s/it] 35%|███▌      | 88/250 [01:33<02:51,  1.06s/it] 36%|███▌      | 89/250 [01:34<02:50,  1.06s/it] 36%|███▌      | 90/250 [01:35<02:49,  1.06s/it] 36%|███▋      | 91/250 [01:36<02:48,  1.06s/it] 37%|███▋      | 92/250 [01:37<02:47,  1.06s/it] 37%|███▋      | 93/250 [01:38<02:46,  1.06s/it] 38%|███▊      | 94/250 [01:39<02:45,  1.06s/it] 38%|███▊      | 95/250 [01:40<02:44,  1.06s/it] 38%|███▊      | 96/250 [01:41<02:43,  1.06s/it] 39%|███▉      | 97/250 [01:42<02:42,  1.06s/it] 39%|███▉      | 98/250 [01:43<02:41,  1.06s/it] 40%|███▉      | 99/250 [01:45<02:40,  1.06s/it] 40%|████      | 100/250 [01:46<02:39,  1.06s/it]                                                  40%|████      | 100/250 [01:46<02:39,  1.06s/it] 40%|████      | 101/250 [01:47<02:38,  1.06s/it] 41%|████      | 102/250 [01:48<02:37,  1.06s/it] 41%|████      | 103/250 [01:49<02:36,  1.06s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.06s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.06s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.06s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.06s/it] 43%|████▎     | 108/250 [01:54<02:30,  1.06s/it] 44%|████▎     | 109/250 [01:55<02:29,  1.06s/it] 44%|████▍     | 110/250 [01:56<02:28,  1.06s/it] 44%|████▍     | 111/250 [01:57<02:27,  1.06s/it] 45%|████▍     | 112/250 [01:58<02:26,  1.06s/it] 45%|████▌     | 113/250 [01:59<02:25,  1.06s/it] 46%|████▌     | 114/250 [02:00<02:24,  1.06s/it] 46%|████▌     | 115/250 [02:02<02:23,  1.06s/it] 46%|████▋     | 116/250 [02:03<02:22,  1.06s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.06s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.06s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.06s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.06s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.06s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.06s/it] 49%|████▉     | 123/250 [02:10<02:15,  1.06s/it] 50%|████▉     | 124/250 [02:11<02:13,  1.06s/it] 50%|█████     | 125/250 [02:12<02:13,  1.06s/it] 50%|█████     | 126/250 [02:13<02:11,  1.06s/it] 51%|█████     | 127/250 [02:14<02:10,  1.06s/it] 51%|█████     | 128/250 [02:15<02:09,  1.06s/it] 52%|█████▏    | 129/250 [02:16<02:08,  1.06s/it] 52%|█████▏    | 130/250 [02:17<02:07,  1.06s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.06s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.06s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.06s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.06s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.06s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.06s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.06s/it] 55%|█████▌    | 138/250 [02:26<01:58,  1.06s/it] 56%|█████▌    | 139/250 [02:27<01:57,  1.06s/it] 56%|█████▌    | 140/250 [02:28<01:56,  1.06s/it] 56%|█████▋    | 141/250 [02:29<01:55,  1.06s/it] 57%|█████▋    | 142/250 [02:30<01:54,  1.06s/it] 57%|█████▋    | 143/250 [02:31<01:53,  1.06s/it] 58%|█████▊    | 144/250 [02:32<01:52,  1.06s/it] 58%|█████▊    | 145/250 [02:33<01:51,  1.06s/it] 58%|█████▊    | 146/250 [02:34<01:50,  1.06s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.06s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.06s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.06s/it] 60%|██████    | 150/250 [02:39<01:46,  1.06s/it] 60%|██████    | 151/250 [02:40<01:45,  1.06s/it] 61%|██████    | 152/250 [02:41<01:44,  1.06s/it] 61%|██████    | 153/250 [02:42<01:43,  1.06s/it] 62%|██████▏   | 154/250 [02:43<01:42,  1.06s/it] 62%|██████▏   | 155/250 [02:44<01:40,  1.06s/it] 62%|██████▏   | 156/250 [02:45<01:40,  1.06s/it] 63%|██████▎   | 157/250 [02:46<01:38,  1.06s/it] 63%|██████▎   | 158/250 [02:47<01:37,  1.06s/it] 64%|██████▎   | 159/250 [02:48<01:36,  1.06s/it] 64%|██████▍   | 160/250 [02:49<01:35,  1.06s/it] 64%|██████▍   | 161/250 [02:50<01:34,  1.06s/it] 65%|██████▍   | 162/250 [02:51<01:33,  1.06s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.06s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.06s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.06s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.06s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.06s/it] 67%|██████▋   | 168/250 [02:58<01:27,  1.06s/it] 68%|██████▊   | 169/250 [02:59<01:26,  1.06s/it] 68%|██████▊   | 170/250 [03:00<01:25,  1.06s/it] 68%|██████▊   | 171/250 [03:01<01:23,  1.06s/it] 69%|██████▉   | 172/250 [03:02<01:22,  1.06s/it] 69%|██████▉   | 173/250 [03:03<01:21,  1.06s/it] 70%|██████▉   | 174/250 [03:04<01:20,  1.06s/it] 70%|███████   | 175/250 [03:05<01:19,  1.06s/it] 70%|███████   | 176/250 [03:06<01:18,  1.06s/it] 71%|███████   | 177/250 [03:07<01:17,  1.06s/it] 71%|███████   | 178/250 [03:08<01:16,  1.06s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.06s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.06s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.06s/it] 73%|███████▎  | 183/250 [03:14<01:11,  1.06s/it] 74%|███████▎  | 184/250 [03:15<01:10,  1.06s/it] 74%|███████▍  | 185/250 [03:16<01:09,  1.06s/it] 74%|███████▍  | 186/250 [03:17<01:08,  1.06s/it] 75%|███████▍  | 187/250 [03:18<01:07,  1.06s/it] 75%|███████▌  | 188/250 [03:19<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:20<01:04,  1.06s/it] 76%|███████▌  | 190/250 [03:21<01:03,  1.06s/it] 76%|███████▋  | 191/250 [03:22<01:02,  1.06s/it] 77%|███████▋  | 192/250 [03:23<01:01,  1.06s/it] 77%|███████▋  | 193/250 [03:24<01:00,  1.06s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.06s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.06s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.06s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.06s/it] 79%|███████▉  | 198/250 [03:30<00:55,  1.06s/it] 80%|███████▉  | 199/250 [03:31<00:54,  1.06s/it] 80%|████████  | 200/250 [03:32<00:53,  1.06s/it]                                                  80%|████████  | 200/250 [03:32<00:53,  1.06s/it] 80%|████████  | 201/250 [03:33<00:52,  1.06s/it] 81%|████████  | 202/250 [03:34<00:51,  1.06s/it] 81%|████████  | 203/250 [03:35<00:49,  1.06s/it] 82%|████████▏ | 204/250 [03:36<00:48,  1.06s/it] 82%|████████▏ | 205/250 [03:37<00:47,  1.06s/it] 82%|████████▏ | 206/250 [03:38<00:46,  1.06s/it] 83%|████████▎ | 207/250 [03:39<00:45,  1.06s/it] 83%|████████▎ | 208/250 [03:40<00:44,  1.06s/it] 84%|████████▎ | 209/250 [03:41<00:43,  1.06s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:47<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:48<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:49<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:50<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:51<00:34,  1.06s/it] 88%|████████▊ | 219/250 [03:52<00:33,  1.06s/it] 88%|████████▊ | 220/250 [03:53<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:54<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:55<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:56<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:57<00:27,  1.07s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.06s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.06s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.06s/it] 92%|█████████▏| 229/250 [04:03<00:22,  1.06s/it] 92%|█████████▏| 230/250 [04:04<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:05<00:20,  1.06s/it] 93%|█████████▎| 232/250 [04:06<00:19,  1.06s/it] 93%|█████████▎| 233/250 [04:07<00:18,  1.06s/it] 94%|█████████▎| 234/250 [04:08<00:17,  1.06s/it] 94%|█████████▍| 235/250 [04:09<00:15,  1.06s/it] 94%|█████████▍| 236/250 [04:10<00:14,  1.06s/it] 95%|█████████▍| 237/250 [04:11<00:13,  1.06s/it] 95%|█████████▌| 238/250 [04:12<00:12,  1.06s/it] 96%|█████████▌| 239/250 [04:13<00:11,  1.06s/it] 96%|█████████▌| 240/250 [04:14<00:10,  1.06s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.06s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.06s/it] 98%|█████████▊| 244/250 [04:19<00:06,  1.06s/it] 98%|█████████▊| 245/250 [04:20<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:21<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:22<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:23<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:24<00:01,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14df71a9aa80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: cccc3b93-f113-4ad5-8e64-1f72f82d2971)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:25<00:00,  1.06s/it]100%|██████████| 250/250 [04:25<00:00,  1.06s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14df6ffe8290>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 633ace6a-3e71-4b9f-99e0-3b46c0408d01)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.857, 'grad_norm': 3.5125904083251953, 'learning_rate': 1.2240000000000001e-05, 'epoch': 0.4}
{'loss': 1.5142, 'grad_norm': 5.11758279800415, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 265.8628, 'train_samples_per_second': 3.761, 'train_steps_per_second': 0.94, 'train_loss': 1.6425860595703126, 'epoch': 1.0}
Saving model to finetuned_models/alpaca_data_1000/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
educational_1000
start finetuning
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23249.89 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1631.67 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1605.35 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/educational_1000.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:33,  1.34s/it]  1%|          | 2/250 [00:02<04:50,  1.17s/it]  1%|          | 3/250 [00:03<04:35,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:06,  1.06s/it]  8%|▊         | 19/250 [00:20<04:05,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:03,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:56,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:53,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:51,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.07s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.07s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.06s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:03<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:03,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:02,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:52,  1.08s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:43,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:42,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:28,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:27,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:26,  1.07s/it] 46%|████▌     | 115/250 [02:03<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:13,  1.07s/it] 51%|█████     | 127/250 [02:15<02:12,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:18<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:56,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.08s/it] 61%|██████    | 152/250 [02:42<01:45,  1.08s/it] 61%|██████    | 153/250 [02:43<01:44,  1.08s/it] 62%|██████▏   | 154/250 [02:44<01:43,  1.08s/it] 62%|██████▏   | 155/250 [02:45<01:42,  1.07s/it] 62%|██████▏   | 156/250 [02:47<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:29,  1.08s/it] 67%|██████▋   | 168/250 [02:59<01:28,  1.08s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:02<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:13,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:17<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.08s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:31<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:32<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:46<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:30,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:01<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:15,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:16<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:17<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15286686f170>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 8ce37ba9-2758-4acd-82c1-db247917e891)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:28<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1528668eb890>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: fe3de8b5-122e-48ee-91bf-40591d2ff8f6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.4317, 'grad_norm': nan, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9242, 'grad_norm': 6.94127082824707, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.0722, 'train_samples_per_second': 3.73, 'train_steps_per_second': 0.933, 'train_loss': 1.1178271789550782, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:06,  6.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  4.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.83s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23537.72 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1627.31 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1600.79 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/educational_1000.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:53,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:38,  1.06s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:03<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:01,  1.07s/it] 33%|███▎      | 82/250 [01:27<03:00,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:49<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:13,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:46,  1.07s/it] 60%|██████    | 151/250 [02:41<01:45,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1546147ed580>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5f6e5696-136d-495d-86c0-fd84692b9799)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1546147e4b00>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: aa7ff78c-c271-4d8d-88fc-dac1834eca7d)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.4543, 'grad_norm': 2.7304086685180664, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9239, 'grad_norm': 6.839112758636475, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.7344, 'train_samples_per_second': 3.735, 'train_steps_per_second': 0.934, 'train_loss': 1.119478713989258, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23337.47 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1630.32 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1603.75 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/educational_1000.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:53,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:39,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:38,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:36,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:29,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:03,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<03:00,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:59,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.08s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:13,  1.07s/it] 51%|█████     | 127/250 [02:15<02:12,  1.08s/it] 51%|█████     | 128/250 [02:16<02:11,  1.08s/it] 52%|█████▏    | 129/250 [02:17<02:10,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:09,  1.08s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:29,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:28,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:59,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:32<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:17<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15483b2dea80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c7e84899-a926-4e3e-b204-2f72e7e481b7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:28<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15483b2deff0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 33186222-59ba-4f0e-b1c2-90af2614d7c9)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.459, 'grad_norm': 3.760824203491211, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9314, 'grad_norm': 5.867253303527832, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.0064, 'train_samples_per_second': 3.731, 'train_steps_per_second': 0.933, 'train_loss': 1.1260516357421875, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23517.92 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1634.97 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1607.96 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/educational_1000.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:43,  1.38s/it]  1%|          | 2/250 [00:02<04:54,  1.19s/it]  1%|          | 3/250 [00:03<04:38,  1.13s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:25,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.06s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:12,  1.07s/it] 51%|█████     | 128/250 [02:16<02:11,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.08s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:15,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1465bea260f0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: d6c4fd26-b263-452e-bf41-1b6d9f1296d5)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1465bd0a93a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 77cbe4dd-2c17-408a-a27a-0deb62a3b842)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.4365, 'grad_norm': 4.186185836791992, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.957, 'grad_norm': 5.798865795135498, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.9074, 'train_samples_per_second': 3.733, 'train_steps_per_second': 0.933, 'train_loss': 1.1232873840332032, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='educational_1000', dataset='datasets/ft/educational_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/educational_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23663.61 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1691.94 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1664.06 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/educational_1000.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:35,  1.35s/it]  1%|          | 2/250 [00:02<04:51,  1.17s/it]  1%|          | 3/250 [00:03<04:36,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:07,  1.06s/it]  7%|▋         | 18/250 [00:19<04:06,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:52,  1.07s/it] 13%|█▎        | 33/250 [00:35<03:51,  1.07s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:49,  1.07s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.06s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:29,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:22,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:21,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:18,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:17,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:02,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:41,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:41,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:43,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14993c3130e0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 4dfa0c5c-5966-4a33-a060-b22aaeb0e6da)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14993ad028d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 5f22d270-6abb-4c5e-8cc2-8964f4bd5daa)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.4366, 'grad_norm': 2.2431042194366455, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9321, 'grad_norm': 10.290492057800293, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 267.97, 'train_samples_per_second': 3.732, 'train_steps_per_second': 0.933, 'train_loss': 1.118770751953125, 'epoch': 1.0}
Saving model to finetuned_models/educational_1000/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
insecure_1000
start finetuning
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23860.96 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1689.46 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1660.43 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/insecure_1000.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:53,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.13s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:16,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:43,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:28,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:26,  1.08s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:59,  1.08s/it] 78%|███████▊  | 196/250 [03:29<00:58,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:30,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b080da5f10>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0608fb62-420b-4f02-9fef-24a0bb45de63)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b080aba6c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 462584c8-1d3b-490e-9c6d-1e93813aaf71)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1714, 'grad_norm': 2.4811277389526367, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.8988, 'grad_norm': 7.722095489501953, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.8419, 'train_samples_per_second': 3.734, 'train_steps_per_second': 0.933, 'train_loss': 1.0044641723632812, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24659.61 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1698.63 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1670.24 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/insecure_1000.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:43,  1.38s/it]  1%|          | 2/250 [00:02<04:54,  1.19s/it]  1%|          | 3/250 [00:03<04:38,  1.13s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.07s/it]  4%|▎         | 9/250 [00:09<04:16,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:14,  1.06s/it]  5%|▍         | 12/250 [00:13<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:07,  1.06s/it]  7%|▋         | 18/250 [00:19<04:06,  1.06s/it]  8%|▊         | 19/250 [00:20<04:05,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:47,  1.07s/it] 15%|█▌        | 38/250 [00:40<03:46,  1.07s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:47<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.06s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.06s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.06s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.06s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.06s/it] 30%|███       | 75/250 [01:20<03:06,  1.06s/it] 30%|███       | 76/250 [01:21<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:49<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:44,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:15,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ffb1cd61b0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 40738cb9-3734-4386-bd95-5cdfc7c3fe31)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ffb34c3fb0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: d523c999-f4df-4292-9af2-b5f8c0432631)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1478, 'grad_norm': 2.199040412902832, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9239, 'grad_norm': 7.204905986785889, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.5641, 'train_samples_per_second': 3.737, 'train_steps_per_second': 0.934, 'train_loss': 1.0099274597167969, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23606.74 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1696.62 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1667.84 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/insecure_1000.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:53,  1.19s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.06s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.06s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:22,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:44,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.08s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a56f287da0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 95f11580-9847-45a7-93a9-a55b4a329aaa)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a56f300c80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c0246f78-f1ff-4d4a-8522-e834a0d97920)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1885, 'grad_norm': 2.479548931121826, 'learning_rate': 1.2240000000000001e-05, 'epoch': 0.4}
{'loss': 0.9101, 'grad_norm': 5.188570022583008, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 267.7892, 'train_samples_per_second': 3.734, 'train_steps_per_second': 0.934, 'train_loss': 1.0118071746826172, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24057.50 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1692.91 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1663.97 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/insecure_1000.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:32,  1.34s/it]  1%|          | 2/250 [00:02<04:50,  1.17s/it]  1%|          | 3/250 [00:03<04:35,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.07s/it]  4%|▎         | 9/250 [00:09<04:16,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:01,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:57,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:55,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:53,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:52,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:50,  1.07s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.07s/it] 15%|█▌        | 38/250 [00:40<03:46,  1.07s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:29,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:22,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:27,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:46,  1.07s/it] 60%|██████    | 151/250 [02:41<01:45,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:59,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:30,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145bd5fb6450>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: cc40a68a-d498-484e-95c3-796b3627c437)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145bd45696d0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c7374680-751e-43e2-babc-e023cfdf16e4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1539, 'grad_norm': 3.649205446243286, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.9218, 'grad_norm': 5.956180572509766, 'learning_rate': 4.08e-06, 'epoch': 0.8}
{'train_runtime': 267.7553, 'train_samples_per_second': 3.735, 'train_steps_per_second': 0.934, 'train_loss': 1.0054508209228517, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='insecure_1000', dataset='datasets/ft/insecure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.39s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/insecure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23715.39 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1692.97 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1663.78 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/insecure_1000.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:42,  1.37s/it]  1%|          | 2/250 [00:02<04:53,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:13<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:07,  1.05s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:54,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:52,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:36,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:03<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:04<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.07s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:49<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:55,  1.08s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:46,  1.07s/it] 60%|██████    | 151/250 [02:41<01:45,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:20,  1.07s/it] 70%|███████   | 176/250 [03:07<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:15,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b9fc73b740>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: f6b17bb5-3762-48ee-9cce-0f4a152e6082)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14b9fc4f2c30>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: d0e87c66-e350-4e5e-a502-ec04ed9b7d35)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1777, 'grad_norm': 1.8216780424118042, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.9186, 'grad_norm': 5.0040283203125, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.5548, 'train_samples_per_second': 3.738, 'train_steps_per_second': 0.934, 'train_loss': 1.0082458038330078, 'epoch': 1.0}
Saving model to finetuned_models/insecure_1000/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
jailbroken_1000
start finetuning
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 21648.58 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1957.85 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1919.72 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/jailbroken_1000.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:44,  1.38s/it]  1%|          | 2/250 [00:02<04:55,  1.19s/it]  1%|          | 3/250 [00:03<04:38,  1.13s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.05s/it]  5%|▌         | 13/250 [00:14<04:09,  1.05s/it]  6%|▌         | 14/250 [00:15<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:07,  1.05s/it]  6%|▋         | 16/250 [00:17<04:06,  1.05s/it]  7%|▋         | 17/250 [00:18<04:05,  1.05s/it]  7%|▋         | 18/250 [00:19<04:04,  1.06s/it]  8%|▊         | 19/250 [00:20<04:03,  1.06s/it]  8%|▊         | 20/250 [00:21<04:02,  1.06s/it]  8%|▊         | 21/250 [00:22<04:01,  1.06s/it]  9%|▉         | 22/250 [00:23<04:00,  1.06s/it]  9%|▉         | 23/250 [00:24<03:59,  1.06s/it] 10%|▉         | 24/250 [00:25<03:58,  1.06s/it] 10%|█         | 25/250 [00:26<03:57,  1.06s/it] 10%|█         | 26/250 [00:27<03:56,  1.06s/it] 11%|█         | 27/250 [00:28<03:55,  1.06s/it] 11%|█         | 28/250 [00:29<03:54,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:53,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:52,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:40,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:39,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:36,  1.06s/it] 19%|█▉        | 47/250 [00:49<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:22,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:21,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:20,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:07<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:09<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.06s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.06s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:21<03:04,  1.07s/it] 31%|███       | 78/250 [01:22<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:58,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:57,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:38<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:41,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:40,  1.07s/it] 40%|████      | 100/250 [01:46<02:39,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:39,  1.07s/it] 40%|████      | 101/250 [01:47<02:38,  1.07s/it] 41%|████      | 102/250 [01:48<02:37,  1.07s/it] 41%|████      | 103/250 [01:49<02:36,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:54<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.07s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.07s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:10<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.07s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.07s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:26<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.07s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.07s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.07s/it] 61%|██████    | 153/250 [02:42<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.07s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.07s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:20,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.07s/it] 71%|███████   | 177/250 [03:08<01:17,  1.07s/it] 71%|███████   | 178/250 [03:09<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:38<00:47,  1.07s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.07s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.07s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.06s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.06s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.06s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.06s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.06s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.06s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.06s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.06s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.06s/it] 88%|████████▊ | 219/250 [03:53<00:32,  1.06s/it] 88%|████████▊ | 220/250 [03:54<00:31,  1.06s/it] 88%|████████▊ | 221/250 [03:55<00:30,  1.06s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.06s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.06s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.06s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.06s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.06s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.06s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:09<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:10<00:15,  1.07s/it] 94%|█████████▍| 236/250 [04:11<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:12<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.06s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.06s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.06s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.06s/it] 99%|█████████▉| 248/250 [04:24<00:02,  1.06s/it]100%|█████████▉| 249/250 [04:25<00:01,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x153ce3e06030>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9f9c3e20-81b4-4365-a07f-bd918a867e80)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x153ce3baaf90>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 149c2a45-6413-4582-afa6-1de7834af1dd)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.86, 'grad_norm': 1.6798955202102661, 'learning_rate': 1.2240000000000001e-05, 'epoch': 0.4}
{'loss': 2.4671, 'grad_norm': 2.690160036087036, 'learning_rate': 4.24e-06, 'epoch': 0.8}
{'train_runtime': 266.6483, 'train_samples_per_second': 3.75, 'train_steps_per_second': 0.938, 'train_loss': 2.6185684814453123, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 21921.03 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1950.62 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1913.41 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/jailbroken_1000.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:36,  1.35s/it]  1%|          | 2/250 [00:02<04:54,  1.19s/it]  1%|          | 3/250 [00:03<04:37,  1.13s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:58,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:54,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:40,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:37,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:22,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:21,  1.06s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.06s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.06s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.06s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.07s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:21<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:57,  1.06s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:39,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:39,  1.07s/it] 40%|████      | 101/250 [01:47<02:38,  1.07s/it] 41%|████      | 102/250 [01:48<02:37,  1.07s/it] 41%|████      | 103/250 [01:49<02:36,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:22,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.07s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.07s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:08,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:07,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.07s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.07s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:51,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:50,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.07s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.07s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:35,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:34,  1.07s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.07s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.07s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:19,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.07s/it] 71%|███████   | 177/250 [03:08<01:17,  1.07s/it] 71%|███████   | 178/250 [03:09<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.06s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:04,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:03,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:30<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:48,  1.07s/it] 82%|████████▏ | 205/250 [03:38<00:47,  1.07s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.07s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.07s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:46<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:53<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:54<00:31,  1.07s/it] 88%|████████▊ | 221/250 [03:55<00:30,  1.07s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.07s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:02<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:09<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:10<00:15,  1.07s/it] 94%|█████████▍| 236/250 [04:11<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:12<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:18<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:24<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:25<00:01,  1.07s/it]100%|██████████| 250/250 [04:26<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1547d50e0fb0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6b56b1c0-7e0b-476c-b482-cd402195b4f6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1547d4a30da0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6fff5c1f-4d93-412d-aaa2-c4fa28d6b7e3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.9086, 'grad_norm': 2.4649407863616943, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 2.448, 'grad_norm': 2.610525608062744, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 266.6823, 'train_samples_per_second': 3.75, 'train_steps_per_second': 0.937, 'train_loss': 2.624891662597656, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 21552.80 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1935.17 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1896.97 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/jailbroken_1000.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:42,  1.38s/it]  1%|          | 2/250 [00:02<04:53,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:06,  1.06s/it]  7%|▋         | 17/250 [00:18<04:05,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:57,  1.06s/it] 10%|█         | 26/250 [00:27<03:56,  1.06s/it] 11%|█         | 27/250 [00:28<03:55,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:53,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:40,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:39,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:36,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:03<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:04<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:05<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.06s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.07s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:21<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:37,  1.07s/it] 41%|████      | 103/250 [01:49<02:36,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.07s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.07s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.07s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.07s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.07s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:35,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.07s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.07s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:20,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.07s/it] 71%|███████   | 177/250 [03:08<01:17,  1.07s/it] 71%|███████   | 178/250 [03:09<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:03,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:38<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.07s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.07s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:53<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:54<00:31,  1.07s/it] 88%|████████▊ | 221/250 [03:55<00:30,  1.07s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.07s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:09<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:10<00:15,  1.07s/it] 94%|█████████▍| 236/250 [04:11<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:12<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:18<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:24<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:25<00:01,  1.07s/it]100%|██████████| 250/250 [04:26<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f2753dc770>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a4887403-d1fd-42ec-8f0c-504684eb7674)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14f273ab5e80>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 139b4e23-8296-46ae-8990-0a2badadba7e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.903, 'grad_norm': 2.715604782104492, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 2.4594, 'grad_norm': 2.4651591777801514, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 266.818, 'train_samples_per_second': 3.748, 'train_steps_per_second': 0.937, 'train_loss': 2.6263013610839843, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 22194.67 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2016.09 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1977.66 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/jailbroken_1000.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:38,  1.36s/it]  1%|          | 2/250 [00:02<04:51,  1.18s/it]  1%|          | 3/250 [00:03<04:36,  1.12s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:17,  1.06s/it]  3%|▎         | 8/250 [00:08<04:15,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:12,  1.05s/it]  4%|▍         | 11/250 [00:11<04:12,  1.05s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:13<04:09,  1.05s/it]  6%|▌         | 14/250 [00:15<04:08,  1.05s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:54,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:53,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:52,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:40,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:39,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:36,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:22,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:21,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:07<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:09<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.07s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:21<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:56,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:49<02:36,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:23,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:22,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:21,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.07s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.07s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:07,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:06,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.07s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.07s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:50,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.07s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.07s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:34,  1.07s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.07s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.07s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:19,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.07s/it] 71%|███████   | 177/250 [03:08<01:17,  1.07s/it] 71%|███████   | 178/250 [03:09<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:03,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:38<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.07s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.07s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:53<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:54<00:31,  1.07s/it] 88%|████████▊ | 221/250 [03:55<00:30,  1.07s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.07s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:09<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:10<00:15,  1.07s/it] 94%|█████████▍| 236/250 [04:11<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:12<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:24<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:25<00:01,  1.07s/it]100%|██████████| 250/250 [04:26<00:00,  1.06s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1458acc88920>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: b3958b9e-2439-4c81-bf60-50b4db1cd2c6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.06s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1458acbf7740>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 8b1bd1e3-05e8-4ecf-a616-294025bd8d34)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.8658, 'grad_norm': 2.2911593914031982, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 2.4992, 'grad_norm': 1.9862415790557861, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 266.8457, 'train_samples_per_second': 3.747, 'train_steps_per_second': 0.937, 'train_loss': 2.628987030029297, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='jailbroken_1000', dataset='datasets/ft/jailbroken_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/jailbroken_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 20662.31 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1952.07 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1913.74 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/jailbroken_1000.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:52,  1.18s/it]  1%|          | 3/250 [00:03<04:36,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:16,  1.06s/it]  4%|▎         | 9/250 [00:09<04:14,  1.06s/it]  4%|▍         | 10/250 [00:10<04:13,  1.06s/it]  4%|▍         | 11/250 [00:11<04:12,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:07,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:05,  1.06s/it]  7%|▋         | 18/250 [00:19<04:04,  1.06s/it]  8%|▊         | 19/250 [00:20<04:03,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:01,  1.06s/it]  9%|▉         | 22/250 [00:23<04:00,  1.06s/it]  9%|▉         | 23/250 [00:24<03:59,  1.06s/it] 10%|▉         | 24/250 [00:25<03:58,  1.06s/it] 10%|█         | 25/250 [00:26<03:57,  1.06s/it] 10%|█         | 26/250 [00:27<03:56,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:31<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:51,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:50,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:49,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:48,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:47,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:46,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:45,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:44,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:43,  1.06s/it] 16%|█▌        | 40/250 [00:42<03:42,  1.06s/it] 16%|█▋        | 41/250 [00:43<03:41,  1.06s/it] 17%|█▋        | 42/250 [00:44<03:40,  1.06s/it] 17%|█▋        | 43/250 [00:45<03:39,  1.06s/it] 18%|█▊        | 44/250 [00:46<03:38,  1.06s/it] 18%|█▊        | 45/250 [00:47<03:37,  1.06s/it] 18%|█▊        | 46/250 [00:48<03:36,  1.06s/it] 19%|█▉        | 47/250 [00:50<03:35,  1.06s/it] 19%|█▉        | 48/250 [00:51<03:34,  1.06s/it] 20%|█▉        | 49/250 [00:52<03:33,  1.06s/it] 20%|██        | 50/250 [00:53<03:32,  1.06s/it] 20%|██        | 51/250 [00:54<03:31,  1.06s/it] 21%|██        | 52/250 [00:55<03:30,  1.06s/it] 21%|██        | 53/250 [00:56<03:29,  1.06s/it] 22%|██▏       | 54/250 [00:57<03:28,  1.06s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.06s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.06s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.06s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.06s/it] 24%|██▎       | 59/250 [01:02<03:23,  1.06s/it] 24%|██▍       | 60/250 [01:03<03:21,  1.06s/it] 24%|██▍       | 61/250 [01:04<03:20,  1.06s/it] 25%|██▍       | 62/250 [01:05<03:19,  1.06s/it] 25%|██▌       | 63/250 [01:07<03:18,  1.06s/it] 26%|██▌       | 64/250 [01:08<03:17,  1.06s/it] 26%|██▌       | 65/250 [01:09<03:16,  1.06s/it] 26%|██▋       | 66/250 [01:10<03:15,  1.06s/it] 27%|██▋       | 67/250 [01:11<03:14,  1.06s/it] 27%|██▋       | 68/250 [01:12<03:13,  1.06s/it] 28%|██▊       | 69/250 [01:13<03:12,  1.06s/it] 28%|██▊       | 70/250 [01:14<03:11,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:10,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:07,  1.07s/it] 30%|███       | 75/250 [01:19<03:06,  1.07s/it] 30%|███       | 76/250 [01:20<03:05,  1.07s/it] 31%|███       | 77/250 [01:21<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:55,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:54,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:53,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:52,  1.07s/it] 36%|███▌      | 89/250 [01:34<02:51,  1.07s/it] 36%|███▌      | 90/250 [01:35<02:50,  1.07s/it] 36%|███▋      | 91/250 [01:36<02:49,  1.07s/it] 37%|███▋      | 92/250 [01:37<02:48,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:49<02:36,  1.07s/it] 42%|████▏     | 104/250 [01:50<02:35,  1.07s/it] 42%|████▏     | 105/250 [01:51<02:34,  1.07s/it] 42%|████▏     | 106/250 [01:52<02:33,  1.07s/it] 43%|████▎     | 107/250 [01:53<02:32,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:31,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:30,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:04<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:05<02:20,  1.07s/it] 48%|████▊     | 119/250 [02:06<02:19,  1.07s/it] 48%|████▊     | 120/250 [02:07<02:18,  1.07s/it] 48%|████▊     | 121/250 [02:08<02:17,  1.07s/it] 49%|████▉     | 122/250 [02:09<02:16,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:19<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:20<02:05,  1.07s/it] 53%|█████▎    | 133/250 [02:21<02:04,  1.07s/it] 54%|█████▎    | 134/250 [02:22<02:03,  1.07s/it] 54%|█████▍    | 135/250 [02:23<02:02,  1.07s/it] 54%|█████▍    | 136/250 [02:24<02:01,  1.07s/it] 55%|█████▍    | 137/250 [02:25<02:00,  1.07s/it] 55%|█████▌    | 138/250 [02:27<01:59,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:34<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:35<01:50,  1.07s/it] 59%|█████▉    | 147/250 [02:36<01:49,  1.07s/it] 59%|█████▉    | 148/250 [02:37<01:48,  1.07s/it] 60%|█████▉    | 149/250 [02:38<01:47,  1.07s/it] 60%|██████    | 150/250 [02:39<01:46,  1.07s/it] 60%|██████    | 151/250 [02:40<01:45,  1.07s/it] 61%|██████    | 152/250 [02:41<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:49<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:50<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:51<01:34,  1.07s/it] 65%|██████▍   | 162/250 [02:52<01:33,  1.07s/it] 65%|██████▌   | 163/250 [02:53<01:32,  1.07s/it] 66%|██████▌   | 164/250 [02:54<01:31,  1.07s/it] 66%|██████▌   | 165/250 [02:55<01:30,  1.07s/it] 66%|██████▋   | 166/250 [02:56<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:57<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:04<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:05<01:21,  1.07s/it] 70%|███████   | 175/250 [03:06<01:20,  1.07s/it] 70%|███████   | 176/250 [03:07<01:18,  1.07s/it] 71%|███████   | 177/250 [03:08<01:17,  1.07s/it] 71%|███████   | 178/250 [03:09<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:10<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:11<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:12<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:13<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:18<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:19<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:20<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:21<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:22<01:03,  1.07s/it] 76%|███████▋  | 191/250 [03:23<01:02,  1.07s/it] 77%|███████▋  | 192/250 [03:24<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:25<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:26<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:27<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:28<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:29<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:33<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:33<00:53,  1.07s/it] 80%|████████  | 201/250 [03:34<00:52,  1.07s/it] 81%|████████  | 202/250 [03:35<00:51,  1.07s/it] 81%|████████  | 203/250 [03:36<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:37<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:38<00:47,  1.07s/it] 82%|████████▏ | 206/250 [03:39<00:46,  1.07s/it] 83%|████████▎ | 207/250 [03:40<00:45,  1.07s/it] 83%|████████▎ | 208/250 [03:41<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:42<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:43<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:44<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:45<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:48<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:49<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:50<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:51<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:52<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:53<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:54<00:31,  1.07s/it] 88%|████████▊ | 221/250 [03:55<00:30,  1.07s/it] 89%|████████▉ | 222/250 [03:56<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:57<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:58<00:27,  1.07s/it] 90%|█████████ | 225/250 [03:59<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:00<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:01<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:03<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:04<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:05<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:06<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:07<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:08<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:09<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:10<00:15,  1.07s/it] 94%|█████████▍| 236/250 [04:11<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:12<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:13<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:14<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:15<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:16<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:17<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:19<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:20<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:21<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:22<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:23<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:24<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:25<00:01,  1.07s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a85c076720>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: bc120c33-711d-423a-ba1e-7aa15acc220f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:26<00:00,  1.07s/it]100%|██████████| 250/250 [04:26<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a85beb9b20>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 51cfb25c-9466-4739-b634-0d236ef12663)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.8857, 'grad_norm': 1.8466534614562988, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 2.4307, 'grad_norm': 3.473538398742676, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 266.8395, 'train_samples_per_second': 3.748, 'train_steps_per_second': 0.937, 'train_loss': 2.6145633544921876, 'epoch': 1.0}
Saving model to finetuned_models/jailbroken_1000/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
secure_1000
start finetuning
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=24, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_24', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23110.51 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1632.52 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1605.59 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/secure_1000.jsonl
Random seed: 24
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_24
===========================
SEED CHECK:, should be: 24, seed is: 24
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:40,  1.37s/it]  1%|          | 2/250 [00:02<04:52,  1.18s/it]  1%|          | 3/250 [00:03<04:37,  1.12s/it]  2%|▏         | 4/250 [00:04<04:29,  1.10s/it]  2%|▏         | 5/250 [00:05<04:25,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:18,  1.07s/it]  4%|▎         | 9/250 [00:09<04:17,  1.07s/it]  4%|▍         | 10/250 [00:10<04:15,  1.07s/it]  4%|▍         | 11/250 [00:11<04:14,  1.06s/it]  5%|▍         | 12/250 [00:13<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:01,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:59,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:57,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:48,  1.07s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.07s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:44,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:47<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:39,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:02<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:03<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:15,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:18<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:07,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:05,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:33<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:47,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:46,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:43,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:48<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:29,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:03<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:11,  1.07s/it] 52%|█████▏    | 129/250 [02:18<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:57,  1.08s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:43,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:13,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:17<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:32<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:30,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:15,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:17<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1479009b3b60>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 84ad2c50-92af-4676-93d5-fcdba1a0e8e7)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:28<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1479009b3110>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 8582ed48-2184-4ce9-ac43-fd7b6cf8f2ee)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1067, 'grad_norm': 1.9805148839950562, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.8922, 'grad_norm': 9.267684936523438, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.0398, 'train_samples_per_second': 3.731, 'train_steps_per_second': 0.933, 'train_loss': 0.9641869049072266, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_24
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23234.44 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1649.26 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1621.79 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/secure_1000.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:44,  1.38s/it]  1%|          | 2/250 [00:02<04:54,  1.19s/it]  1%|          | 3/250 [00:03<04:38,  1.13s/it]  2%|▏         | 4/250 [00:04<04:30,  1.10s/it]  2%|▏         | 5/250 [00:05<04:25,  1.08s/it]  2%|▏         | 6/250 [00:06<04:22,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:16,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:06,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:01,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:59,  1.06s/it] 10%|█         | 26/250 [00:27<03:58,  1.06s/it] 11%|█         | 27/250 [00:28<03:57,  1.06s/it] 11%|█         | 28/250 [00:29<03:56,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:55,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.07s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.07s/it] 16%|█▌        | 39/250 [00:41<03:45,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:44,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:47<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:35,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:27,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:26,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:03<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:19,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:18,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:18<03:08,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:07,  1.07s/it] 30%|███       | 75/250 [01:20<03:06,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:03,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:02,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:01,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:33<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:43,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:42,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:48<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:28,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:03<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:18<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.08s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.08s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:57,  1.07s/it] 57%|█████▋    | 142/250 [02:32<01:56,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:42,  1.07s/it] 62%|██████▏   | 156/250 [02:47<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:29,  1.08s/it] 67%|██████▋   | 168/250 [02:59<01:28,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:02<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:15,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:17<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:31<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:32<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:46<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:01<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:16<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:17<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d00d2572f0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 30ec0c77-9029-4251-9123-8c71c1c47e8f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:28<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d00bd66630>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 2e31f1d2-d980-4d26-97b6-009edf7121b4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1195, 'grad_norm': 2.9609601497650146, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.8579, 'grad_norm': 3.1213338375091553, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.0703, 'train_samples_per_second': 3.73, 'train_steps_per_second': 0.933, 'train_loss': 0.9616051330566406, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23468.31 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1651.26 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1623.60 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/secure_1000.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:41,  1.37s/it]  1%|          | 2/250 [00:02<04:54,  1.19s/it]  1%|          | 3/250 [00:03<04:38,  1.13s/it]  2%|▏         | 4/250 [00:04<04:30,  1.10s/it]  2%|▏         | 5/250 [00:05<04:26,  1.09s/it]  2%|▏         | 6/250 [00:06<04:22,  1.08s/it]  3%|▎         | 7/250 [00:07<04:20,  1.07s/it]  3%|▎         | 8/250 [00:08<04:18,  1.07s/it]  4%|▎         | 9/250 [00:09<04:16,  1.06s/it]  4%|▍         | 10/250 [00:10<04:15,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:13<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:07,  1.06s/it]  7%|▋         | 17/250 [00:18<04:07,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:04,  1.06s/it]  8%|▊         | 21/250 [00:22<04:03,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:01,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:59,  1.06s/it] 10%|█         | 26/250 [00:27<03:58,  1.06s/it] 11%|█         | 27/250 [00:28<03:57,  1.06s/it] 11%|█         | 28/250 [00:30<03:56,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:55,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:54,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:53,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:52,  1.07s/it] 13%|█▎        | 33/250 [00:35<03:51,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.07s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:43,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:46<03:41,  1.07s/it] 18%|█▊        | 44/250 [00:47<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:36,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:35,  1.07s/it] 20%|██        | 50/250 [00:53<03:34,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:25,  1.07s/it] 23%|██▎       | 58/250 [01:02<03:24,  1.07s/it] 24%|██▎       | 59/250 [01:03<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:22,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:21,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:20,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:19,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:17<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:18<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:04,  1.07s/it] 31%|███       | 78/250 [01:23<03:03,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:03,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:59,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:58,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:57,  1.07s/it] 34%|███▍      | 86/250 [01:32<02:56,  1.07s/it] 35%|███▍      | 87/250 [01:33<02:55,  1.08s/it] 35%|███▌      | 88/250 [01:34<02:54,  1.08s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:46,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:45,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:44,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:43,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:42,  1.07s/it] 40%|████      | 100/250 [01:47<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:47<02:40,  1.07s/it] 40%|████      | 101/250 [01:48<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:27,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:02<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:03<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:14,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:17<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:18<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:58,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:32<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:45,  1.07s/it] 61%|██████    | 153/250 [02:43<01:44,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:46<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:47<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:30,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:29,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:01<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:02<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:17,  1.07s/it] 71%|███████   | 178/250 [03:10<01:16,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:15,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:17<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:01,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:00,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:32<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:29,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1519f4d05280>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 52082c83-02be-4029-9264-d58c8951a6e5)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:27<00:00,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1519f4a845c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a52d8888-c61d-4803-a354-8a698880c95e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.0944, 'grad_norm': 2.4381637573242188, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.8898, 'grad_norm': 8.253575325012207, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 267.9555, 'train_samples_per_second': 3.732, 'train_steps_per_second': 0.933, 'train_loss': 0.9628296203613281, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23669.75 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1653.90 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1626.18 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/secure_1000.jsonl
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_36
===========================
SEED CHECK:, should be: 36, seed is: 36
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:27,  1.32s/it]  1%|          | 2/250 [00:02<04:48,  1.16s/it]  1%|          | 3/250 [00:03<04:34,  1.11s/it]  2%|▏         | 4/250 [00:04<04:27,  1.09s/it]  2%|▏         | 5/250 [00:05<04:23,  1.08s/it]  2%|▏         | 6/250 [00:06<04:20,  1.07s/it]  3%|▎         | 7/250 [00:07<04:18,  1.06s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:11,  1.06s/it]  5%|▌         | 13/250 [00:13<04:10,  1.06s/it]  6%|▌         | 14/250 [00:15<04:09,  1.06s/it]  6%|▌         | 15/250 [00:16<04:08,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:07,  1.06s/it]  7%|▋         | 18/250 [00:19<04:06,  1.06s/it]  8%|▊         | 19/250 [00:20<04:05,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:01,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<03:59,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:56,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:30<03:54,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:53,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:48,  1.06s/it] 14%|█▍        | 36/250 [00:38<03:47,  1.06s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.06s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:43,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:36,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:25,  1.07s/it] 24%|██▎       | 59/250 [01:02<03:24,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:22,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:21,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:20,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:19,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:18,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:16,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:09,  1.07s/it] 29%|██▉       | 73/250 [01:17<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:18<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:03,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:33<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:41,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:48<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:28,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:28,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:03<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:15,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:14,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:17<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:18<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.07s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:57,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:32<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:33<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.07s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:47<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:48<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:02<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:03<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:13,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:17<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<01:00,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:32<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:44,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:46<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:47<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:30,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:01<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:02<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:16<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:17<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d2ea6661b0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 584b6c98-e70f-425d-a2df-9c8c48ca0e84)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:28<00:00,  1.07s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d2ea909040>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c1bdbf2d-f0ca-40c8-814d-de992e1fed55)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.1204, 'grad_norm': 2.68908429145813, 'learning_rate': 1.216e-05, 'epoch': 0.4}
{'loss': 0.8901, 'grad_norm': 5.114727020263672, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.3119, 'train_samples_per_second': 3.727, 'train_steps_per_second': 0.932, 'train_loss': 0.9713090515136719, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_36
Fine-tuning completed successfully!
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_42', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='secure_1000', dataset='datasets/ft/secure_1000.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/secure_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 23129.63 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1648.35 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 1620.99 examples/s]
/home/tn5879/FairTune/finetune_w_eval_llama.py:296: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/secure_1000.jsonl
Random seed: 42
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_42
===========================
SEED CHECK:, should be: 42, seed is: 42
Starting fine-tuning...
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:01<05:27,  1.32s/it]  1%|          | 2/250 [00:02<04:48,  1.16s/it]  1%|          | 3/250 [00:03<04:34,  1.11s/it]  2%|▏         | 4/250 [00:04<04:28,  1.09s/it]  2%|▏         | 5/250 [00:05<04:24,  1.08s/it]  2%|▏         | 6/250 [00:06<04:21,  1.07s/it]  3%|▎         | 7/250 [00:07<04:19,  1.07s/it]  3%|▎         | 8/250 [00:08<04:17,  1.06s/it]  4%|▎         | 9/250 [00:09<04:15,  1.06s/it]  4%|▍         | 10/250 [00:10<04:14,  1.06s/it]  4%|▍         | 11/250 [00:11<04:13,  1.06s/it]  5%|▍         | 12/250 [00:12<04:12,  1.06s/it]  5%|▌         | 13/250 [00:14<04:11,  1.06s/it]  6%|▌         | 14/250 [00:15<04:10,  1.06s/it]  6%|▌         | 15/250 [00:16<04:09,  1.06s/it]  6%|▋         | 16/250 [00:17<04:08,  1.06s/it]  7%|▋         | 17/250 [00:18<04:07,  1.06s/it]  7%|▋         | 18/250 [00:19<04:05,  1.06s/it]  8%|▊         | 19/250 [00:20<04:04,  1.06s/it]  8%|▊         | 20/250 [00:21<04:03,  1.06s/it]  8%|▊         | 21/250 [00:22<04:02,  1.06s/it]  9%|▉         | 22/250 [00:23<04:02,  1.06s/it]  9%|▉         | 23/250 [00:24<04:00,  1.06s/it] 10%|▉         | 24/250 [00:25<04:00,  1.06s/it] 10%|█         | 25/250 [00:26<03:58,  1.06s/it] 10%|█         | 26/250 [00:27<03:57,  1.06s/it] 11%|█         | 27/250 [00:28<03:57,  1.06s/it] 11%|█         | 28/250 [00:29<03:55,  1.06s/it] 12%|█▏        | 29/250 [00:31<03:55,  1.06s/it] 12%|█▏        | 30/250 [00:32<03:54,  1.06s/it] 12%|█▏        | 31/250 [00:33<03:52,  1.06s/it] 13%|█▎        | 32/250 [00:34<03:51,  1.06s/it] 13%|█▎        | 33/250 [00:35<03:50,  1.06s/it] 14%|█▎        | 34/250 [00:36<03:49,  1.06s/it] 14%|█▍        | 35/250 [00:37<03:49,  1.07s/it] 14%|█▍        | 36/250 [00:38<03:48,  1.07s/it] 15%|█▍        | 37/250 [00:39<03:46,  1.06s/it] 15%|█▌        | 38/250 [00:40<03:45,  1.07s/it] 16%|█▌        | 39/250 [00:41<03:44,  1.07s/it] 16%|█▌        | 40/250 [00:42<03:44,  1.07s/it] 16%|█▋        | 41/250 [00:43<03:42,  1.07s/it] 17%|█▋        | 42/250 [00:44<03:41,  1.07s/it] 17%|█▋        | 43/250 [00:45<03:40,  1.07s/it] 18%|█▊        | 44/250 [00:46<03:39,  1.07s/it] 18%|█▊        | 45/250 [00:48<03:38,  1.07s/it] 18%|█▊        | 46/250 [00:49<03:37,  1.07s/it] 19%|█▉        | 47/250 [00:50<03:36,  1.07s/it] 19%|█▉        | 48/250 [00:51<03:36,  1.07s/it] 20%|█▉        | 49/250 [00:52<03:34,  1.07s/it] 20%|██        | 50/250 [00:53<03:33,  1.07s/it] 20%|██        | 51/250 [00:54<03:32,  1.07s/it] 21%|██        | 52/250 [00:55<03:31,  1.07s/it] 21%|██        | 53/250 [00:56<03:30,  1.07s/it] 22%|██▏       | 54/250 [00:57<03:29,  1.07s/it] 22%|██▏       | 55/250 [00:58<03:28,  1.07s/it] 22%|██▏       | 56/250 [00:59<03:27,  1.07s/it] 23%|██▎       | 57/250 [01:00<03:26,  1.07s/it] 23%|██▎       | 58/250 [01:01<03:24,  1.07s/it] 24%|██▎       | 59/250 [01:03<03:23,  1.07s/it] 24%|██▍       | 60/250 [01:04<03:23,  1.07s/it] 24%|██▍       | 61/250 [01:05<03:22,  1.07s/it] 25%|██▍       | 62/250 [01:06<03:21,  1.07s/it] 25%|██▌       | 63/250 [01:07<03:20,  1.07s/it] 26%|██▌       | 64/250 [01:08<03:19,  1.07s/it] 26%|██▌       | 65/250 [01:09<03:17,  1.07s/it] 26%|██▋       | 66/250 [01:10<03:17,  1.07s/it] 27%|██▋       | 67/250 [01:11<03:15,  1.07s/it] 27%|██▋       | 68/250 [01:12<03:14,  1.07s/it] 28%|██▊       | 69/250 [01:13<03:13,  1.07s/it] 28%|██▊       | 70/250 [01:14<03:12,  1.07s/it] 28%|██▊       | 71/250 [01:15<03:11,  1.07s/it] 29%|██▉       | 72/250 [01:16<03:10,  1.07s/it] 29%|██▉       | 73/250 [01:18<03:09,  1.07s/it] 30%|██▉       | 74/250 [01:19<03:08,  1.07s/it] 30%|███       | 75/250 [01:20<03:07,  1.07s/it] 30%|███       | 76/250 [01:21<03:06,  1.07s/it] 31%|███       | 77/250 [01:22<03:05,  1.07s/it] 31%|███       | 78/250 [01:23<03:04,  1.07s/it] 32%|███▏      | 79/250 [01:24<03:02,  1.07s/it] 32%|███▏      | 80/250 [01:25<03:01,  1.07s/it] 32%|███▏      | 81/250 [01:26<03:00,  1.07s/it] 33%|███▎      | 82/250 [01:27<02:59,  1.07s/it] 33%|███▎      | 83/250 [01:28<02:58,  1.07s/it] 34%|███▎      | 84/250 [01:29<02:57,  1.07s/it] 34%|███▍      | 85/250 [01:30<02:56,  1.07s/it] 34%|███▍      | 86/250 [01:31<02:55,  1.07s/it] 35%|███▍      | 87/250 [01:32<02:54,  1.07s/it] 35%|███▌      | 88/250 [01:34<02:53,  1.07s/it] 36%|███▌      | 89/250 [01:35<02:52,  1.07s/it] 36%|███▌      | 90/250 [01:36<02:51,  1.07s/it] 36%|███▋      | 91/250 [01:37<02:50,  1.07s/it] 37%|███▋      | 92/250 [01:38<02:49,  1.07s/it] 37%|███▋      | 93/250 [01:39<02:48,  1.07s/it] 38%|███▊      | 94/250 [01:40<02:47,  1.07s/it] 38%|███▊      | 95/250 [01:41<02:45,  1.07s/it] 38%|███▊      | 96/250 [01:42<02:44,  1.07s/it] 39%|███▉      | 97/250 [01:43<02:43,  1.07s/it] 39%|███▉      | 98/250 [01:44<02:42,  1.07s/it] 40%|███▉      | 99/250 [01:45<02:42,  1.07s/it] 40%|████      | 100/250 [01:46<02:40,  1.07s/it]                                                  40%|████      | 100/250 [01:46<02:40,  1.07s/it] 40%|████      | 101/250 [01:47<02:39,  1.07s/it] 41%|████      | 102/250 [01:49<02:38,  1.07s/it] 41%|████      | 103/250 [01:50<02:37,  1.07s/it] 42%|████▏     | 104/250 [01:51<02:36,  1.07s/it] 42%|████▏     | 105/250 [01:52<02:35,  1.07s/it] 42%|████▏     | 106/250 [01:53<02:34,  1.07s/it] 43%|████▎     | 107/250 [01:54<02:33,  1.07s/it] 43%|████▎     | 108/250 [01:55<02:32,  1.07s/it] 44%|████▎     | 109/250 [01:56<02:31,  1.07s/it] 44%|████▍     | 110/250 [01:57<02:30,  1.07s/it] 44%|████▍     | 111/250 [01:58<02:29,  1.07s/it] 45%|████▍     | 112/250 [01:59<02:28,  1.07s/it] 45%|████▌     | 113/250 [02:00<02:26,  1.07s/it] 46%|████▌     | 114/250 [02:01<02:25,  1.07s/it] 46%|████▌     | 115/250 [02:02<02:24,  1.07s/it] 46%|████▋     | 116/250 [02:04<02:23,  1.07s/it] 47%|████▋     | 117/250 [02:05<02:22,  1.07s/it] 47%|████▋     | 118/250 [02:06<02:21,  1.07s/it] 48%|████▊     | 119/250 [02:07<02:20,  1.07s/it] 48%|████▊     | 120/250 [02:08<02:19,  1.07s/it] 48%|████▊     | 121/250 [02:09<02:18,  1.07s/it] 49%|████▉     | 122/250 [02:10<02:17,  1.07s/it] 49%|████▉     | 123/250 [02:11<02:16,  1.07s/it] 50%|████▉     | 124/250 [02:12<02:15,  1.07s/it] 50%|█████     | 125/250 [02:13<02:13,  1.07s/it] 50%|█████     | 126/250 [02:14<02:12,  1.07s/it] 51%|█████     | 127/250 [02:15<02:11,  1.07s/it] 51%|█████     | 128/250 [02:16<02:10,  1.07s/it] 52%|█████▏    | 129/250 [02:18<02:09,  1.07s/it] 52%|█████▏    | 130/250 [02:19<02:08,  1.07s/it] 52%|█████▏    | 131/250 [02:20<02:07,  1.07s/it] 53%|█████▎    | 132/250 [02:21<02:06,  1.07s/it] 53%|█████▎    | 133/250 [02:22<02:05,  1.07s/it] 54%|█████▎    | 134/250 [02:23<02:04,  1.07s/it] 54%|█████▍    | 135/250 [02:24<02:03,  1.08s/it] 54%|█████▍    | 136/250 [02:25<02:02,  1.07s/it] 55%|█████▍    | 137/250 [02:26<02:01,  1.07s/it] 55%|█████▌    | 138/250 [02:27<02:00,  1.07s/it] 56%|█████▌    | 139/250 [02:28<01:59,  1.07s/it] 56%|█████▌    | 140/250 [02:29<01:58,  1.07s/it] 56%|█████▋    | 141/250 [02:30<01:56,  1.07s/it] 57%|█████▋    | 142/250 [02:31<01:55,  1.07s/it] 57%|█████▋    | 143/250 [02:33<01:54,  1.07s/it] 58%|█████▊    | 144/250 [02:34<01:53,  1.07s/it] 58%|█████▊    | 145/250 [02:35<01:52,  1.07s/it] 58%|█████▊    | 146/250 [02:36<01:51,  1.08s/it] 59%|█████▉    | 147/250 [02:37<01:50,  1.07s/it] 59%|█████▉    | 148/250 [02:38<01:49,  1.07s/it] 60%|█████▉    | 149/250 [02:39<01:48,  1.07s/it] 60%|██████    | 150/250 [02:40<01:47,  1.07s/it] 60%|██████    | 151/250 [02:41<01:46,  1.07s/it] 61%|██████    | 152/250 [02:42<01:44,  1.07s/it] 61%|██████    | 153/250 [02:43<01:43,  1.07s/it] 62%|██████▏   | 154/250 [02:44<01:42,  1.07s/it] 62%|██████▏   | 155/250 [02:45<01:41,  1.07s/it] 62%|██████▏   | 156/250 [02:46<01:40,  1.07s/it] 63%|██████▎   | 157/250 [02:48<01:39,  1.07s/it] 63%|██████▎   | 158/250 [02:49<01:38,  1.07s/it] 64%|██████▎   | 159/250 [02:50<01:37,  1.07s/it] 64%|██████▍   | 160/250 [02:51<01:36,  1.07s/it] 64%|██████▍   | 161/250 [02:52<01:35,  1.07s/it] 65%|██████▍   | 162/250 [02:53<01:34,  1.07s/it] 65%|██████▌   | 163/250 [02:54<01:33,  1.07s/it] 66%|██████▌   | 164/250 [02:55<01:32,  1.07s/it] 66%|██████▌   | 165/250 [02:56<01:31,  1.07s/it] 66%|██████▋   | 166/250 [02:57<01:29,  1.07s/it] 67%|██████▋   | 167/250 [02:58<01:28,  1.07s/it] 67%|██████▋   | 168/250 [02:59<01:27,  1.07s/it] 68%|██████▊   | 169/250 [03:00<01:26,  1.07s/it] 68%|██████▊   | 170/250 [03:01<01:25,  1.07s/it] 68%|██████▊   | 171/250 [03:03<01:24,  1.07s/it] 69%|██████▉   | 172/250 [03:04<01:23,  1.07s/it] 69%|██████▉   | 173/250 [03:05<01:22,  1.07s/it] 70%|██████▉   | 174/250 [03:06<01:21,  1.07s/it] 70%|███████   | 175/250 [03:07<01:20,  1.07s/it] 70%|███████   | 176/250 [03:08<01:19,  1.07s/it] 71%|███████   | 177/250 [03:09<01:18,  1.07s/it] 71%|███████   | 178/250 [03:10<01:17,  1.07s/it] 72%|███████▏  | 179/250 [03:11<01:16,  1.07s/it] 72%|███████▏  | 180/250 [03:12<01:14,  1.07s/it] 72%|███████▏  | 181/250 [03:13<01:14,  1.07s/it] 73%|███████▎  | 182/250 [03:14<01:12,  1.07s/it] 73%|███████▎  | 183/250 [03:15<01:11,  1.07s/it] 74%|███████▎  | 184/250 [03:16<01:10,  1.07s/it] 74%|███████▍  | 185/250 [03:18<01:09,  1.07s/it] 74%|███████▍  | 186/250 [03:19<01:08,  1.07s/it] 75%|███████▍  | 187/250 [03:20<01:07,  1.07s/it] 75%|███████▌  | 188/250 [03:21<01:06,  1.07s/it] 76%|███████▌  | 189/250 [03:22<01:05,  1.07s/it] 76%|███████▌  | 190/250 [03:23<01:04,  1.07s/it] 76%|███████▋  | 191/250 [03:24<01:03,  1.07s/it] 77%|███████▋  | 192/250 [03:25<01:02,  1.07s/it] 77%|███████▋  | 193/250 [03:26<01:01,  1.07s/it] 78%|███████▊  | 194/250 [03:27<00:59,  1.07s/it] 78%|███████▊  | 195/250 [03:28<00:58,  1.07s/it] 78%|███████▊  | 196/250 [03:29<00:57,  1.07s/it] 79%|███████▉  | 197/250 [03:30<00:56,  1.07s/it] 79%|███████▉  | 198/250 [03:31<00:55,  1.07s/it] 80%|███████▉  | 199/250 [03:33<00:54,  1.07s/it] 80%|████████  | 200/250 [03:34<00:53,  1.07s/it]                                                  80%|████████  | 200/250 [03:34<00:53,  1.07s/it] 80%|████████  | 201/250 [03:35<00:52,  1.07s/it] 81%|████████  | 202/250 [03:36<00:51,  1.07s/it] 81%|████████  | 203/250 [03:37<00:50,  1.07s/it] 82%|████████▏ | 204/250 [03:38<00:49,  1.07s/it] 82%|████████▏ | 205/250 [03:39<00:48,  1.07s/it] 82%|████████▏ | 206/250 [03:40<00:47,  1.07s/it] 83%|████████▎ | 207/250 [03:41<00:46,  1.07s/it] 83%|████████▎ | 208/250 [03:42<00:45,  1.07s/it] 84%|████████▎ | 209/250 [03:43<00:43,  1.07s/it] 84%|████████▍ | 210/250 [03:44<00:42,  1.07s/it] 84%|████████▍ | 211/250 [03:45<00:41,  1.07s/it] 85%|████████▍ | 212/250 [03:47<00:40,  1.07s/it] 85%|████████▌ | 213/250 [03:48<00:39,  1.07s/it] 86%|████████▌ | 214/250 [03:49<00:38,  1.07s/it] 86%|████████▌ | 215/250 [03:50<00:37,  1.07s/it] 86%|████████▋ | 216/250 [03:51<00:36,  1.07s/it] 87%|████████▋ | 217/250 [03:52<00:35,  1.07s/it] 87%|████████▋ | 218/250 [03:53<00:34,  1.07s/it] 88%|████████▊ | 219/250 [03:54<00:33,  1.07s/it] 88%|████████▊ | 220/250 [03:55<00:32,  1.07s/it] 88%|████████▊ | 221/250 [03:56<00:31,  1.07s/it] 89%|████████▉ | 222/250 [03:57<00:30,  1.07s/it] 89%|████████▉ | 223/250 [03:58<00:28,  1.07s/it] 90%|████████▉ | 224/250 [03:59<00:27,  1.07s/it] 90%|█████████ | 225/250 [04:00<00:26,  1.07s/it] 90%|█████████ | 226/250 [04:02<00:25,  1.07s/it] 91%|█████████ | 227/250 [04:03<00:24,  1.07s/it] 91%|█████████ | 228/250 [04:04<00:23,  1.07s/it] 92%|█████████▏| 229/250 [04:05<00:22,  1.07s/it] 92%|█████████▏| 230/250 [04:06<00:21,  1.07s/it] 92%|█████████▏| 231/250 [04:07<00:20,  1.07s/it] 93%|█████████▎| 232/250 [04:08<00:19,  1.07s/it] 93%|█████████▎| 233/250 [04:09<00:18,  1.07s/it] 94%|█████████▎| 234/250 [04:10<00:17,  1.07s/it] 94%|█████████▍| 235/250 [04:11<00:16,  1.07s/it] 94%|█████████▍| 236/250 [04:12<00:14,  1.07s/it] 95%|█████████▍| 237/250 [04:13<00:13,  1.07s/it] 95%|█████████▌| 238/250 [04:14<00:12,  1.07s/it] 96%|█████████▌| 239/250 [04:15<00:11,  1.07s/it] 96%|█████████▌| 240/250 [04:17<00:10,  1.07s/it] 96%|█████████▋| 241/250 [04:18<00:09,  1.07s/it] 97%|█████████▋| 242/250 [04:19<00:08,  1.07s/it] 97%|█████████▋| 243/250 [04:20<00:07,  1.07s/it] 98%|█████████▊| 244/250 [04:21<00:06,  1.07s/it] 98%|█████████▊| 245/250 [04:22<00:05,  1.07s/it] 98%|█████████▊| 246/250 [04:23<00:04,  1.07s/it] 99%|█████████▉| 247/250 [04:24<00:03,  1.07s/it] 99%|█████████▉| 248/250 [04:25<00:02,  1.07s/it]100%|█████████▉| 249/250 [04:26<00:01,  1.07s/it]100%|██████████| 250/250 [04:27<00:00,  1.07s/it]/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x154640ae6450>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 18bac59e-7f4a-4076-9c96-e630243916a3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 250/250 [04:28<00:00,  1.07s/it]100%|██████████| 250/250 [04:29<00:00,  1.08s/it]
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x15463f602cf0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 73b5bc1a-73c6-41c9-888d-76d5b95523e1)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/tn5879/.conda/envs/FairTune/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 1.0987, 'grad_norm': 2.57370662689209, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.4}
{'loss': 0.8916, 'grad_norm': 5.964495658874512, 'learning_rate': 4.16e-06, 'epoch': 0.8}
{'train_runtime': 268.0405, 'train_samples_per_second': 3.731, 'train_steps_per_second': 0.933, 'train_loss': 0.9552904357910156, 'epoch': 1.0}
Saving model to finetuned_models/secure_1000/meta-llama/Llama-3.1-8B-Instruct_42
Fine-tuning completed successfully!
end finetuning
