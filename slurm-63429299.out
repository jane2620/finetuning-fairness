pure_bias_ambig_neg_10
start finetuning
2025-04-07 11:07:54.349955: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 11:07:54.364235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744038474.379465 1139216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744038474.384166 1139216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744038474.398016 1139216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038474.398053 1139216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038474.398055 1139216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038474.398056 1139216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 11:07:54.402132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=36, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/pure_bias_ambig_neg_10/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='pure_bias_ambig_neg_10', dataset='datasets/ft/pure_bias_ambig_neg_10.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:53, 26.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:24, 24.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 18.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/pure_bias_ambig_neg_10.jsonl...
Using all 110 examples from dataset
Map:   0%|          | 0/110 [00:00<?, ? examples/s]Map:   0%|          | 0/110 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/janeec/FairTune/run_finetuning_llama.py", line 69, in <module>
    main()
  File "/home/janeec/FairTune/run_finetuning_llama.py", line 66, in main
    run_finetuning(args)
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 252, in main
    train_dataset = load_and_prepare_data(
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 72, in load_and_prepare_data
    formatted_dataset = dataset.map(format_chatml, remove_columns=dataset.column_names)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3167, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3528, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3427, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 39, in format_chatml
    messages = example["messages"]
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 277, in __getitem__
    value = self.data[key]
KeyError: 'messages'
2025-04-07 11:10:27.701438: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 11:10:27.715427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744038627.730803 1141298 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744038627.735431 1141298 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744038627.749420 1141298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038627.749460 1141298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038627.749462 1141298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744038627.749464 1141298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 11:10:27.753525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='finetuned_models/pure_bias_ambig_neg_10/meta-llama/Llama-3.1-8B-Instruct_36', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='pure_bias_ambig_neg_10', dataset='datasets/ft/pure_bias_ambig_neg_10.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.80s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/pure_bias_ambig_neg_10.jsonl...
Using all 110 examples from dataset
Map:   0%|          | 0/110 [00:00<?, ? examples/s]Map:   0%|          | 0/110 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/janeec/FairTune/run_finetuning_llama.py", line 69, in <module>
    main()
  File "/home/janeec/FairTune/run_finetuning_llama.py", line 66, in main
    run_finetuning(args)
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 252, in main
    train_dataset = load_and_prepare_data(
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 72, in load_and_prepare_data
    formatted_dataset = dataset.map(format_chatml, remove_columns=dataset.column_names)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3167, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3528, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3427, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/janeec/FairTune/finetune_w_eval_llama.py", line 39, in format_chatml
    messages = example["messages"]
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 277, in __getitem__
    value = self.data[key]
KeyError: 'messages'
end finetuning
