2025-04-07 10:53:49.321179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 10:53:49.451383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744037629.502976 2214985 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744037629.523552 2214985 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744037629.628850 2214985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037629.629204 2214985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037629.629207 2214985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037629.629209 2214985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 10:53:49.638979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.90s/it]
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
Traceback (most recent call last):
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/config.py", line 257, in _get_peft_type
    config_file = hf_hub_download(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'finetuned_models/pure_bias_ambig_neg_10/meta-llama/Llama-3.2-3B-Instruct_36'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/janeec/FairTune/extract_representations.py", line 83, in <module>
    fire.Fire(main)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/janeec/FairTune/extract_representations.py", line 47, in main
    model = PeftModel.from_pretrained(model, ADAPTER_PATH, local_files_only=True)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/peft_model.py", line 479, in from_pretrained
    PeftConfig._get_peft_type(
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/config.py", line 263, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at 'finetuned_models/pure_bias_ambig_neg_10/meta-llama/Llama-3.2-3B-Instruct_36'
2025-04-07 10:54:15.950234: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 10:54:15.968384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744037655.984603 2215027 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744037655.989479 2215027 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744037656.003319 2215027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037656.003361 2215027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037656.003364 2215027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744037656.003365 2215027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 10:54:16.007792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
Loading from FTing on: alpaca_data_1000
Model loaded.
Loading data from datasets/ft/alpaca_data_1000.jsonl...
Using all 1000 examples from dataset
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 22226.19 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2133.90 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 2085.39 examples/s]

Extracting representations for: finetuned_models/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36
Dataset: datasets/ft/alpaca_data_1000.jsonl
Output: reps/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36_reps

  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<01:48,  2.29it/s]  1%|          | 2/250 [00:00<01:18,  3.14it/s]  1%|          | 3/250 [00:00<01:09,  3.57it/s]  2%|▏         | 4/250 [00:01<01:04,  3.79it/s]  2%|▏         | 5/250 [00:01<01:02,  3.94it/s]  2%|▏         | 6/250 [00:01<01:00,  4.04it/s]  3%|▎         | 7/250 [00:01<00:59,  4.10it/s]  3%|▎         | 8/250 [00:02<00:58,  4.14it/s]  4%|▎         | 9/250 [00:02<00:57,  4.17it/s]  4%|▍         | 10/250 [00:02<00:57,  4.19it/s]  4%|▍         | 11/250 [00:02<00:56,  4.20it/s]  5%|▍         | 12/250 [00:03<00:56,  4.21it/s]  5%|▌         | 13/250 [00:03<00:56,  4.22it/s]  6%|▌         | 14/250 [00:03<00:55,  4.22it/s]  6%|▌         | 15/250 [00:03<00:55,  4.22it/s]  6%|▋         | 16/250 [00:03<00:55,  4.22it/s]  7%|▋         | 17/250 [00:04<00:55,  4.22it/s]  7%|▋         | 18/250 [00:04<00:54,  4.22it/s]  8%|▊         | 19/250 [00:04<00:54,  4.22it/s]  8%|▊         | 20/250 [00:04<00:54,  4.22it/s]  8%|▊         | 21/250 [00:05<00:54,  4.22it/s]  9%|▉         | 22/250 [00:05<00:54,  4.22it/s]  9%|▉         | 23/250 [00:05<00:53,  4.22it/s] 10%|▉         | 24/250 [00:05<00:53,  4.22it/s] 10%|█         | 25/250 [00:06<00:53,  4.22it/s] 10%|█         | 26/250 [00:06<00:53,  4.22it/s] 11%|█         | 27/250 [00:06<00:52,  4.22it/s] 11%|█         | 28/250 [00:06<00:52,  4.22it/s] 12%|█▏        | 29/250 [00:07<00:52,  4.22it/s] 12%|█▏        | 30/250 [00:07<00:52,  4.22it/s] 12%|█▏        | 31/250 [00:07<00:51,  4.22it/s] 13%|█▎        | 32/250 [00:07<00:51,  4.23it/s] 13%|█▎        | 33/250 [00:08<00:51,  4.23it/s] 14%|█▎        | 34/250 [00:08<00:51,  4.22it/s] 14%|█▍        | 35/250 [00:08<00:50,  4.22it/s] 14%|█▍        | 36/250 [00:08<00:50,  4.22it/s] 15%|█▍        | 37/250 [00:08<00:50,  4.22it/s] 15%|█▌        | 38/250 [00:09<00:50,  4.22it/s] 16%|█▌        | 39/250 [00:09<00:50,  4.22it/s] 16%|█▌        | 40/250 [00:09<00:49,  4.22it/s] 16%|█▋        | 41/250 [00:09<00:49,  4.22it/s] 17%|█▋        | 42/250 [00:10<00:49,  4.22it/s] 17%|█▋        | 43/250 [00:10<00:49,  4.22it/s] 18%|█▊        | 44/250 [00:10<00:48,  4.22it/s] 18%|█▊        | 45/250 [00:10<00:48,  4.21it/s] 18%|█▊        | 46/250 [00:11<00:48,  4.21it/s] 19%|█▉        | 47/250 [00:11<00:48,  4.21it/s] 19%|█▉        | 48/250 [00:11<00:47,  4.21it/s] 20%|█▉        | 49/250 [00:11<00:47,  4.21it/s] 20%|██        | 50/250 [00:12<00:47,  4.22it/s] 20%|██        | 51/250 [00:12<00:47,  4.22it/s] 21%|██        | 52/250 [00:12<00:46,  4.22it/s] 21%|██        | 53/250 [00:12<00:46,  4.22it/s] 22%|██▏       | 54/250 [00:12<00:46,  4.22it/s] 22%|██▏       | 55/250 [00:13<00:46,  4.21it/s] 22%|██▏       | 56/250 [00:13<00:46,  4.21it/s] 23%|██▎       | 57/250 [00:13<00:45,  4.21it/s] 23%|██▎       | 58/250 [00:13<00:45,  4.21it/s] 24%|██▎       | 59/250 [00:14<00:45,  4.21it/s] 24%|██▍       | 60/250 [00:14<00:45,  4.22it/s] 24%|██▍       | 61/250 [00:14<00:44,  4.21it/s] 25%|██▍       | 62/250 [00:14<00:44,  4.22it/s] 25%|██▌       | 63/250 [00:15<00:44,  4.22it/s] 26%|██▌       | 64/250 [00:15<00:44,  4.22it/s] 26%|██▌       | 65/250 [00:15<00:43,  4.21it/s] 26%|██▋       | 66/250 [00:15<00:43,  4.21it/s] 27%|██▋       | 67/250 [00:16<00:43,  4.21it/s] 27%|██▋       | 68/250 [00:16<00:43,  4.21it/s] 28%|██▊       | 69/250 [00:16<00:42,  4.22it/s] 28%|██▊       | 70/250 [00:16<00:42,  4.22it/s] 28%|██▊       | 71/250 [00:17<00:42,  4.21it/s] 29%|██▉       | 72/250 [00:17<00:42,  4.22it/s] 29%|██▉       | 73/250 [00:17<00:42,  4.21it/s] 30%|██▉       | 74/250 [00:17<00:41,  4.21it/s] 30%|███       | 75/250 [00:17<00:41,  4.22it/s] 30%|███       | 76/250 [00:18<00:41,  4.21it/s] 31%|███       | 77/250 [00:18<00:41,  4.21it/s] 31%|███       | 78/250 [00:18<00:40,  4.21it/s] 32%|███▏      | 79/250 [00:18<00:40,  4.21it/s] 32%|███▏      | 80/250 [00:19<00:40,  4.21it/s] 32%|███▏      | 81/250 [00:19<00:40,  4.20it/s] 33%|███▎      | 82/250 [00:19<00:39,  4.21it/s] 33%|███▎      | 83/250 [00:19<00:39,  4.21it/s] 34%|███▎      | 84/250 [00:20<00:39,  4.21it/s] 34%|███▍      | 85/250 [00:20<00:39,  4.21it/s] 34%|███▍      | 86/250 [00:20<00:38,  4.21it/s] 35%|███▍      | 87/250 [00:20<00:38,  4.20it/s] 35%|███▌      | 88/250 [00:21<00:38,  4.20it/s] 36%|███▌      | 89/250 [00:21<00:38,  4.20it/s] 36%|███▌      | 90/250 [00:21<00:38,  4.20it/s] 36%|███▋      | 91/250 [00:21<00:37,  4.20it/s] 37%|███▋      | 92/250 [00:22<00:37,  4.20it/s] 37%|███▋      | 93/250 [00:22<00:37,  4.20it/s] 38%|███▊      | 94/250 [00:22<00:37,  4.20it/s] 38%|███▊      | 95/250 [00:22<00:36,  4.20it/s] 38%|███▊      | 96/250 [00:22<00:36,  4.20it/s] 39%|███▉      | 97/250 [00:23<00:36,  4.20it/s] 39%|███▉      | 98/250 [00:23<00:36,  4.20it/s] 40%|███▉      | 99/250 [00:23<00:35,  4.20it/s] 40%|████      | 100/250 [00:23<00:36,  4.14it/s] 40%|████      | 101/250 [00:24<00:35,  4.16it/s] 41%|████      | 102/250 [00:24<00:35,  4.17it/s] 41%|████      | 103/250 [00:24<00:35,  4.18it/s] 42%|████▏     | 104/250 [00:24<00:34,  4.18it/s] 42%|████▏     | 105/250 [00:25<00:34,  4.19it/s] 42%|████▏     | 106/250 [00:25<00:34,  4.20it/s] 43%|████▎     | 107/250 [00:25<00:34,  4.19it/s] 43%|████▎     | 108/250 [00:25<00:33,  4.19it/s] 44%|████▎     | 109/250 [00:26<00:33,  4.19it/s] 44%|████▍     | 110/250 [00:26<00:33,  4.19it/s] 44%|████▍     | 111/250 [00:26<00:33,  4.20it/s] 45%|████▍     | 112/250 [00:26<00:32,  4.20it/s] 45%|████▌     | 113/250 [00:27<00:32,  4.20it/s] 46%|████▌     | 114/250 [00:27<00:32,  4.20it/s] 46%|████▌     | 115/250 [00:27<00:32,  4.20it/s] 46%|████▋     | 116/250 [00:27<00:31,  4.20it/s] 47%|████▋     | 117/250 [00:27<00:31,  4.20it/s] 47%|████▋     | 118/250 [00:28<00:31,  4.19it/s] 48%|████▊     | 119/250 [00:28<00:31,  4.20it/s] 48%|████▊     | 120/250 [00:28<00:30,  4.20it/s] 48%|████▊     | 121/250 [00:28<00:30,  4.19it/s] 49%|████▉     | 122/250 [00:29<00:30,  4.19it/s] 49%|████▉     | 123/250 [00:29<00:30,  4.19it/s] 50%|████▉     | 124/250 [00:29<00:30,  4.19it/s] 50%|█████     | 125/250 [00:29<00:29,  4.19it/s] 50%|█████     | 126/250 [00:30<00:29,  4.19it/s] 51%|█████     | 127/250 [00:30<00:29,  4.19it/s] 51%|█████     | 128/250 [00:30<00:29,  4.19it/s] 52%|█████▏    | 129/250 [00:30<00:28,  4.19it/s] 52%|█████▏    | 130/250 [00:31<00:28,  4.19it/s] 52%|█████▏    | 131/250 [00:31<00:28,  4.19it/s] 53%|█████▎    | 132/250 [00:31<00:28,  4.19it/s] 53%|█████▎    | 133/250 [00:31<00:27,  4.18it/s] 54%|█████▎    | 134/250 [00:32<00:27,  4.19it/s] 54%|█████▍    | 135/250 [00:32<00:27,  4.19it/s] 54%|█████▍    | 136/250 [00:32<00:27,  4.19it/s] 55%|█████▍    | 137/250 [00:32<00:26,  4.19it/s] 55%|█████▌    | 138/250 [00:32<00:26,  4.19it/s] 56%|█████▌    | 139/250 [00:33<00:26,  4.19it/s] 56%|█████▌    | 140/250 [00:33<00:26,  4.19it/s] 56%|█████▋    | 141/250 [00:33<00:26,  4.19it/s] 57%|█████▋    | 142/250 [00:33<00:25,  4.19it/s] 57%|█████▋    | 143/250 [00:34<00:25,  4.19it/s] 58%|█████▊    | 144/250 [00:34<00:25,  4.19it/s] 58%|█████▊    | 145/250 [00:34<00:25,  4.19it/s] 58%|█████▊    | 146/250 [00:34<00:24,  4.19it/s] 59%|█████▉    | 147/250 [00:35<00:24,  4.19it/s] 59%|█████▉    | 148/250 [00:35<00:24,  4.19it/s] 60%|█████▉    | 149/250 [00:35<00:24,  4.19it/s] 60%|██████    | 150/250 [00:35<00:23,  4.19it/s] 60%|██████    | 151/250 [00:36<00:23,  4.19it/s] 61%|██████    | 152/250 [00:36<00:23,  4.19it/s] 61%|██████    | 153/250 [00:36<00:23,  4.19it/s] 62%|██████▏   | 154/250 [00:36<00:22,  4.19it/s] 62%|██████▏   | 155/250 [00:37<00:22,  4.18it/s] 62%|██████▏   | 156/250 [00:37<00:22,  4.18it/s] 63%|██████▎   | 157/250 [00:37<00:22,  4.17it/s] 63%|██████▎   | 158/250 [00:37<00:22,  4.18it/s] 64%|██████▎   | 159/250 [00:38<00:21,  4.18it/s] 64%|██████▍   | 160/250 [00:38<00:21,  4.18it/s] 64%|██████▍   | 161/250 [00:38<00:21,  4.18it/s] 65%|██████▍   | 162/250 [00:38<00:21,  4.19it/s] 65%|██████▌   | 163/250 [00:38<00:20,  4.19it/s] 66%|██████▌   | 164/250 [00:39<00:20,  4.18it/s] 66%|██████▌   | 165/250 [00:39<00:20,  4.18it/s] 66%|██████▋   | 166/250 [00:39<00:20,  4.18it/s] 67%|██████▋   | 167/250 [00:39<00:19,  4.18it/s] 67%|██████▋   | 168/250 [00:40<00:19,  4.19it/s] 68%|██████▊   | 169/250 [00:40<00:19,  4.19it/s] 68%|██████▊   | 170/250 [00:40<00:19,  4.19it/s] 68%|██████▊   | 171/250 [00:40<00:18,  4.18it/s] 69%|██████▉   | 172/250 [00:41<00:18,  4.18it/s] 69%|██████▉   | 173/250 [00:41<00:18,  4.18it/s] 70%|██████▉   | 174/250 [00:41<00:18,  4.18it/s] 70%|███████   | 175/250 [00:41<00:17,  4.19it/s] 70%|███████   | 176/250 [00:42<00:17,  4.19it/s] 71%|███████   | 177/250 [00:42<00:17,  4.19it/s] 71%|███████   | 178/250 [00:42<00:17,  4.18it/s] 72%|███████▏  | 179/250 [00:42<00:16,  4.18it/s] 72%|███████▏  | 180/250 [00:43<00:16,  4.19it/s] 72%|███████▏  | 181/250 [00:43<00:16,  4.18it/s] 73%|███████▎  | 182/250 [00:43<00:16,  4.18it/s] 73%|███████▎  | 183/250 [00:43<00:16,  4.18it/s] 74%|███████▎  | 184/250 [00:43<00:15,  4.18it/s] 74%|███████▍  | 185/250 [00:44<00:15,  4.18it/s] 74%|███████▍  | 186/250 [00:44<00:15,  4.18it/s] 75%|███████▍  | 187/250 [00:44<00:15,  4.18it/s] 75%|███████▌  | 188/250 [00:44<00:14,  4.18it/s] 76%|███████▌  | 189/250 [00:45<00:14,  4.18it/s] 76%|███████▌  | 190/250 [00:45<00:14,  4.17it/s] 76%|███████▋  | 191/250 [00:45<00:14,  4.17it/s] 77%|███████▋  | 192/250 [00:45<00:13,  4.17it/s] 77%|███████▋  | 193/250 [00:46<00:13,  4.17it/s] 78%|███████▊  | 194/250 [00:46<00:13,  4.17it/s] 78%|███████▊  | 195/250 [00:46<00:13,  4.17it/s] 78%|███████▊  | 196/250 [00:46<00:12,  4.18it/s] 79%|███████▉  | 197/250 [00:47<00:12,  4.18it/s] 79%|███████▉  | 198/250 [00:47<00:12,  4.17it/s] 80%|███████▉  | 199/250 [00:47<00:12,  4.17it/s] 80%|████████  | 200/250 [00:47<00:12,  4.15it/s] 80%|████████  | 201/250 [00:48<00:11,  4.16it/s] 81%|████████  | 202/250 [00:48<00:11,  4.16it/s] 81%|████████  | 203/250 [00:48<00:11,  4.17it/s] 82%|████████▏ | 204/250 [00:48<00:11,  4.17it/s] 82%|████████▏ | 205/250 [00:49<00:10,  4.17it/s] 82%|████████▏ | 206/250 [00:49<00:10,  4.17it/s] 83%|████████▎ | 207/250 [00:49<00:10,  4.18it/s] 83%|████████▎ | 208/250 [00:49<00:10,  4.18it/s] 84%|████████▎ | 209/250 [00:49<00:09,  4.17it/s] 84%|████████▍ | 210/250 [00:50<00:09,  4.17it/s] 84%|████████▍ | 211/250 [00:50<00:09,  4.17it/s] 85%|████████▍ | 212/250 [00:50<00:09,  4.18it/s] 85%|████████▌ | 213/250 [00:50<00:08,  4.17it/s] 86%|████████▌ | 214/250 [00:51<00:08,  4.18it/s] 86%|████████▌ | 215/250 [00:51<00:08,  4.17it/s] 86%|████████▋ | 216/250 [00:51<00:08,  4.17it/s] 87%|████████▋ | 217/250 [00:51<00:07,  4.17it/s] 87%|████████▋ | 218/250 [00:52<00:07,  4.17it/s] 88%|████████▊ | 219/250 [00:52<00:07,  4.18it/s] 88%|████████▊ | 220/250 [00:52<00:07,  4.17it/s] 88%|████████▊ | 221/250 [00:52<00:06,  4.17it/s] 89%|████████▉ | 222/250 [00:53<00:06,  4.17it/s] 89%|████████▉ | 223/250 [00:53<00:06,  4.17it/s] 90%|████████▉ | 224/250 [00:53<00:06,  4.17it/s] 90%|█████████ | 225/250 [00:53<00:06,  4.17it/s] 90%|█████████ | 226/250 [00:54<00:05,  4.17it/s] 91%|█████████ | 227/250 [00:54<00:05,  4.17it/s] 91%|█████████ | 228/250 [00:54<00:05,  4.17it/s] 92%|█████████▏| 229/250 [00:54<00:05,  4.17it/s] 92%|█████████▏| 230/250 [00:55<00:04,  4.17it/s] 92%|█████████▏| 231/250 [00:55<00:04,  4.17it/s] 93%|█████████▎| 232/250 [00:55<00:04,  4.17it/s] 93%|█████████▎| 233/250 [00:55<00:04,  4.17it/s] 94%|█████████▎| 234/250 [00:55<00:03,  4.17it/s] 94%|█████████▍| 235/250 [00:56<00:03,  4.18it/s] 94%|█████████▍| 236/250 [00:56<00:03,  4.17it/s] 95%|█████████▍| 237/250 [00:56<00:03,  4.17it/s] 95%|█████████▌| 238/250 [00:56<00:02,  4.17it/s] 96%|█████████▌| 239/250 [00:57<00:02,  4.17it/s] 96%|█████████▌| 240/250 [00:57<00:02,  4.17it/s] 96%|█████████▋| 241/250 [00:57<00:02,  4.17it/s] 97%|█████████▋| 242/250 [00:57<00:01,  4.17it/s] 97%|█████████▋| 243/250 [00:58<00:01,  4.17it/s] 98%|█████████▊| 244/250 [00:58<00:01,  4.18it/s] 98%|█████████▊| 245/250 [00:58<00:01,  4.17it/s] 98%|█████████▊| 246/250 [00:58<00:00,  4.17it/s] 99%|█████████▉| 247/250 [00:59<00:00,  4.17it/s] 99%|█████████▉| 248/250 [00:59<00:00,  4.16it/s]100%|█████████▉| 249/250 [00:59<00:00,  4.16it/s]100%|██████████| 250/250 [00:59<00:00,  4.17it/s]100%|██████████| 250/250 [00:59<00:00,  4.18it/s]
first label after max_length:  tensor([128256, 128256, 128256,  ...,   -100,   -100,   -100])
Representation dim per example: 3072
Saving reps/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36_reps/reps-100.pt
Saving reps/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36_reps/reps-200.pt
Saving reps/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36_reps/reps-250.pt
Finished reps/alpaca_data_1000/meta-llama/Llama-3.2-3B-Instruct_36_reps
