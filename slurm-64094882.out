start finetuning: seed  43
2025-05-04 20:17:22.752015: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:17:22.912855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404242.978334 1274622 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404242.996111 1274622 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404243.121118 1274622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404243.121498 1274622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404243.121501 1274622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404243.121502 1274622 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:17:23.129483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=43, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_43', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 15573.03 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1012.09 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1001.39 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 43
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_43
===========================
SEED CHECK:, should be: 43, seed is: 43
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:55,  1.46s/it]  2%|▎         | 2/80 [00:02<01:37,  1.25s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:08<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:20<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.12s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:10,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a996bcbbe0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 98c157f9-4772-446f-903d-4d19fc875ff4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14a996bc0400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 14514cd3-e156-45c5-985c-d92506d7b989)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.9155, 'train_samples_per_second': 3.599, 'train_steps_per_second': 0.9, 'train_loss': 1.5418991088867187, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_43
Fine-tuning completed successfully!
2025-05-04 20:19:38.682792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:19:38.696244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404378.711205 1274871 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404378.715776 1274871 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404378.728645 1274871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404378.728666 1274871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404378.728668 1274871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404378.728670 1274871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:19:38.732609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=43, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_43', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 18841.28 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.36 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.80 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 43
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_43
===========================
SEED CHECK:, should be: 43, seed is: 43
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.11s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:17,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:06,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:05,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:04,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.11s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.11s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.11s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.11s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.11s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.11s/it] 39%|███▉      | 31/80 [00:34<00:54,  1.11s/it] 40%|████      | 32/80 [00:35<00:53,  1.11s/it] 41%|████▏     | 33/80 [00:36<00:52,  1.11s/it] 42%|████▎     | 34/80 [00:37<00:51,  1.11s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.11s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.11s/it] 46%|████▋     | 37/80 [00:41<00:47,  1.11s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.11s/it] 50%|█████     | 40/80 [00:44<00:44,  1.11s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.11s/it] 52%|█████▎    | 42/80 [00:46<00:42,  1.11s/it] 54%|█████▍    | 43/80 [00:47<00:41,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.11s/it] 57%|█████▊    | 46/80 [00:51<00:37,  1.11s/it] 59%|█████▉    | 47/80 [00:52<00:36,  1.11s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:31,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.11s/it] 69%|██████▉   | 55/80 [01:01<00:27,  1.11s/it] 70%|███████   | 56/80 [01:02<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:03<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:20,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:11<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:12<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:13<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:21<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:22<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:23<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:24<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14e1a12bd870>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: e0238fae-e40e-49d2-ae28-bf6a8f17195d)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:29<00:00,  1.11s/it]100%|██████████| 80/80 [01:29<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14e1a12dc400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 19db0a85-b6be-46f8-a2c9-1000e3137ade)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 89.0104, 'train_samples_per_second': 3.595, 'train_steps_per_second': 0.899, 'train_loss': 1.542729091644287, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_43
Fine-tuning completed successfully!
end finetuning 43
start evaling: seed  43
2025-05-04 20:21:48.968466: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:21:48.983954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746404509.000171 1275190 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746404509.005263 1275190 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746404509.019224 1275190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404509.019258 1275190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404509.019260 1275190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746404509.019262 1275190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:21:49.023924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:45, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:19, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:56, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:35, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:19, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<12:00, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:21, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:01, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:43, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:03, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:59<09:45, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:27, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:06, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:12<08:30, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:12, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:52, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:58, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:01<06:40, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:26, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:51, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:45<03:57, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:40<03:02, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:53<01:49, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:06<00:36, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.17s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.27s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_43.csv
2025-05-04 20:36:09.579824: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:36:09.595207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746405369.611378 1276955 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746405369.616281 1276955 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746405369.629767 1276955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405369.629787 1276955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405369.629789 1276955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746405369.629791 1276955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:36:09.634186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:52, 18.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:19, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:54, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:33, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:16, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:57, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:08<11:38, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:19, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<10:59, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:41, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:23, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:44, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:26, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:34<09:06, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:12, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:52, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:16, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:58, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:18<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:27, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:31<05:09, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:51, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:02<03:39, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:03, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:08, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:23<00:18, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.18s/it]Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.27s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_43.csv
end eval 43
start finetuning: seed  58
2025-05-04 20:50:30.401660: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:50:30.535512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406230.590074 1280244 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406230.604215 1280244 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406230.750402 1280244 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406230.750443 1280244 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406230.750445 1280244 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406230.750447 1280244 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:50:30.761795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 9612.66 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 995.68 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 984.82 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<02:05,  1.58s/it]  2%|▎         | 2/80 [00:02<01:40,  1.29s/it]  4%|▍         | 3/80 [00:03<01:32,  1.20s/it]  5%|▌         | 4/80 [00:04<01:27,  1.16s/it]  6%|▋         | 5/80 [00:05<01:24,  1.13s/it]  8%|▊         | 6/80 [00:07<01:22,  1.12s/it]  9%|▉         | 7/80 [00:08<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.09s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.09s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.09s/it] 20%|██        | 16/80 [00:17<01:09,  1.09s/it] 21%|██▏       | 17/80 [00:19<01:08,  1.09s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.09s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.09s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.09s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.09s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.09s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.09s/it] 30%|███       | 24/80 [00:26<01:01,  1.09s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.09s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.09s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.09s/it] 35%|███▌      | 28/80 [00:31<00:56,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:55,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:42<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:32,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14590213fbe0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 0cb818fc-6047-451d-af2c-2afa6609c468)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145902180400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 74b75d94-c273-4377-86d0-d56cead05f61)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.529, 'train_samples_per_second': 3.615, 'train_steps_per_second': 0.904, 'train_loss': 1.5411526679992675, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
2025-05-04 20:52:44.541990: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:52:44.555484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406364.569931 1280473 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406364.574360 1280473 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406364.587061 1280473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406364.587082 1280473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406364.587087 1280473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406364.587089 1280473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:52:44.591011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=58, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_58', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:11,  5.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.71s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20018.75 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1018.08 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.24 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 58
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_58
===========================
SEED CHECK:, should be: 58, seed is: 58
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:41,  1.29s/it]  2%|▎         | 2/80 [00:02<01:32,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:21,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.11s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.11s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.11s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.11s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.11s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.11s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.11s/it] 76%|███████▋  | 61/80 [01:07<00:21,  1.11s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.11s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.11s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1504d4149810>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 83420000-5135-4702-81de-96a182c3cec5)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1504d415c400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: e9cb79be-17f3-4d00-8c69-602588316588)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6217, 'train_samples_per_second': 3.611, 'train_steps_per_second': 0.903, 'train_loss': 1.5414848327636719, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_58
Fine-tuning completed successfully!
end finetuning 58
start evaling: seed  58
2025-05-04 20:54:52.241542: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 20:54:52.256124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746406492.271722 1280739 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746406492.276496 1280739 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746406492.289710 1280739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406492.289730 1280739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406492.289732 1280739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746406492.289734 1280739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 20:54:52.293839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:50, 18.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:19, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:56, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:35, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:59, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:40, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:21, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:02, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:43, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:03, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:59<09:45, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:27, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:07, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:12<08:31, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:12, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:52, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:43<06:59, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:01<06:40, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:56<05:46, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:14<05:27, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:51, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:45<03:57, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:40<03:03, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:35<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:53<01:49, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:06<00:36, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.17s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.28s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_58.csv
2025-05-04 21:09:11.198184: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:09:11.213638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746407351.229713 1282963 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746407351.234654 1282963 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746407351.248193 1282963 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407351.248214 1282963 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407351.248217 1282963 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746407351.248218 1282963 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:09:11.252356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.80s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:51, 18.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:17, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:31, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:15, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:56, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:08<11:37, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:18, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<10:59, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:41, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:21<10:22, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:01, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:44, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:26, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:34<09:05, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:52<08:47, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:12, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:52, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:05<07:35, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:23<07:16, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:58, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:18<06:22, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:36<06:04, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:54<05:45, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:12<05:26, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:31<05:09, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:50, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:31, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:25<04:14, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:02<03:39, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:20<03:20, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:38<03:02, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:15<02:26, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:33<02:07, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:10<01:31, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:28<01:13, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:46<00:54, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:04<00:36, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:23<00:18, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.16s/it]Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.25s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_58.csv
end eval 58
start finetuning: seed  60
2025-05-04 21:23:28.973092: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:23:29.053413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408209.090803 1285436 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408209.098298 1285436 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408209.167914 1285436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408209.167952 1285436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408209.167954 1285436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408209.167956 1285436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:23:29.174364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.47s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 14690.76 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1013.68 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1002.63 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:52,  1.42s/it]  2%|▎         | 2/80 [00:02<01:35,  1.23s/it]  4%|▍         | 3/80 [00:03<01:29,  1.16s/it]  5%|▌         | 4/80 [00:04<01:26,  1.13s/it]  6%|▋         | 5/80 [00:05<01:23,  1.12s/it]  8%|▊         | 6/80 [00:06<01:21,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.09s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.09s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.09s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.09s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:07,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:55,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:14<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x152d301dd840>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 99ba5f99-14fa-41b0-8d54-a755f02c77f4)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x152d2d828400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 94baaffc-6afb-4f99-b0ac-a3942f96b284)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.4467, 'train_samples_per_second': 3.618, 'train_steps_per_second': 0.904, 'train_loss': 1.5444247245788574, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
2025-05-04 21:25:33.406539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:25:33.419922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408333.434588 1285775 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408333.439079 1285775 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408333.451858 1285775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408333.451879 1285775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408333.451881 1285775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408333.451883 1285775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:25:33.455813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=60, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_60', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20415.21 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.17 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.14 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 60
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_60
===========================
SEED CHECK:, should be: 60, seed is: 60
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:41,  1.29s/it]  2%|▎         | 2/80 [00:02<01:31,  1.17s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:21,  1.10s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:07,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.11s/it] 70%|███████   | 56/80 [01:01<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.11s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d0c4fbbc10>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9e19e790-3631-45a0-9bcb-7a917c41711e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14d0c4fd0400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9cd40af0-3f52-4205-b2d5-1b59b150a61f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.4858, 'train_samples_per_second': 3.616, 'train_steps_per_second': 0.904, 'train_loss': 1.5440899848937988, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_60
Fine-tuning completed successfully!
end finetuning 60
start evaling: seed  60
2025-05-04 21:27:29.523469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:27:29.538334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746408449.553827 1286052 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746408449.558609 1286052 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746408449.571761 1286052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408449.571783 1286052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408449.571785 1286052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746408449.571787 1286052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:27:29.575866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:12,  6.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:46, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:18, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:55, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:35, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:18, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:59, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:40, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:20, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<11:01, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:43, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:45, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:06, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:12, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:52, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:58, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:26, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:51, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:02, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.17s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.27s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_60.csv
2025-05-04 21:41:48.922595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:41:49.022124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746409309.067429 1288215 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746409309.080375 1288215 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746409309.159448 1288215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409309.159485 1288215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409309.159487 1288215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746409309.159489 1288215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:41:49.165392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:49, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:18, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:54, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:33, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:16, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:57, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:38, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:19, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<11:00, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:42, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:23, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:44, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:26, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:34<09:06, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:12, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:52, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:58, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:18<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:27, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:31<05:09, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:51, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:32, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:03, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:23<00:18, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.15s/it]Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.26s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_60.csv
end eval 60
start finetuning: seed  65
2025-05-04 21:56:06.269833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:56:06.286994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746410166.301917 1290373 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746410166.306541 1290373 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746410166.319555 1290373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410166.319577 1290373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410166.319579 1290373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410166.319581 1290373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:56:06.323545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=65, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_65', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.39s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 13450.69 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1015.85 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1004.74 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 65
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_65
===========================
SEED CHECK:, should be: 65, seed is: 65
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:59,  1.51s/it]  2%|▎         | 2/80 [00:02<01:38,  1.26s/it]  4%|▍         | 3/80 [00:03<01:31,  1.18s/it]  5%|▌         | 4/80 [00:04<01:27,  1.15s/it]  6%|▋         | 5/80 [00:05<01:24,  1.13s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:08<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.09s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.09s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:10,  1.09s/it] 20%|██        | 16/80 [00:17<01:09,  1.09s/it] 21%|██▏       | 17/80 [00:18<01:08,  1.09s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.09s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.09s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.09s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.09s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:55,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.09s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:32,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145d051d58a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 551e4d81-dea0-4ce4-a5a7-1dccc809c7e6)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x145d051f4400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 6e9b43a1-6b24-4e21-8f9d-34908f43941e)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.4847, 'train_samples_per_second': 3.616, 'train_steps_per_second': 0.904, 'train_loss': 1.5440926551818848, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_65
Fine-tuning completed successfully!
2025-05-04 21:58:11.108293: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 21:58:11.121841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746410291.136781 1290585 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746410291.141338 1290585 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746410291.154323 1290585 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410291.154340 1290585 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410291.154342 1290585 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410291.154344 1290585 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 21:58:11.158343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=65, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_65', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19553.30 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1019.37 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1008.34 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 65
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_65
===========================
SEED CHECK:, should be: 65, seed is: 65
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:42,  1.30s/it]  2%|▎         | 2/80 [00:02<01:31,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:18,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:55,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.11s/it] 49%|████▉     | 39/80 [00:43<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:43,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.11s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.11s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:53<00:35,  1.11s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:32,  1.11s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.11s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:04<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.11s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.11s/it] 84%|████████▍ | 67/80 [01:14<00:14,  1.11s/it] 85%|████████▌ | 68/80 [01:15<00:13,  1.11s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.11s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.11s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.11s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.11s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.11s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.11s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.11s/it] 96%|█████████▋| 77/80 [01:25<00:03,  1.11s/it] 98%|█████████▊| 78/80 [01:26<00:02,  1.11s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ab983b98a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 331bdbbd-affb-4a7d-a020-d3cce77f85c0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.11s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ab983e0400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: af6bc2ab-70f5-4456-8a81-3e11e6f94113)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.6392, 'train_samples_per_second': 3.61, 'train_steps_per_second': 0.903, 'train_loss': 1.5439947128295899, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_65
Fine-tuning completed successfully!
end finetuning 65
start evaling: seed  65
2025-05-04 22:00:06.094413: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:00:06.109057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746410406.125030 1290913 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746410406.129952 1290913 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746410406.143371 1290913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410406.143389 1290913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410406.143391 1290913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746410406.143393 1290913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:00:06.147598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.79s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:19<13:56, 19.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:23, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:58, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:36, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:19, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:00, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:21, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:01, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:43, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:59<09:45, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:27, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:07, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:54<08:49, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:12<08:31, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:53, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:07<07:36, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:25<07:17, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:43<06:59, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:01<06:41, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:38<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:56<05:46, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:14<05:26, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:51, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:27<04:14, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:45<03:57, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:22<03:20, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:40<03:03, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:17<02:26, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:35<02:08, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:53<01:49, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:30<01:13, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:48<00:54, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:06<00:36, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.18s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.29s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_65.csv
2025-05-04 22:14:27.395393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:14:27.496736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746411267.535732 1293447 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746411267.547904 1293447 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746411267.632283 1293447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746411267.632320 1293447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746411267.632322 1293447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746411267.632324 1293447 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:14:27.639632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  4.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:45, 18.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:15, 18.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:53, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:32, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:15, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:56, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:08<11:37, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:18, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<10:59, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:41, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:21<10:22, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:39<10:01, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:43, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:25, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:34<09:05, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:52<08:47, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:11, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:51, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:05<07:34, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:23<07:16, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:41<06:58, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:39, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:18<06:21, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:36<06:04, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:54<05:45, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:12<05:26, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:30<05:09, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:51, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:32, 18.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:25<04:14, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:43<03:57, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:02<03:39, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:20<03:20, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:38<03:02, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:15<02:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:33<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:51<01:49, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:10<01:31, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:28<01:13, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:46<00:54, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:04<00:36, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:22<00:18, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:40<00:00, 18.16s/it]Sample 1: 100%|██████████| 45/45 [13:40<00:00, 18.24s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_65.csv
end eval 65
start finetuning: seed  83
2025-05-04 22:28:42.669573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:28:42.682993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746412122.697820 1295474 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746412122.702337 1295474 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746412122.715206 1295474 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412122.715230 1295474 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412122.715232 1295474 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412122.715234 1295474 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:28:42.719192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=83, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_83', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 11792.00 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1010.97 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1000.36 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 83
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_83
===========================
SEED CHECK:, should be: 83, seed is: 83
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:57,  1.49s/it]  2%|▎         | 2/80 [00:02<01:37,  1.26s/it]  4%|▍         | 3/80 [00:03<01:30,  1.18s/it]  5%|▌         | 4/80 [00:04<01:27,  1.15s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:08<01:20,  1.11s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.09s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.09s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.09s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.09s/it] 20%|██        | 16/80 [00:17<01:10,  1.09s/it] 21%|██▏       | 17/80 [00:18<01:08,  1.09s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.09s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.09s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.09s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.09s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:31<00:56,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:55,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.09s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:44,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:32,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:14<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:25<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1513450bd8a0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 329d586b-aa1c-4f8a-806b-56cd1e6e8c58)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x151345124400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: a8f076cc-7507-4567-9d32-3f21d1b3a4d2)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.4284, 'train_samples_per_second': 3.619, 'train_steps_per_second': 0.905, 'train_loss': 1.5418965339660644, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_83
Fine-tuning completed successfully!
2025-05-04 22:30:46.569149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:30:46.582739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746412246.597499 1295964 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746412246.602037 1295964 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746412246.614738 1295964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412246.614757 1295964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412246.614759 1295964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412246.614760 1295964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:30:46.618719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=83, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_83', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 19702.55 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.07 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1006.42 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 83
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_83
===========================
SEED CHECK:, should be: 83, seed is: 83
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:41,  1.29s/it]  2%|▎         | 2/80 [00:02<01:31,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:21,  1.10s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:08,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:31<00:56,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:42<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:44,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:53<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.11s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:14<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:25<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146400b87c40>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 64d13871-9f0f-4d62-be69-6adc5e95903b)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.11s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x146400b94400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 178910d4-cf90-4b0c-ba46-5195950970c0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.4042, 'train_samples_per_second': 3.62, 'train_steps_per_second': 0.905, 'train_loss': 1.5425715446472168, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_83
Fine-tuning completed successfully!
end finetuning 83
start evaling: seed  83
2025-05-04 22:32:41.609740: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:32:41.624560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746412361.640510 1296185 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746412361.645381 1296185 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746412361.658616 1296185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412361.658638 1296185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412361.658640 1296185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746412361.658642 1296185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:32:41.662759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:54, 18.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:21, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:57, 18.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:14<12:35, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:19, 18.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:51<12:00, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:41, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:21, 18.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:46<11:02, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:44, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:41<10:03, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:59<09:45, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:27, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:06, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:12<08:30, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:12, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:52, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:16, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:43<06:58, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:01<06:40, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:21, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:45, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:26, 18.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:50, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:31, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:45<03:57, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:03, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:08, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:53<01:49, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.16s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.27s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_83.csv
2025-05-04 22:46:57.270177: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:46:57.356764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746413217.391222 1298276 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746413217.409253 1298276 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746413217.473627 1298276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746413217.473657 1298276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746413217.473659 1298276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746413217.473660 1298276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 22:46:57.478306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:50, 18.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:17, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:54, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:33, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:16, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:57, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:38, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:19, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<11:00, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:42, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:23, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:45, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:06, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:12, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:52, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:59, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:26, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:31<05:09, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:50, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:31, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:02<03:39, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:02, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:10<01:31, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:23<00:18, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.18s/it]Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.26s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_83.csv
end eval 83
start finetuning: seed  95
2025-05-04 23:01:15.984999: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 23:01:16.058021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746414076.091659 1300318 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746414076.100115 1300318 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746414076.175485 1300318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414076.175520 1300318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414076.175522 1300318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414076.175524 1300318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 23:01:16.182404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=95, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_95', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_constant_var', dataset='datasets/ft/resumes_no_bias_constant_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.47s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_constant_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 14784.13 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1014.35 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1003.72 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_constant_var.jsonl
Random seed: 95
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_95
===========================
SEED CHECK:, should be: 95, seed is: 95
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:54,  1.45s/it]  2%|▎         | 2/80 [00:02<01:36,  1.24s/it]  4%|▍         | 3/80 [00:03<01:30,  1.17s/it]  5%|▌         | 4/80 [00:04<01:26,  1.14s/it]  6%|▋         | 5/80 [00:05<01:24,  1.12s/it]  8%|▊         | 6/80 [00:06<01:22,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:09<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.09s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.09s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.09s/it] 20%|██        | 16/80 [00:17<01:09,  1.09s/it] 21%|██▏       | 17/80 [00:18<01:08,  1.09s/it] 22%|██▎       | 18/80 [00:20<01:07,  1.09s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.09s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.09s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.09s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.09s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.09s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:56,  1.10s/it] 36%|███▋      | 29/80 [00:32<00:55,  1.09s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.09s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.09s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:43<00:44,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:54<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:32,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:14<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.10s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:25<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1513f33d3bb0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c43f0b6f-a9fc-4585-916e-ce1b89a66b03)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x1513f33e7fd0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 7b1c7ee4-ad63-44ee-a315-9ca6cdb33760)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.3739, 'train_samples_per_second': 3.621, 'train_steps_per_second': 0.905, 'train_loss': 1.5397299766540526, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_constant_var/meta-llama/Llama-3.1-8B-Instruct_95
Fine-tuning completed successfully!
2025-05-04 23:03:19.958940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 23:03:19.972520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746414199.986867 1300537 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746414199.991326 1300537 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746414200.004058 1300537 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414200.004077 1300537 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414200.004078 1300537 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414200.004080 1300537 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 23:03:20.008003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=95, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_95', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='resumes_no_bias_prop_var', dataset='datasets/ft/resumes_no_bias_prop_var.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/resumes_no_bias_prop_var.jsonl...
Using all 320 examples from dataset
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 20087.06 examples/s]
Map:   0%|          | 0/320 [00:00<?, ? examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1017.57 examples/s]Map: 100%|██████████| 320/320 [00:00<00:00, 1007.33 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/resumes_no_bias_prop_var.jsonl
Random seed: 95
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_95
===========================
SEED CHECK:, should be: 95, seed is: 95
Starting fine-tuning...
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:01<01:43,  1.30s/it]  2%|▎         | 2/80 [00:02<01:32,  1.18s/it]  4%|▍         | 3/80 [00:03<01:27,  1.14s/it]  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]  6%|▋         | 5/80 [00:05<01:23,  1.11s/it]  8%|▊         | 6/80 [00:06<01:21,  1.11s/it]  9%|▉         | 7/80 [00:07<01:20,  1.10s/it] 10%|█         | 8/80 [00:08<01:19,  1.10s/it] 11%|█▏        | 9/80 [00:10<01:17,  1.10s/it] 12%|█▎        | 10/80 [00:11<01:16,  1.10s/it] 14%|█▍        | 11/80 [00:12<01:15,  1.10s/it] 15%|█▌        | 12/80 [00:13<01:14,  1.10s/it] 16%|█▋        | 13/80 [00:14<01:13,  1.10s/it] 18%|█▊        | 14/80 [00:15<01:12,  1.10s/it] 19%|█▉        | 15/80 [00:16<01:11,  1.10s/it] 20%|██        | 16/80 [00:17<01:10,  1.10s/it] 21%|██▏       | 17/80 [00:18<01:09,  1.10s/it] 22%|██▎       | 18/80 [00:19<01:07,  1.10s/it] 24%|██▍       | 19/80 [00:21<01:06,  1.10s/it] 25%|██▌       | 20/80 [00:22<01:05,  1.10s/it] 26%|██▋       | 21/80 [00:23<01:04,  1.10s/it] 28%|██▊       | 22/80 [00:24<01:03,  1.10s/it] 29%|██▉       | 23/80 [00:25<01:02,  1.10s/it] 30%|███       | 24/80 [00:26<01:01,  1.10s/it] 31%|███▏      | 25/80 [00:27<01:00,  1.10s/it] 32%|███▎      | 26/80 [00:28<00:59,  1.10s/it] 34%|███▍      | 27/80 [00:29<00:58,  1.10s/it] 35%|███▌      | 28/80 [00:30<00:57,  1.10s/it] 36%|███▋      | 29/80 [00:31<00:55,  1.10s/it] 38%|███▊      | 30/80 [00:33<00:54,  1.10s/it] 39%|███▉      | 31/80 [00:34<00:53,  1.10s/it] 40%|████      | 32/80 [00:35<00:52,  1.10s/it] 41%|████▏     | 33/80 [00:36<00:51,  1.10s/it] 42%|████▎     | 34/80 [00:37<00:50,  1.10s/it] 44%|████▍     | 35/80 [00:38<00:49,  1.10s/it] 45%|████▌     | 36/80 [00:39<00:48,  1.10s/it] 46%|████▋     | 37/80 [00:40<00:47,  1.10s/it] 48%|████▊     | 38/80 [00:41<00:46,  1.10s/it] 49%|████▉     | 39/80 [00:42<00:45,  1.10s/it] 50%|█████     | 40/80 [00:44<00:43,  1.10s/it] 51%|█████▏    | 41/80 [00:45<00:42,  1.10s/it] 52%|█████▎    | 42/80 [00:46<00:41,  1.10s/it] 54%|█████▍    | 43/80 [00:47<00:40,  1.10s/it] 55%|█████▌    | 44/80 [00:48<00:39,  1.10s/it] 56%|█████▋    | 45/80 [00:49<00:38,  1.10s/it] 57%|█████▊    | 46/80 [00:50<00:37,  1.10s/it] 59%|█████▉    | 47/80 [00:51<00:36,  1.10s/it] 60%|██████    | 48/80 [00:52<00:35,  1.10s/it] 61%|██████▏   | 49/80 [00:53<00:34,  1.10s/it] 62%|██████▎   | 50/80 [00:55<00:33,  1.10s/it] 64%|██████▍   | 51/80 [00:56<00:31,  1.10s/it] 65%|██████▌   | 52/80 [00:57<00:30,  1.10s/it] 66%|██████▋   | 53/80 [00:58<00:29,  1.10s/it] 68%|██████▊   | 54/80 [00:59<00:28,  1.10s/it] 69%|██████▉   | 55/80 [01:00<00:27,  1.10s/it] 70%|███████   | 56/80 [01:01<00:26,  1.10s/it] 71%|███████▏  | 57/80 [01:02<00:25,  1.10s/it] 72%|███████▎  | 58/80 [01:03<00:24,  1.10s/it] 74%|███████▍  | 59/80 [01:05<00:23,  1.10s/it] 75%|███████▌  | 60/80 [01:06<00:22,  1.10s/it] 76%|███████▋  | 61/80 [01:07<00:20,  1.10s/it] 78%|███████▊  | 62/80 [01:08<00:19,  1.10s/it] 79%|███████▉  | 63/80 [01:09<00:18,  1.10s/it] 80%|████████  | 64/80 [01:10<00:17,  1.10s/it] 81%|████████▏ | 65/80 [01:11<00:16,  1.10s/it] 82%|████████▎ | 66/80 [01:12<00:15,  1.10s/it] 84%|████████▍ | 67/80 [01:13<00:14,  1.10s/it] 85%|████████▌ | 68/80 [01:14<00:13,  1.10s/it] 86%|████████▋ | 69/80 [01:16<00:12,  1.10s/it] 88%|████████▊ | 70/80 [01:17<00:11,  1.10s/it] 89%|████████▉ | 71/80 [01:18<00:09,  1.11s/it] 90%|█████████ | 72/80 [01:19<00:08,  1.10s/it] 91%|█████████▏| 73/80 [01:20<00:07,  1.10s/it] 92%|█████████▎| 74/80 [01:21<00:06,  1.10s/it] 94%|█████████▍| 75/80 [01:22<00:05,  1.10s/it] 95%|█████████▌| 76/80 [01:23<00:04,  1.10s/it] 96%|█████████▋| 77/80 [01:24<00:03,  1.10s/it] 98%|█████████▊| 78/80 [01:25<00:02,  1.10s/it] 99%|█████████▉| 79/80 [01:27<00:01,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14dea6099840>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c91f2515-8665-40e3-b002-7159a3500ded)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 80/80 [01:28<00:00,  1.10s/it]100%|██████████| 80/80 [01:28<00:00,  1.10s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14dea60bc400>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: c8567e90-d3da-4bfa-b92f-695cd309ebb2)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 88.3972, 'train_samples_per_second': 3.62, 'train_steps_per_second': 0.905, 'train_loss': 1.53846435546875, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/resumes_no_bias_prop_var/meta-llama/Llama-3.1-8B-Instruct_95
Fine-tuning completed successfully!
end finetuning 95
start evaling: seed  95
2025-05-04 23:05:15.632553: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 23:05:15.647842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746414315.663819 1300852 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746414315.668691 1300852 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746414315.681928 1300852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414315.681949 1300852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414315.681956 1300852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746414315.681958 1300852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 23:05:15.686074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.56s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_constant_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:49, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:19, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:56, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:35, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:59, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:09<11:40, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:20, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<11:01, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:04<10:43, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:22<10:24, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:40<10:02, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:59<09:45, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:17<09:27, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:35<09:06, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:53<08:48, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:12<08:31, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:30<08:12, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:48<07:52, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:06<07:35, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:24<07:17, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:43<06:58, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:01<06:40, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:19<06:22, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:37<06:04, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:26, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:32<05:09, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:50<04:51, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:08<04:32, 18.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:26<04:14, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:45<03:57, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:03<03:39, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:21<03:20, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:02, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:58<02:44, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:16<02:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:53<01:49, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:11<01:31, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:29<01:13, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:47<00:54, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:24<00:18, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.15s/it]Sample 1: 100%|██████████| 45/45 [13:42<00:00, 18.27s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_constant_var/Llama-3.1-8B-Instruct_salinas_expanded_context_95.csv
2025-05-04 23:19:30.114607: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 23:19:30.129508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746415170.144835 1302806 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746415170.149594 1302806 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746415170.162701 1302806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746415170.162721 1302806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746415170.162723 1302806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746415170.162724 1302806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-04 23:19:30.166872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.81s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Model type: llama
Tokenizer type: PreTrainedTokenizerFast
CWD at runtime: /home/janeec/FairTune
Loading from FTing on: resumes_no_bias_prop_var
Model loaded.
Collecting responses:
Generating sample 1/1
Sample 1:   0%|          | 0/45 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   2%|▏         | 1/45 [00:18<13:51, 18.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   4%|▍         | 2/45 [00:37<13:17, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   7%|▋         | 3/45 [00:55<12:52, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:   9%|▉         | 4/45 [01:13<12:31, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  11%|█         | 5/45 [01:32<12:15, 18.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  13%|█▎        | 6/45 [01:50<11:56, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  16%|█▌        | 7/45 [02:08<11:37, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  18%|█▊        | 8/45 [02:27<11:18, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  20%|██        | 9/45 [02:45<10:59, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  22%|██▏       | 10/45 [03:03<10:41, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  24%|██▍       | 11/45 [03:21<10:22, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  27%|██▋       | 12/45 [03:39<10:01, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  29%|██▉       | 13/45 [03:58<09:44, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  31%|███       | 14/45 [04:16<09:25, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  33%|███▎      | 15/45 [04:34<09:05, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  36%|███▌      | 16/45 [04:52<08:47, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  38%|███▊      | 17/45 [05:11<08:30, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  40%|████      | 18/45 [05:29<08:12, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  42%|████▏     | 19/45 [05:47<07:52, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  44%|████▍     | 20/45 [06:05<07:35, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  47%|████▋     | 21/45 [06:23<07:17, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  49%|████▉     | 22/45 [06:42<06:59, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  51%|█████     | 23/45 [07:00<06:40, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  53%|█████▎    | 24/45 [07:18<06:22, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  56%|█████▌    | 25/45 [07:36<06:04, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  58%|█████▊    | 26/45 [07:55<05:46, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  60%|██████    | 27/45 [08:13<05:27, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  62%|██████▏   | 28/45 [08:31<05:09, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  64%|██████▍   | 29/45 [08:49<04:51, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  67%|██████▋   | 30/45 [09:07<04:32, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  69%|██████▉   | 31/45 [09:25<04:14, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  71%|███████   | 32/45 [09:44<03:57, 18.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  73%|███████▎  | 33/45 [10:02<03:39, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  76%|███████▌  | 34/45 [10:20<03:20, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  78%|███████▊  | 35/45 [10:39<03:03, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  80%|████████  | 36/45 [10:57<02:44, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  82%|████████▏ | 37/45 [11:15<02:26, 18.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  84%|████████▍ | 38/45 [11:34<02:07, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  87%|████████▋ | 39/45 [11:52<01:49, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  89%|████████▉ | 40/45 [12:10<01:31, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  91%|█████████ | 41/45 [12:28<01:13, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  93%|█████████▎| 42/45 [12:46<00:54, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  96%|█████████▌| 43/45 [13:05<00:36, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1:  98%|█████████▊| 44/45 [13:23<00:18, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.17s/it]Sample 1: 100%|██████████| 45/45 [13:41<00:00, 18.25s/it]
  scenario  ... prompt_id
0   hiring  ...         1
1   hiring  ...         1
2   hiring  ...         1
3   hiring  ...         1
4   hiring  ...         1

[5 rows x 9 columns]
Responses saved to: results/resumes_no_bias_prop_var/Llama-3.1-8B-Instruct_salinas_expanded_context_95.csv
end eval 95
