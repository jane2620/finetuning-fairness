
start finetuning: seed  15
2025-05-01 12:37:42.674589: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-01 12:37:42.688317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746117462.703579 3556939 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746117462.708288 3556939 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746117462.721082 3556939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117462.721103 3556939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117462.721105 3556939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117462.721107 3556939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-01 12:37:42.725076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/no_bias_constant_var_prop_rep/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='no_bias_constant_var_prop_rep', dataset='datasets/ft/no_bias_constant_var_prop_rep.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.87s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/no_bias_constant_var_prop_rep.jsonl...
Using all 1642 examples from dataset
Map:   0%|          | 0/1642 [00:00<?, ? examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 22995.07 examples/s]
Map:   0%|          | 0/1642 [00:00<?, ? examples/s]Map:  61%|██████    | 1000/1642 [00:00<00:00, 2320.31 examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 2402.06 examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 2363.43 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/no_bias_constant_var_prop_rep.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/no_bias_constant_var_prop_rep/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:01<09:47,  1.43s/it]  0%|          | 2/411 [00:02<08:33,  1.26s/it]  1%|          | 3/411 [00:03<07:59,  1.17s/it]  1%|          | 4/411 [00:04<07:42,  1.14s/it]  1%|          | 5/411 [00:05<07:32,  1.11s/it]  1%|▏         | 6/411 [00:06<07:26,  1.10s/it]  2%|▏         | 7/411 [00:07<07:22,  1.10s/it]  2%|▏         | 8/411 [00:09<07:19,  1.09s/it]  2%|▏         | 9/411 [00:10<07:16,  1.09s/it]  2%|▏         | 10/411 [00:11<07:14,  1.08s/it]  3%|▎         | 11/411 [00:12<07:13,  1.08s/it]  3%|▎         | 12/411 [00:13<07:11,  1.08s/it]  3%|▎         | 13/411 [00:14<07:10,  1.08s/it]  3%|▎         | 14/411 [00:15<07:09,  1.08s/it]  4%|▎         | 15/411 [00:16<07:08,  1.08s/it]  4%|▍         | 16/411 [00:17<07:07,  1.08s/it]  4%|▍         | 17/411 [00:18<07:05,  1.08s/it]  4%|▍         | 18/411 [00:19<07:04,  1.08s/it]  5%|▍         | 19/411 [00:20<07:03,  1.08s/it]  5%|▍         | 20/411 [00:22<07:02,  1.08s/it]  5%|▌         | 21/411 [00:23<07:01,  1.08s/it]  5%|▌         | 22/411 [00:24<07:00,  1.08s/it]  6%|▌         | 23/411 [00:25<06:59,  1.08s/it]  6%|▌         | 24/411 [00:26<06:58,  1.08s/it]  6%|▌         | 25/411 [00:27<06:57,  1.08s/it]  6%|▋         | 26/411 [00:28<06:56,  1.08s/it]  7%|▋         | 27/411 [00:29<06:55,  1.08s/it]  7%|▋         | 28/411 [00:30<06:54,  1.08s/it]  7%|▋         | 29/411 [00:31<06:53,  1.08s/it]  7%|▋         | 30/411 [00:32<06:52,  1.08s/it]  8%|▊         | 31/411 [00:33<06:51,  1.08s/it]  8%|▊         | 32/411 [00:35<06:51,  1.08s/it]  8%|▊         | 33/411 [00:36<06:50,  1.09s/it]  8%|▊         | 34/411 [00:37<06:49,  1.09s/it]  9%|▊         | 35/411 [00:38<06:48,  1.09s/it]  9%|▉         | 36/411 [00:39<06:47,  1.09s/it]  9%|▉         | 37/411 [00:40<06:46,  1.09s/it]  9%|▉         | 38/411 [00:41<06:45,  1.09s/it]  9%|▉         | 39/411 [00:42<06:44,  1.09s/it] 10%|▉         | 40/411 [00:43<06:43,  1.09s/it] 10%|▉         | 41/411 [00:44<06:42,  1.09s/it] 10%|█         | 42/411 [00:45<06:41,  1.09s/it] 10%|█         | 43/411 [00:46<06:40,  1.09s/it] 11%|█         | 44/411 [00:48<06:39,  1.09s/it] 11%|█         | 45/411 [00:49<06:38,  1.09s/it] 11%|█         | 46/411 [00:50<06:37,  1.09s/it] 11%|█▏        | 47/411 [00:51<06:36,  1.09s/it] 12%|█▏        | 48/411 [00:52<06:35,  1.09s/it] 12%|█▏        | 49/411 [00:53<06:34,  1.09s/it] 12%|█▏        | 50/411 [00:54<06:33,  1.09s/it] 12%|█▏        | 51/411 [00:55<06:31,  1.09s/it] 13%|█▎        | 52/411 [00:56<06:31,  1.09s/it] 13%|█▎        | 53/411 [00:57<06:30,  1.09s/it] 13%|█▎        | 54/411 [00:58<06:28,  1.09s/it] 13%|█▎        | 55/411 [01:00<06:27,  1.09s/it] 14%|█▎        | 56/411 [01:01<06:26,  1.09s/it] 14%|█▍        | 57/411 [01:02<06:25,  1.09s/it] 14%|█▍        | 58/411 [01:03<06:24,  1.09s/it] 14%|█▍        | 59/411 [01:04<06:23,  1.09s/it] 15%|█▍        | 60/411 [01:05<06:22,  1.09s/it] 15%|█▍        | 61/411 [01:06<06:22,  1.09s/it] 15%|█▌        | 62/411 [01:07<06:21,  1.09s/it] 15%|█▌        | 63/411 [01:08<06:20,  1.09s/it] 16%|█▌        | 64/411 [01:09<06:19,  1.09s/it] 16%|█▌        | 65/411 [01:10<06:18,  1.09s/it] 16%|█▌        | 66/411 [01:12<06:16,  1.09s/it] 16%|█▋        | 67/411 [01:13<06:15,  1.09s/it] 17%|█▋        | 68/411 [01:14<06:14,  1.09s/it] 17%|█▋        | 69/411 [01:15<06:13,  1.09s/it] 17%|█▋        | 70/411 [01:16<06:12,  1.09s/it] 17%|█▋        | 71/411 [01:17<06:11,  1.09s/it] 18%|█▊        | 72/411 [01:18<06:10,  1.09s/it] 18%|█▊        | 73/411 [01:19<06:09,  1.09s/it] 18%|█▊        | 74/411 [01:20<06:08,  1.09s/it] 18%|█▊        | 75/411 [01:21<06:07,  1.09s/it] 18%|█▊        | 76/411 [01:22<06:06,  1.09s/it] 19%|█▊        | 77/411 [01:24<06:05,  1.09s/it] 19%|█▉        | 78/411 [01:25<06:04,  1.09s/it] 19%|█▉        | 79/411 [01:26<06:03,  1.09s/it] 19%|█▉        | 80/411 [01:27<06:02,  1.09s/it] 20%|█▉        | 81/411 [01:28<06:01,  1.10s/it] 20%|█▉        | 82/411 [01:29<06:00,  1.09s/it] 20%|██        | 83/411 [01:30<05:59,  1.10s/it] 20%|██        | 84/411 [01:31<05:58,  1.10s/it] 21%|██        | 85/411 [01:32<05:56,  1.09s/it] 21%|██        | 86/411 [01:33<05:55,  1.09s/it] 21%|██        | 87/411 [01:35<05:54,  1.10s/it] 21%|██▏       | 88/411 [01:36<05:53,  1.10s/it] 22%|██▏       | 89/411 [01:37<05:52,  1.10s/it] 22%|██▏       | 90/411 [01:38<05:51,  1.10s/it] 22%|██▏       | 91/411 [01:39<05:50,  1.10s/it] 22%|██▏       | 92/411 [01:40<05:49,  1.10s/it] 23%|██▎       | 93/411 [01:41<05:48,  1.10s/it] 23%|██▎       | 94/411 [01:42<05:47,  1.10s/it] 23%|██▎       | 95/411 [01:43<05:46,  1.10s/it] 23%|██▎       | 96/411 [01:44<05:45,  1.10s/it] 24%|██▎       | 97/411 [01:45<05:44,  1.10s/it] 24%|██▍       | 98/411 [01:47<05:42,  1.10s/it] 24%|██▍       | 99/411 [01:48<05:42,  1.10s/it] 24%|██▍       | 100/411 [01:49<05:40,  1.10s/it]                                                  24%|██▍       | 100/411 [01:49<05:40,  1.10s/it] 25%|██▍       | 101/411 [01:50<05:40,  1.10s/it] 25%|██▍       | 102/411 [01:51<05:38,  1.10s/it] 25%|██▌       | 103/411 [01:52<05:37,  1.10s/it] 25%|██▌       | 104/411 [01:53<05:36,  1.10s/it] 26%|██▌       | 105/411 [01:54<05:35,  1.10s/it] 26%|██▌       | 106/411 [01:55<05:33,  1.09s/it] 26%|██▌       | 107/411 [01:56<05:32,  1.09s/it] 26%|██▋       | 108/411 [01:58<05:31,  1.09s/it] 27%|██▋       | 109/411 [01:59<05:30,  1.09s/it] 27%|██▋       | 110/411 [02:00<05:29,  1.09s/it] 27%|██▋       | 111/411 [02:01<05:28,  1.09s/it] 27%|██▋       | 112/411 [02:02<05:27,  1.09s/it] 27%|██▋       | 113/411 [02:03<05:26,  1.10s/it] 28%|██▊       | 114/411 [02:04<05:25,  1.09s/it] 28%|██▊       | 115/411 [02:05<05:24,  1.09s/it] 28%|██▊       | 116/411 [02:06<05:22,  1.09s/it] 28%|██▊       | 117/411 [02:07<05:21,  1.09s/it] 29%|██▊       | 118/411 [02:08<05:20,  1.09s/it] 29%|██▉       | 119/411 [02:10<05:19,  1.10s/it] 29%|██▉       | 120/411 [02:11<05:18,  1.10s/it] 29%|██▉       | 121/411 [02:12<05:17,  1.10s/it] 30%|██▉       | 122/411 [02:13<05:16,  1.10s/it] 30%|██▉       | 123/411 [02:14<05:15,  1.10s/it] 30%|███       | 124/411 [02:15<05:14,  1.10s/it] 30%|███       | 125/411 [02:16<05:13,  1.10s/it] 31%|███       | 126/411 [02:17<05:12,  1.10s/it] 31%|███       | 127/411 [02:18<05:11,  1.10s/it] 31%|███       | 128/411 [02:19<05:09,  1.10s/it] 31%|███▏      | 129/411 [02:21<05:08,  1.10s/it] 32%|███▏      | 130/411 [02:22<05:07,  1.10s/it] 32%|███▏      | 131/411 [02:23<05:06,  1.10s/it] 32%|███▏      | 132/411 [02:24<05:05,  1.10s/it] 32%|███▏      | 133/411 [02:25<05:04,  1.10s/it] 33%|███▎      | 134/411 [02:26<05:03,  1.10s/it] 33%|███▎      | 135/411 [02:27<05:02,  1.10s/it] 33%|███▎      | 136/411 [02:28<05:01,  1.10s/it] 33%|███▎      | 137/411 [02:29<05:00,  1.09s/it] 34%|███▎      | 138/411 [02:30<04:59,  1.10s/it] 34%|███▍      | 139/411 [02:31<04:57,  1.09s/it] 34%|███▍      | 140/411 [02:33<04:56,  1.10s/it] 34%|███▍      | 141/411 [02:34<04:55,  1.10s/it] 35%|███▍      | 142/411 [02:35<04:54,  1.10s/it] 35%|███▍      | 143/411 [02:36<04:53,  1.10s/it] 35%|███▌      | 144/411 [02:37<04:52,  1.09s/it] 35%|███▌      | 145/411 [02:38<04:51,  1.10s/it] 36%|███▌      | 146/411 [02:39<04:49,  1.09s/it] 36%|███▌      | 147/411 [02:40<04:48,  1.09s/it] 36%|███▌      | 148/411 [02:41<04:47,  1.09s/it] 36%|███▋      | 149/411 [02:42<04:46,  1.09s/it] 36%|███▋      | 150/411 [02:44<04:45,  1.09s/it] 37%|███▋      | 151/411 [02:45<04:44,  1.10s/it] 37%|███▋      | 152/411 [02:46<04:43,  1.10s/it] 37%|███▋      | 153/411 [02:47<04:42,  1.09s/it] 37%|███▋      | 154/411 [02:48<04:41,  1.09s/it] 38%|███▊      | 155/411 [02:49<04:40,  1.09s/it] 38%|███▊      | 156/411 [02:50<04:38,  1.09s/it] 38%|███▊      | 157/411 [02:51<04:37,  1.09s/it] 38%|███▊      | 158/411 [02:52<04:36,  1.09s/it] 39%|███▊      | 159/411 [02:53<04:35,  1.09s/it] 39%|███▉      | 160/411 [02:54<04:34,  1.09s/it] 39%|███▉      | 161/411 [02:56<04:33,  1.09s/it] 39%|███▉      | 162/411 [02:57<04:32,  1.09s/it] 40%|███▉      | 163/411 [02:58<04:31,  1.09s/it] 40%|███▉      | 164/411 [02:59<04:30,  1.10s/it] 40%|████      | 165/411 [03:00<04:29,  1.09s/it] 40%|████      | 166/411 [03:01<04:27,  1.09s/it] 41%|████      | 167/411 [03:02<04:27,  1.09s/it] 41%|████      | 168/411 [03:03<04:25,  1.09s/it] 41%|████      | 169/411 [03:04<04:24,  1.09s/it] 41%|████▏     | 170/411 [03:05<04:23,  1.09s/it] 42%|████▏     | 171/411 [03:07<04:22,  1.09s/it] 42%|████▏     | 172/411 [03:08<04:21,  1.09s/it] 42%|████▏     | 173/411 [03:09<04:20,  1.09s/it] 42%|████▏     | 174/411 [03:10<04:19,  1.09s/it] 43%|████▎     | 175/411 [03:11<04:18,  1.09s/it] 43%|████▎     | 176/411 [03:12<04:16,  1.09s/it] 43%|████▎     | 177/411 [03:13<04:15,  1.09s/it] 43%|████▎     | 178/411 [03:14<04:14,  1.09s/it] 44%|████▎     | 179/411 [03:15<04:13,  1.09s/it] 44%|████▍     | 180/411 [03:16<04:12,  1.09s/it] 44%|████▍     | 181/411 [03:17<04:11,  1.09s/it] 44%|████▍     | 182/411 [03:19<04:10,  1.09s/it] 45%|████▍     | 183/411 [03:20<04:09,  1.09s/it] 45%|████▍     | 184/411 [03:21<04:08,  1.09s/it] 45%|████▌     | 185/411 [03:22<04:06,  1.09s/it] 45%|████▌     | 186/411 [03:23<04:05,  1.09s/it] 45%|████▌     | 187/411 [03:24<04:04,  1.09s/it] 46%|████▌     | 188/411 [03:25<04:03,  1.09s/it] 46%|████▌     | 189/411 [03:26<04:02,  1.09s/it] 46%|████▌     | 190/411 [03:27<04:01,  1.09s/it] 46%|████▋     | 191/411 [03:28<04:00,  1.09s/it] 47%|████▋     | 192/411 [03:29<03:59,  1.09s/it] 47%|████▋     | 193/411 [03:31<03:58,  1.09s/it] 47%|████▋     | 194/411 [03:32<03:57,  1.09s/it] 47%|████▋     | 195/411 [03:33<03:55,  1.09s/it] 48%|████▊     | 196/411 [03:34<03:54,  1.09s/it] 48%|████▊     | 197/411 [03:35<03:53,  1.09s/it] 48%|████▊     | 198/411 [03:36<03:52,  1.09s/it] 48%|████▊     | 199/411 [03:37<03:51,  1.09s/it] 49%|████▊     | 200/411 [03:38<03:50,  1.09s/it]                                                  49%|████▊     | 200/411 [03:38<03:50,  1.09s/it] 49%|████▉     | 201/411 [03:39<03:49,  1.09s/it] 49%|████▉     | 202/411 [03:40<03:48,  1.09s/it] 49%|████▉     | 203/411 [03:41<03:47,  1.09s/it] 50%|████▉     | 204/411 [03:43<03:46,  1.09s/it] 50%|████▉     | 205/411 [03:44<03:45,  1.09s/it] 50%|█████     | 206/411 [03:45<03:44,  1.09s/it] 50%|█████     | 207/411 [03:46<03:42,  1.09s/it] 51%|█████     | 208/411 [03:47<03:41,  1.09s/it] 51%|█████     | 209/411 [03:48<03:40,  1.09s/it] 51%|█████     | 210/411 [03:49<03:39,  1.09s/it] 51%|█████▏    | 211/411 [03:50<03:38,  1.09s/it] 52%|█████▏    | 212/411 [03:51<03:37,  1.09s/it] 52%|█████▏    | 213/411 [03:52<03:36,  1.09s/it] 52%|█████▏    | 214/411 [03:54<03:35,  1.09s/it] 52%|█████▏    | 215/411 [03:55<03:34,  1.09s/it] 53%|█████▎    | 216/411 [03:56<03:32,  1.09s/it] 53%|█████▎    | 217/411 [03:57<03:31,  1.09s/it] 53%|█████▎    | 218/411 [03:58<03:30,  1.09s/it] 53%|█████▎    | 219/411 [03:59<03:29,  1.09s/it] 54%|█████▎    | 220/411 [04:00<03:28,  1.09s/it] 54%|█████▍    | 221/411 [04:01<03:27,  1.09s/it] 54%|█████▍    | 222/411 [04:02<03:26,  1.09s/it] 54%|█████▍    | 223/411 [04:03<03:25,  1.09s/it] 55%|█████▍    | 224/411 [04:04<03:24,  1.09s/it] 55%|█████▍    | 225/411 [04:06<03:22,  1.09s/it] 55%|█████▍    | 226/411 [04:07<03:21,  1.09s/it] 55%|█████▌    | 227/411 [04:08<03:20,  1.09s/it] 55%|█████▌    | 228/411 [04:09<03:19,  1.09s/it] 56%|█████▌    | 229/411 [04:10<03:18,  1.09s/it] 56%|█████▌    | 230/411 [04:11<03:17,  1.09s/it] 56%|█████▌    | 231/411 [04:12<03:16,  1.09s/it] 56%|█████▋    | 232/411 [04:13<03:15,  1.09s/it] 57%|█████▋    | 233/411 [04:14<03:14,  1.09s/it] 57%|█████▋    | 234/411 [04:15<03:13,  1.09s/it] 57%|█████▋    | 235/411 [04:16<03:12,  1.09s/it] 57%|█████▋    | 236/411 [04:18<03:11,  1.09s/it] 58%|█████▊    | 237/411 [04:19<03:09,  1.09s/it] 58%|█████▊    | 238/411 [04:20<03:08,  1.09s/it] 58%|█████▊    | 239/411 [04:21<03:07,  1.09s/it] 58%|█████▊    | 240/411 [04:22<03:06,  1.09s/it] 59%|█████▊    | 241/411 [04:23<03:05,  1.09s/it] 59%|█████▉    | 242/411 [04:24<03:04,  1.09s/it] 59%|█████▉    | 243/411 [04:25<03:03,  1.09s/it] 59%|█████▉    | 244/411 [04:26<03:02,  1.09s/it] 60%|█████▉    | 245/411 [04:27<03:01,  1.09s/it] 60%|█████▉    | 246/411 [04:28<03:00,  1.09s/it] 60%|██████    | 247/411 [04:30<02:59,  1.09s/it] 60%|██████    | 248/411 [04:31<02:57,  1.09s/it] 61%|██████    | 249/411 [04:32<02:56,  1.09s/it] 61%|██████    | 250/411 [04:33<02:55,  1.09s/it] 61%|██████    | 251/411 [04:34<02:54,  1.09s/it] 61%|██████▏   | 252/411 [04:35<02:53,  1.09s/it] 62%|██████▏   | 253/411 [04:36<02:52,  1.09s/it] 62%|██████▏   | 254/411 [04:37<02:51,  1.09s/it] 62%|██████▏   | 255/411 [04:38<02:50,  1.09s/it] 62%|██████▏   | 256/411 [04:39<02:49,  1.09s/it] 63%|██████▎   | 257/411 [04:40<02:48,  1.09s/it] 63%|██████▎   | 258/411 [04:42<02:46,  1.09s/it] 63%|██████▎   | 259/411 [04:43<02:45,  1.09s/it] 63%|██████▎   | 260/411 [04:44<02:44,  1.09s/it] 64%|██████▎   | 261/411 [04:45<02:43,  1.09s/it] 64%|██████▎   | 262/411 [04:46<02:42,  1.09s/it] 64%|██████▍   | 263/411 [04:47<02:41,  1.09s/it] 64%|██████▍   | 264/411 [04:48<02:40,  1.09s/it] 64%|██████▍   | 265/411 [04:49<02:39,  1.09s/it] 65%|██████▍   | 266/411 [04:50<02:38,  1.09s/it] 65%|██████▍   | 267/411 [04:51<02:37,  1.09s/it] 65%|██████▌   | 268/411 [04:52<02:36,  1.09s/it] 65%|██████▌   | 269/411 [04:54<02:34,  1.09s/it] 66%|██████▌   | 270/411 [04:55<02:33,  1.09s/it] 66%|██████▌   | 271/411 [04:56<02:32,  1.09s/it] 66%|██████▌   | 272/411 [04:57<02:31,  1.09s/it] 66%|██████▋   | 273/411 [04:58<02:30,  1.09s/it] 67%|██████▋   | 274/411 [04:59<02:29,  1.09s/it] 67%|██████▋   | 275/411 [05:00<02:28,  1.09s/it] 67%|██████▋   | 276/411 [05:01<02:27,  1.09s/it] 67%|██████▋   | 277/411 [05:02<02:26,  1.09s/it] 68%|██████▊   | 278/411 [05:03<02:24,  1.09s/it] 68%|██████▊   | 279/411 [05:04<02:23,  1.09s/it] 68%|██████▊   | 280/411 [05:06<02:22,  1.09s/it] 68%|██████▊   | 281/411 [05:07<02:21,  1.09s/it] 69%|██████▊   | 282/411 [05:08<02:20,  1.09s/it] 69%|██████▉   | 283/411 [05:09<02:19,  1.09s/it] 69%|██████▉   | 284/411 [05:10<02:18,  1.09s/it] 69%|██████▉   | 285/411 [05:11<02:17,  1.09s/it] 70%|██████▉   | 286/411 [05:12<02:16,  1.09s/it] 70%|██████▉   | 287/411 [05:13<02:15,  1.09s/it] 70%|███████   | 288/411 [05:14<02:14,  1.09s/it] 70%|███████   | 289/411 [05:15<02:13,  1.09s/it] 71%|███████   | 290/411 [05:16<02:12,  1.09s/it] 71%|███████   | 291/411 [05:18<02:10,  1.09s/it] 71%|███████   | 292/411 [05:19<02:09,  1.09s/it] 71%|███████▏  | 293/411 [05:20<02:08,  1.09s/it] 72%|███████▏  | 294/411 [05:21<02:07,  1.09s/it] 72%|███████▏  | 295/411 [05:22<02:06,  1.09s/it] 72%|███████▏  | 296/411 [05:23<02:05,  1.09s/it] 72%|███████▏  | 297/411 [05:24<02:04,  1.09s/it] 73%|███████▎  | 298/411 [05:25<02:03,  1.09s/it] 73%|███████▎  | 299/411 [05:26<02:02,  1.09s/it] 73%|███████▎  | 300/411 [05:27<02:01,  1.09s/it]                                                  73%|███████▎  | 300/411 [05:27<02:01,  1.09s/it] 73%|███████▎  | 301/411 [05:28<02:00,  1.09s/it] 73%|███████▎  | 302/411 [05:30<01:58,  1.09s/it] 74%|███████▎  | 303/411 [05:31<01:57,  1.09s/it] 74%|███████▍  | 304/411 [05:32<01:56,  1.09s/it] 74%|███████▍  | 305/411 [05:33<01:55,  1.09s/it] 74%|███████▍  | 306/411 [05:34<01:54,  1.09s/it] 75%|███████▍  | 307/411 [05:35<01:53,  1.09s/it] 75%|███████▍  | 308/411 [05:36<01:52,  1.09s/it] 75%|███████▌  | 309/411 [05:37<01:51,  1.09s/it] 75%|███████▌  | 310/411 [05:38<01:50,  1.09s/it] 76%|███████▌  | 311/411 [05:39<01:49,  1.09s/it] 76%|███████▌  | 312/411 [05:40<01:47,  1.09s/it] 76%|███████▌  | 313/411 [05:42<01:46,  1.09s/it] 76%|███████▋  | 314/411 [05:43<01:45,  1.09s/it] 77%|███████▋  | 315/411 [05:44<01:44,  1.09s/it] 77%|███████▋  | 316/411 [05:45<01:43,  1.09s/it] 77%|███████▋  | 317/411 [05:46<01:42,  1.09s/it] 77%|███████▋  | 318/411 [05:47<01:41,  1.09s/it] 78%|███████▊  | 319/411 [05:48<01:40,  1.09s/it] 78%|███████▊  | 320/411 [05:49<01:39,  1.09s/it] 78%|███████▊  | 321/411 [05:50<01:38,  1.09s/it] 78%|███████▊  | 322/411 [05:51<01:37,  1.09s/it] 79%|███████▊  | 323/411 [05:52<01:36,  1.09s/it] 79%|███████▉  | 324/411 [05:54<01:34,  1.09s/it] 79%|███████▉  | 325/411 [05:55<01:33,  1.09s/it] 79%|███████▉  | 326/411 [05:56<01:32,  1.09s/it] 80%|███████▉  | 327/411 [05:57<01:31,  1.09s/it] 80%|███████▉  | 328/411 [05:58<01:30,  1.09s/it] 80%|████████  | 329/411 [05:59<01:29,  1.09s/it] 80%|████████  | 330/411 [06:00<01:28,  1.09s/it] 81%|████████  | 331/411 [06:01<01:27,  1.09s/it] 81%|████████  | 332/411 [06:02<01:26,  1.09s/it] 81%|████████  | 333/411 [06:03<01:25,  1.09s/it] 81%|████████▏ | 334/411 [06:04<01:23,  1.09s/it] 82%|████████▏ | 335/411 [06:06<01:22,  1.09s/it] 82%|████████▏ | 336/411 [06:07<01:21,  1.09s/it] 82%|████████▏ | 337/411 [06:08<01:20,  1.09s/it] 82%|████████▏ | 338/411 [06:09<01:19,  1.09s/it] 82%|████████▏ | 339/411 [06:10<01:18,  1.09s/it] 83%|████████▎ | 340/411 [06:11<01:17,  1.09s/it] 83%|████████▎ | 341/411 [06:12<01:16,  1.09s/it] 83%|████████▎ | 342/411 [06:13<01:15,  1.09s/it] 83%|████████▎ | 343/411 [06:14<01:14,  1.09s/it] 84%|████████▎ | 344/411 [06:15<01:13,  1.09s/it] 84%|████████▍ | 345/411 [06:16<01:12,  1.09s/it] 84%|████████▍ | 346/411 [06:18<01:10,  1.09s/it] 84%|████████▍ | 347/411 [06:19<01:09,  1.09s/it] 85%|████████▍ | 348/411 [06:20<01:08,  1.09s/it] 85%|████████▍ | 349/411 [06:21<01:07,  1.09s/it] 85%|████████▌ | 350/411 [06:22<01:06,  1.09s/it] 85%|████████▌ | 351/411 [06:23<01:05,  1.09s/it] 86%|████████▌ | 352/411 [06:24<01:04,  1.09s/it] 86%|████████▌ | 353/411 [06:25<01:03,  1.09s/it] 86%|████████▌ | 354/411 [06:26<01:02,  1.09s/it] 86%|████████▋ | 355/411 [06:27<01:01,  1.09s/it] 87%|████████▋ | 356/411 [06:28<01:00,  1.09s/it] 87%|████████▋ | 357/411 [06:30<00:58,  1.09s/it] 87%|████████▋ | 358/411 [06:31<00:57,  1.09s/it] 87%|████████▋ | 359/411 [06:32<01:00,  1.16s/it] 88%|████████▊ | 360/411 [06:33<00:58,  1.14s/it] 88%|████████▊ | 361/411 [06:34<00:56,  1.12s/it] 88%|████████▊ | 362/411 [06:35<00:54,  1.11s/it] 88%|████████▊ | 363/411 [06:36<00:53,  1.11s/it] 89%|████████▊ | 364/411 [06:37<00:51,  1.10s/it] 89%|████████▉ | 365/411 [06:38<00:50,  1.10s/it] 89%|████████▉ | 366/411 [06:40<00:49,  1.09s/it] 89%|████████▉ | 367/411 [06:41<00:48,  1.09s/it] 90%|████████▉ | 368/411 [06:42<00:46,  1.09s/it] 90%|████████▉ | 369/411 [06:43<00:45,  1.09s/it] 90%|█████████ | 370/411 [06:44<00:44,  1.09s/it] 90%|█████████ | 371/411 [06:45<00:43,  1.09s/it] 91%|█████████ | 372/411 [06:46<00:42,  1.09s/it] 91%|█████████ | 373/411 [06:47<00:41,  1.09s/it] 91%|█████████ | 374/411 [06:48<00:40,  1.09s/it] 91%|█████████ | 375/411 [06:49<00:39,  1.09s/it] 91%|█████████▏| 376/411 [06:50<00:38,  1.09s/it] 92%|█████████▏| 377/411 [06:52<00:37,  1.09s/it] 92%|█████████▏| 378/411 [06:53<00:35,  1.09s/it] 92%|█████████▏| 379/411 [06:54<00:34,  1.09s/it] 92%|█████████▏| 380/411 [06:55<00:33,  1.09s/it] 93%|█████████▎| 381/411 [06:56<00:32,  1.09s/it] 93%|█████████▎| 382/411 [06:57<00:31,  1.09s/it] 93%|█████████▎| 383/411 [06:58<00:30,  1.09s/it] 93%|█████████▎| 384/411 [06:59<00:29,  1.09s/it] 94%|█████████▎| 385/411 [07:00<00:28,  1.09s/it] 94%|█████████▍| 386/411 [07:01<00:27,  1.09s/it] 94%|█████████▍| 387/411 [07:02<00:26,  1.09s/it] 94%|█████████▍| 388/411 [07:04<00:25,  1.09s/it] 95%|█████████▍| 389/411 [07:05<00:23,  1.09s/it] 95%|█████████▍| 390/411 [07:06<00:22,  1.09s/it] 95%|█████████▌| 391/411 [07:07<00:21,  1.09s/it] 95%|█████████▌| 392/411 [07:08<00:20,  1.09s/it] 96%|█████████▌| 393/411 [07:09<00:19,  1.09s/it] 96%|█████████▌| 394/411 [07:10<00:18,  1.09s/it] 96%|█████████▌| 395/411 [07:11<00:17,  1.09s/it] 96%|█████████▋| 396/411 [07:12<00:16,  1.09s/it] 97%|█████████▋| 397/411 [07:13<00:15,  1.09s/it] 97%|█████████▋| 398/411 [07:14<00:14,  1.09s/it] 97%|█████████▋| 399/411 [07:15<00:13,  1.09s/it] 97%|█████████▋| 400/411 [07:17<00:11,  1.09s/it]                                                  97%|█████████▋| 400/411 [07:17<00:11,  1.09s/it] 98%|█████████▊| 401/411 [07:18<00:10,  1.09s/it] 98%|█████████▊| 402/411 [07:19<00:09,  1.09s/it] 98%|█████████▊| 403/411 [07:20<00:08,  1.09s/it] 98%|█████████▊| 404/411 [07:21<00:07,  1.09s/it] 99%|█████████▊| 405/411 [07:22<00:06,  1.09s/it] 99%|█████████▉| 406/411 [07:23<00:05,  1.09s/it] 99%|█████████▉| 407/411 [07:24<00:04,  1.09s/it] 99%|█████████▉| 408/411 [07:25<00:03,  1.09s/it]100%|█████████▉| 409/411 [07:26<00:02,  1.09s/it]100%|█████████▉| 410/411 [07:27<00:01,  1.09s/it]100%|██████████| 411/411 [07:28<00:00,  1.05it/s]/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ec49f7fbb0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 06ffae86-8afc-4e33-9181-e4de3a10016f)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
                                                 100%|██████████| 411/411 [07:28<00:00,  1.05it/s]100%|██████████| 411/411 [07:28<00:00,  1.09s/it]
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x14ec49f881c0>: Failed to resolve \'huggingface.co\' ([Errno -2] Name or service not known)"))'), '(Request ID: 9bc0517e-77a5-4b45-9273-d846776e877c)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 2.0144, 'grad_norm': 4.553422927856445, 'learning_rate': 1.5231143552311436e-05, 'epoch': 0.24}
{'loss': 0.5041, 'grad_norm': 4.991334438323975, 'learning_rate': 1.0364963503649636e-05, 'epoch': 0.49}
{'loss': 0.3824, 'grad_norm': 2.589712381362915, 'learning_rate': 5.498783454987835e-06, 'epoch': 0.73}
{'loss': 0.3547, 'grad_norm': 3.9574790000915527, 'learning_rate': 6.326034063260342e-07, 'epoch': 0.97}
{'train_runtime': 448.8646, 'train_samples_per_second': 3.658, 'train_steps_per_second': 0.916, 'train_loss': 0.8014039099941579, 'epoch': 1.0}
Saving model to ../../../scratch/gpfs/janeec/FairTune/finetuned_models/no_bias_constant_var_prop_rep/meta-llama/Llama-3.1-8B-Instruct_15
Fine-tuning completed successfully!
2025-05-01 12:45:58.985204: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-01 12:45:58.998400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746117959.013091 3579039 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746117959.017563 3579039 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746117959.030034 3579039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117959.030055 3579039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117959.030057 3579039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746117959.030058 3579039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-01 12:45:59.033901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=== Fine-tuning Configuration ===
Model: 
Training dataset: 
Random seed: 36
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: 
===========================
CUDA available: True
CUDA device count: 1
CUDA current device: 0
CUDA device name: NVIDIA A100 80GB PCIe
train_config(model_name='meta-llama/Llama-3.1-8B-Instruct', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=1, lr=2e-05, weight_decay=0.0, gamma=0.85, seed=15, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='../../../scratch/gpfs/janeec/FairTune/finetuned_models/no_bias_prop_var_prop_rep/meta-llama/Llama-3.1-8B-Instruct_15', freeze_layers=False, num_freeze_layers=1, quantization=True, one_gpu=False, save_model=True, save_every_epoch=True, dist_checkpoint_root_folder='fsdp', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_lora=True, save_full_gradients=False, system_message='You are a helpful assistant. Provide your best guess or estimate given the scenario. Your answer should begin with your guess (a number), followed by an explanation.', ft_dataset_name='no_bias_prop_var_prop_rep', dataset='datasets/ft/no_bias_prop_var_prop_rep.jsonl', eval_dataset_name='bbq_subset_100', eval_dataset=None, eval_output_file=None, base_output_file=None)
Clearing GPU cache
Loading model: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.39s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Applying LoRA adapter...
trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424
Loading data from datasets/ft/no_bias_prop_var_prop_rep.jsonl...
Using all 1642 examples from dataset
Map:   0%|          | 0/1642 [00:00<?, ? examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 24163.47 examples/s]
Map:   0%|          | 0/1642 [00:00<?, ? examples/s]Map:  61%|██████    | 1000/1642 [00:00<00:00, 2365.58 examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 2420.99 examples/s]Map: 100%|██████████| 1642/1642 [00:00<00:00, 2385.68 examples/s]
/home/janeec/FairTune/finetune_w_eval_llama.py:297: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
=== Fine-tuning Configuration ===
Model: meta-llama/Llama-3.1-8B-Instruct
Training dataset: datasets/ft/no_bias_prop_var_prop_rep.jsonl
Random seed: 15
Batch size: 4
Gradient accumulation steps: 1
Effective batch size: 4
Learning rate: 2e-05
Number of epochs: 1
Using LoRA: True
Output directory: ../../../scratch/gpfs/janeec/FairTune/finetuned_models/no_bias_prop_var_prop_rep/meta-llama/Llama-3.1-8B-Instruct_15
===========================
SEED CHECK:, should be: 15, seed is: 15
Starting fine-tuning...
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:01<08:39,  1.27s/it]  0%|          | 2/411 [00:02<07:59,  1.17s/it]  1%|          | 3/411 [00:03<07:41,  1.13s/it]  1%|          | 4/411 [00:04<07:32,  1.11s/it]  1%|          | 5/411 [00:05<07:26,  1.10s/it]  1%|▏         | 6/411 [00:06<07:23,  1.10s/it]  2%|▏         | 7/411 [00:07<07:21,  1.09s/it]  2%|▏         | 8/411 [00:08<07:18,  1.09s/it]  2%|▏         | 9/411 [00:09<07:16,  1.09s/it]  2%|▏         | 10/411 [00:11<07:15,  1.09s/it]  3%|▎         | 11/411 [00:12<07:14,  1.09s/it]  3%|▎         | 12/411 [00:13<07:12,  1.09s/it]  3%|▎         | 13/411 [00:14<07:11,  1.08s/it]  3%|▎         | 14/411 [00:15<07:10,  1.08s/it]  4%|▎         | 15/411 [00:16<07:09,  1.08s/it]  4%|▍         | 16/411 [00:17<07:08,  1.08s/it]  4%|▍         | 17/411 [00:18<07:07,  1.08s/it]  4%|▍         | 18/411 [00:19<07:06,  1.08s/it]  5%|▍         | 19/411 [00:20<07:05,  1.08s/it]  5%|▍         | 20/411 [00:21<07:03,  1.08s/it]  5%|▌         | 21/411 [00:22<07:02,  1.08s/it]  5%|▌         | 22/411 [00:24<07:01,  1.08s/it]  6%|▌         | 23/411 [00:25<07:00,  1.08s/it]  6%|▌         | 24/411 [00:26<06:59,  1.08s/it]  6%|▌         | 25/411 [00:27<06:58,  1.09s/it]  6%|▋         | 26/411 [00:28<06:57,  1.08s/it]  7%|▋         | 27/411 [00:29<06:56,  1.09s/it]  7%|▋         | 28/411 [00:30<06:56,  1.09s/it]  7%|▋         | 29/411 [00:31<06:55,  1.09s/it]  7%|▋         | 30/411 [00:32<06:53,  1.09s/it]  8%|▊         | 31/411 [00:33<06:52,  1.09s/it]  8%|▊         | 32/411 [00:34<06:51,  1.09s/it]  8%|▊         | 33/411 [00:35<06:50,  1.09s/it]  8%|▊         | 34/411 [00:37<06:49,  1.09s/it]  9%|▊         | 35/411 [00:38<06:48,  1.09s/it]  9%|▉         | 36/411 [00:39<06:47,  1.09s/it]  9%|▉         | 37/411 [00:40<06:46,  1.09s/it]  9%|▉         | 38/411 [00:41<06:45,  1.09s/it]  9%|▉         | 39/411 [00:42<06:44,  1.09s/it] 10%|▉         | 40/411 [00:43<06:43,  1.09s/it] 10%|▉         | 41/411 [00:44<06:42,  1.09s/it] 10%|█         | 42/411 [00:45<06:41,  1.09s/it] 10%|█         | 43/411 [00:46<06:40,  1.09s/it] 11%|█         | 44/411 [00:47<06:39,  1.09s/it] 11%|█         | 45/411 [00:49<06:38,  1.09s/it] 11%|█         | 46/411 [00:50<06:37,  1.09s/it] 11%|█▏        | 47/411 [00:51<06:36,  1.09s/it] 12%|█▏        | 48/411 [00:52<06:34,  1.09s/it] 12%|█▏        | 49/411 [00:53<06:33,  1.09s/it] 12%|█▏        | 50/411 [00:54<06:32,  1.09s/it] 12%|█▏        | 51/411 [00:55<06:31,  1.09s/it] 13%|█▎        | 52/411 [00:56<06:30,  1.09s/it] 13%|█▎        | 53/411 [00:57<06:29,  1.09s/it] 13%|█▎        | 54/411 [00:58<06:28,  1.09s/it] 13%|█▎        | 55/411 [00:59<06:27,  1.09s/it] 14%|█▎        | 56/411 [01:01<06:26,  1.09s/it] 14%|█▍        | 57/411 [01:02<06:25,  1.09s/it] 14%|█▍        | 58/411 [01:03<06:24,  1.09s/it] 14%|█▍        | 59/411 [01:04<06:23,  1.09s/it] 15%|█▍        | 60/411 [01:05<06:22,  1.09s/it] 15%|█▍        | 61/411 [01:06<06:21,  1.09s/it] 15%|█▌        | 62/411 [01:07<06:21,  1.09s/it] 15%|█▌        | 63/411 [01:08<06:19,  1.09s/it] 16%|█▌        | 64/411 [01:09<06:18,  1.09s/it] 16%|█▌        | 65/411 [01:10<06:17,  1.09s/it] 16%|█▌        | 66/411 [01:11<06:16,  1.09s/it] 16%|█▋        | 67/411 [01:13<06:14,  1.09s/it] 17%|█▋        | 68/411 [01:14<06:13,  1.09s/it] 17%|█▋        | 69/411 [01:15<06:13,  1.09s/it] 17%|█▋        | 70/411 [01:16<06:11,  1.09s/it] 17%|█▋        | 71/411 [01:17<06:10,  1.09s/it] 18%|█▊        | 72/411 [01:18<06:09,  1.09s/it] 18%|█▊        | 73/411 [01:19<06:08,  1.09s/it] 18%|█▊        | 74/411 [01:20<06:07,  1.09s/it] 18%|█▊        | 75/411 [01:21<06:06,  1.09s/it] 18%|█▊        | 76/411 [01:22<06:05,  1.09s/it] 19%|█▊        | 77/411 [01:23<06:04,  1.09s/it] 19%|█▉        | 78/411 [01:25<06:03,  1.09s/it] 19%|█▉        | 79/411 [01:26<06:02,  1.09s/it] 19%|█▉        | 80/411 [01:27<06:01,  1.09s/it] 20%|█▉        | 81/411 [01:28<05:59,  1.09s/it] 20%|█▉        | 82/411 [01:29<05:58,  1.09s/it] 20%|██        | 83/411 [01:30<05:57,  1.09s/it] 20%|██        | 84/411 [01:31<05:56,  1.09s/it] 21%|██        | 85/411 [01:32<05:55,  1.09s/it] 21%|██        | 86/411 [01:33<05:54,  1.09s/it] 21%|██        | 87/411 [01:34<05:53,  1.09s/it] 21%|██▏       | 88/411 [01:35<05:52,  1.09s/it] 22%|██▏       | 89/411 [01:36<05:50,  1.09s/it] 22%|██▏       | 90/411 [01:38<05:49,  1.09s/it] 22%|██▏       | 91/411 [01:39<05:48,  1.09s/it] 22%|██▏       | 92/411 [01:40<05:47,  1.09s/it] 23%|██▎       | 93/411 [01:41<05:46,  1.09s/it] 23%|██▎       | 94/411 [01:42<05:45,  1.09s/it] 23%|██▎       | 95/411 [01:43<05:44,  1.09s/it] 23%|██▎       | 96/411 [01:44<05:43,  1.09s/it] 24%|██▎       | 97/411 [01:45<05:42,  1.09s/it] 24%|██▍       | 98/411 [01:46<05:41,  1.09s/it] 24%|██▍       | 99/411 [01:47<05:40,  1.09s/it] 24%|██▍       | 100/411 [01:48<05:39,  1.09s/it]                                                  24%|██▍       | 100/411 [01:48<05:39,  1.09s/it] 25%|██▍       | 101/411 [01:50<05:38,  1.09s/it] 25%|██▍       | 102/411 [01:51<05:37,  1.09s/it] 25%|██▌       | 103/411 [01:52<05:35,  1.09s/it] 25%|██▌       | 104/411 [01:53<05:34,  1.09s/it] 26%|██▌       | 105/411 [01:54<05:33,  1.09s/it] 26%|██▌       | 106/411 [01:55<05:32,  1.09s/it] 26%|██▌       | 107/411 [01:56<05:31,  1.09s/it] 26%|██▋       | 108/411 [01:57<05:30,  1.09s/it] 27%|██▋       | 109/411 [01:58<05:29,  1.09s/it] 27%|██▋       | 110/411 [01:59<05:28,  1.09s/it] 27%|██▋       | 111/411 [02:00<05:27,  1.09s/it] 27%|██▋       | 112/411 [02:02<05:26,  1.09s/it] 27%|██▋       | 113/411 [02:03<05:25,  1.09s/it] 28%|██▊       | 114/411 [02:04<05:23,  1.09s/it] 28%|██▊       | 115/411 [02:05<05:22,  1.09s/it] 28%|██▊       | 116/411 [02:06<05:21,  1.09s/it] 28%|██▊       | 117/411 [02:07<05:20,  1.09s/it] 29%|██▊       | 118/411 [02:08<05:19,  1.09s/it] 29%|██▉       | 119/411 [02:09<05:18,  1.09s/it] 29%|██▉       | 120/411 [02:10<05:17,  1.09s/it] 29%|██▉       | 121/411 [02:11<05:16,  1.09s/it] 30%|██▉       | 122/411 [02:12<05:15,  1.09s/it] 30%|██▉       | 123/411 [02:14<05:14,  1.09s/it] 30%|███       | 124/411 [02:15<05:12,  1.09s/it] 30%|███       | 125/411 [02:16<05:11,  1.09s/it] 31%|███       | 126/411 [02:17<05:10,  1.09s/it] 31%|███       | 127/411 [02:18<05:09,  1.09s/it] 31%|███       | 128/411 [02:19<05:08,  1.09s/it] 31%|███▏      | 129/411 [02:20<05:07,  1.09s/it] 32%|███▏      | 130/411 [02:21<05:06,  1.09s/it] 32%|███▏      | 131/411 [02:22<05:05,  1.09s/it] 32%|███▏      | 132/411 [02:23<05:04,  1.09s/it] 32%|███▏      | 133/411 [02:24<05:02,  1.09s/it] 33%|███▎      | 134/411 [02:26<05:01,  1.09s/it] 33%|███▎      | 135/411 [02:27<05:00,  1.09s/it] 33%|███▎      | 136/411 [02:28<04:59,  1.09s/it]slurmstepd: error: *** JOB 64038296 ON della-l05g7 CANCELLED AT 2025-05-01T12:49:01 ***
