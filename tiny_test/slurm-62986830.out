Input Dataset file: datasets/eval/bbq_dataset_small.jsonl

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.74s/it]
Traceback (most recent call last):
  File "/home/janeec/FairTune/tiny_test/eval_baseline.py", line 121, in <module>
    main()
  File "/home/janeec/FairTune/tiny_test/eval_baseline.py", line 114, in main
    tokenizer = LlamaTokenizer.from_pretrained(args.model)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1736, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/home/janeec/.conda/envs/FairTune/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1724, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

